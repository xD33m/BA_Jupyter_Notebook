{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit ('.venv': venv)"
  },
  "interpreter": {
   "hash": "3fa4fbff26baf5372c1227c893813242fd1c5dcfd93352f945b61348dd56c099"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Gensim - Easy"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "#Import a test data set provided in gensim to train amodel\r\n",
    "\r\n",
    "from gensim.test.utils import common_texts\r\n",
    "from gensim.models import Word2Vec\r\n",
    "\r\n",
    "#Build the model, by selecting the parameters.\r\n",
    "our_model=Word2Vec(sentences=common_texts, vector_size=10, window=5, min_count=1, workers=4)\r\n",
    "\r\n",
    "#Save the model\r\n",
    "our_model.save(\"tempmodel.w2v\")\r\n",
    "\r\n",
    "#Inspect the model by looking for the most similar words for a test word.\r\n",
    "print(our_model.wv.most_similar('computer',topn=5))\r\n",
    "\r\n",
    "#Let us see what the 10-dimensional vector for 'computer'looks like.\r\n",
    "#print(our_model['computer'])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('eps', 0.2914133071899414), ('trees', 0.05541801080107689), ('minors', 0.04264770820736885), ('survey', -0.02176349051296711), ('interface', -0.1523357331752777)]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocessing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tensorflow, Keras - Advanced"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import io\r\n",
    "import re\r\n",
    "import string\r\n",
    "import tensorflow as tf\r\n",
    "import tqdm\r\n",
    "\r\n",
    "from tensorflow.keras import Model\r\n",
    "from tensorflow.keras.layers import Dot, Embedding, Flatten\r\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\r\n",
    "\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "import matplotlib\r\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# Load the TensorBoard notebook extension\r\n",
    "%load_ext tensorboard\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "sentence = \"The wide road shimmered in the hot sun\"\r\n",
    "tokens = list(sentence.lower().split())\r\n",
    "print(len(tokens))\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "8\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "vocab, index = {}, 1  # start indexing from 1\r\n",
    "vocab['<pad>'] = 0  # add a padding token\r\n",
    "for token in tokens:\r\n",
    "  if token not in vocab:\r\n",
    "    vocab[token] = index\r\n",
    "    index += 1\r\n",
    "vocab_size = len(vocab)\r\n",
    "print(vocab)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'<pad>': 0, 'the': 1, 'wide': 2, 'road': 3, 'shimmered': 4, 'in': 5, 'hot': 6, 'sun': 7}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "inverse_vocab = {index: token for token, index in vocab.items()}\r\n",
    "print(inverse_vocab)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{0: '<pad>', 1: 'the', 2: 'wide', 3: 'road', 4: 'shimmered', 5: 'in', 6: 'hot', 7: 'sun'}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "example_sequence = [vocab[word] for word in tokens]\r\n",
    "print(example_sequence)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1, 2, 3, 4, 5, 1, 6, 7]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "window_size = 2\r\n",
    "positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\r\n",
    "      example_sequence,\r\n",
    "      vocabulary_size=vocab_size,\r\n",
    "      window_size=window_size,\r\n",
    "      negative_samples=0)\r\n",
    "print(len(positive_skip_grams))\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "26\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "for target, context in positive_skip_grams[:5]:\r\n",
    "  print(f\"({target}, {context}): ({inverse_vocab[target]}, {inverse_vocab[context]})\")\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(2, 3): (wide, road)\n",
      "(6, 5): (hot, in)\n",
      "(6, 7): (hot, sun)\n",
      "(5, 3): (in, road)\n",
      "(7, 6): (sun, hot)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# Get target and context words for one positive skip-gram.\r\n",
    "target_word, context_word = positive_skip_grams[0]\r\n",
    "\r\n",
    "# Set the number of negative samples per positive context.\r\n",
    "num_ns = 4\r\n",
    "\r\n",
    "context_class = tf.reshape(tf.constant(context_word, dtype=\"int64\"), (1, 1))\r\n",
    "negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\r\n",
    "    true_classes=context_class,  # class that should be sampled as 'positive'\r\n",
    "    num_true=1,  # each positive skip-gram has 1 positive context class\r\n",
    "    num_sampled=num_ns,  # number of negative context words to sample\r\n",
    "    unique=True,  # all the negative samples should be unique\r\n",
    "    range_max=vocab_size,  # pick index of the samples from [0, vocab_size]\r\n",
    "    seed=4,  # seed for reproducibility\r\n",
    "    name=\"negative_sampling\"  # name of this operation\r\n",
    ")\r\n",
    "print(negative_sampling_candidates)\r\n",
    "print([inverse_vocab[index.numpy()] for index in negative_sampling_candidates])\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tf.Tensor([3 7 0 4], shape=(4,), dtype=int64)\n",
      "['road', 'sun', '<pad>', 'shimmered']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# Add a dimension so you can use concatenation (on the next step).\r\n",
    "negative_sampling_candidates = tf.expand_dims(negative_sampling_candidates, 1)\r\n",
    "\r\n",
    "# Concat positive context word with negative sampled words.\r\n",
    "context = tf.concat([context_class, negative_sampling_candidates], 0)\r\n",
    "\r\n",
    "# Label first context word as 1 (positive) followed by num_ns 0s (negative).\r\n",
    "label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\r\n",
    "\r\n",
    "# Reshape target to shape (1,) and context and label to (num_ns+1,).\r\n",
    "target = tf.squeeze(target_word)\r\n",
    "context = tf.squeeze(context)\r\n",
    "label = tf.squeeze(label)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "print(f\"target_index    : {target}\")\r\n",
    "print(f\"target_word     : {inverse_vocab[target_word]}\")\r\n",
    "print(f\"context_indices : {context}\")\r\n",
    "print(f\"context_words   : {[inverse_vocab[c.numpy()] for c in context]}\")\r\n",
    "print(f\"label           : {label}\")\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "target_index    : 2\n",
      "target_word     : wide\n",
      "context_indices : [3 3 7 0 4]\n",
      "context_words   : ['road', 'road', 'sun', '<pad>', 'shimmered']\n",
      "label           : [1 0 0 0 0]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "print(\"target  :\", target)\r\n",
    "print(\"context :\", context)\r\n",
    "print(\"label   :\", label)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "target  : tf.Tensor(2, shape=(), dtype=int32)\n",
      "context : tf.Tensor([3 3 7 0 4], shape=(5,), dtype=int64)\n",
      "label   : tf.Tensor([1 0 0 0 0], shape=(5,), dtype=int64)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In Keras gibt's ne Funktion die genau das alles tut:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(size=10)\r\n",
    "print(sampling_table)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.00315225 0.00315225 0.00547597 0.00741556 0.00912817 0.01068435\n",
      " 0.01212381 0.01347162 0.01474487 0.0159558 ]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "# Generates skip-gram pairs with negative sampling for a list of sequences\r\n",
    "# (int-encoded sentences) based on window size, number of negative samples\r\n",
    "# and vocabulary size.\r\n",
    "def generate_training_data(sequences, window_size, num_ns, vocab_size, seed):\r\n",
    "  # Elements of each training example are appended to these lists.\r\n",
    "  targets, contexts, labels = [], [], []\r\n",
    "\r\n",
    "  # Build the sampling table for vocab_size tokens.\r\n",
    "  sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\r\n",
    "\r\n",
    "  # Iterate over all sequences (sentences) in dataset.\r\n",
    "  for sequence in tqdm.tqdm(sequences):\r\n",
    "\r\n",
    "    # Generate positive skip-gram pairs for a sequence (sentence).\r\n",
    "    positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\r\n",
    "          sequence,\r\n",
    "          vocabulary_size=vocab_size,\r\n",
    "          sampling_table=sampling_table,\r\n",
    "          window_size=window_size,\r\n",
    "          negative_samples=0)\r\n",
    "\r\n",
    "    # Iterate over each positive skip-gram pair to produce training examples\r\n",
    "    # with positive context word and negative samples.\r\n",
    "    for target_word, context_word in positive_skip_grams:\r\n",
    "      context_class = tf.expand_dims(\r\n",
    "          tf.constant([context_word], dtype=\"int64\"), 1)\r\n",
    "      negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\r\n",
    "          true_classes=context_class,\r\n",
    "          num_true=1,\r\n",
    "          num_sampled=num_ns,\r\n",
    "          unique=True,\r\n",
    "          range_max=vocab_size,\r\n",
    "          seed=seed,\r\n",
    "          name=\"negative_sampling\")\r\n",
    "\r\n",
    "      # Build context and label vectors (for one target word)\r\n",
    "      negative_sampling_candidates = tf.expand_dims(\r\n",
    "          negative_sampling_candidates, 1)\r\n",
    "\r\n",
    "      context = tf.concat([context_class, negative_sampling_candidates], 0)\r\n",
    "      label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\r\n",
    "\r\n",
    "      # Append each element from the training example to global lists.\r\n",
    "      targets.append(target_word)\r\n",
    "      contexts.append(context)\r\n",
    "      labels.append(label)\r\n",
    "\r\n",
    "  return targets, contexts, labels\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Erstellen eines Word-Embeddings-Models"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preprocessing für Word Embeddings:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "with open(path_to_file) as f: \r\n",
    "  lines = f.read().splitlines()\r\n",
    "for line in lines[:20]:\r\n",
    "  print(line)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "text_ds = tf.data.TextLineDataset(path_to_file).filter(lambda x: tf.cast(tf.strings.length(x), bool))\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "# Now, create a custom standardization function to lowercase the text and\r\n",
    "# remove punctuation.\r\n",
    "def custom_standardization(input_data):\r\n",
    "  lowercase = tf.strings.lower(input_data)\r\n",
    "  return tf.strings.regex_replace(lowercase,\r\n",
    "                                  '[%s]' % re.escape(string.punctuation), '')\r\n",
    "\r\n",
    "\r\n",
    "# Define the vocabulary size and number of words in a sequence.\r\n",
    "vocab_size = 4096\r\n",
    "sequence_length = 10\r\n",
    "\r\n",
    "# Use the text vectorization layer to normalize, split, and map strings to\r\n",
    "# integers. Set output_sequence_length length to pad all samples to same length.\r\n",
    "vectorize_layer = TextVectorization(\r\n",
    "    standardize=custom_standardization,\r\n",
    "    max_tokens=vocab_size,\r\n",
    "    output_mode='int',\r\n",
    "    output_sequence_length=sequence_length)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "vectorize_layer.adapt(text_ds.batch(1024))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "# Save the created vocabulary for reference.\r\n",
    "inverse_vocab = vectorize_layer.get_vocabulary()\r\n",
    "print(inverse_vocab[:20])\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['', '[UNK]', 'the', 'and', 'to', 'i', 'of', 'you', 'my', 'a', 'that', 'in', 'is', 'not', 'for', 'with', 'me', 'it', 'be', 'your']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "# Vectorize the data in text_ds.\r\n",
    "text_vector_ds = text_ds.batch(1024).prefetch(tf.data.AUTOTUNE).map(vectorize_layer).unbatch()\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "sequences = list(text_vector_ds.as_numpy_iterator())\r\n",
    "print(len(sequences))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "32777\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "for seq in sequences[:5]:\r\n",
    "  print(f\"{seq} => {[inverse_vocab[i] for i in seq]}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[ 89 270   0   0   0   0   0   0   0   0] => ['first', 'citizen', '', '', '', '', '', '', '', '']\n",
      "[138  36 982 144 673 125  16 106   0   0] => ['before', 'we', 'proceed', 'any', 'further', 'hear', 'me', 'speak', '', '']\n",
      "[34  0  0  0  0  0  0  0  0  0] => ['all', '', '', '', '', '', '', '', '', '']\n",
      "[106 106   0   0   0   0   0   0   0   0] => ['speak', 'speak', '', '', '', '', '', '', '', '']\n",
      "[ 89 270   0   0   0   0   0   0   0   0] => ['first', 'citizen', '', '', '', '', '', '', '', '']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "targets, contexts, labels = generate_training_data(\r\n",
    "    sequences=sequences,\r\n",
    "    window_size=2,\r\n",
    "    num_ns=4,\r\n",
    "    vocab_size=vocab_size,\r\n",
    "    seed=4)\r\n",
    "print(len(targets), len(contexts), len(labels))\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 32777/32777 [00:08<00:00, 4057.85it/s]65292 65292 65292\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "BATCH_SIZE = 1024\r\n",
    "BUFFER_SIZE = 10000\r\n",
    "dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\r\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\r\n",
    "print(dataset)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<BatchDataset shapes: (((1024,), (1024, 5, 1)), (1024, 5)), types: ((tf.int32, tf.int64), tf.int64)>\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "dataset = dataset.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\r\n",
    "print(dataset)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<PrefetchDataset shapes: (((1024,), (1024, 5, 1)), (1024, 5)), types: ((tf.int32, tf.int64), tf.int64)>\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "class Word2Vec(Model):\r\n",
    "  def __init__(self, vocab_size, embedding_dim):\r\n",
    "    super(Word2Vec, self).__init__()\r\n",
    "    self.target_embedding = Embedding(vocab_size,\r\n",
    "                                      embedding_dim,\r\n",
    "                                      input_length=1,\r\n",
    "                                      name=\"w2v_embedding\")\r\n",
    "    self.context_embedding = Embedding(vocab_size,\r\n",
    "                                       embedding_dim,\r\n",
    "                                       input_length=num_ns+1)\r\n",
    "    self.dots = Dot(axes=(3, 2))\r\n",
    "    self.flatten = Flatten()\r\n",
    "\r\n",
    "  def call(self, pair):\r\n",
    "    target, context = pair\r\n",
    "    word_emb = self.target_embedding(target)\r\n",
    "    context_emb = self.context_embedding(context)\r\n",
    "    dots = self.dots([context_emb, word_emb])\r\n",
    "    return self.flatten(dots)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "embedding_dim = 128\r\n",
    "word2vec = Word2Vec(vocab_size, embedding_dim)\r\n",
    "word2vec.compile(optimizer='adam',\r\n",
    "                 loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\r\n",
    "                 metrics=['accuracy'])\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "word2vec.fit(dataset, epochs=20, callbacks=[tensorboard_callback])\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/20\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 1.6082 - accuracy: 0.2329\n",
      "Epoch 2/20\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 1.5886 - accuracy: 0.5538\n",
      "Epoch 3/20\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 1.5411 - accuracy: 0.5969\n",
      "Epoch 4/20\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 1.4595 - accuracy: 0.5751\n",
      "Epoch 5/20\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 1.3621 - accuracy: 0.5846\n",
      "Epoch 6/20\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 1.2649 - accuracy: 0.6110\n",
      "Epoch 7/20\n",
      "63/63 [==============================] - 1s 13ms/step - loss: 1.1740 - accuracy: 0.6442\n",
      "Epoch 8/20\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 1.0897 - accuracy: 0.6785\n",
      "Epoch 9/20\n",
      "63/63 [==============================] - 1s 14ms/step - loss: 1.0116 - accuracy: 0.7103\n",
      "Epoch 10/20\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 0.9390 - accuracy: 0.7383\n",
      "Epoch 11/20\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.8717 - accuracy: 0.7632\n",
      "Epoch 12/20\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.8095 - accuracy: 0.7856\n",
      "Epoch 13/20\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 0.7522 - accuracy: 0.8057\n",
      "Epoch 14/20\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.6995 - accuracy: 0.8237\n",
      "Epoch 15/20\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.6513 - accuracy: 0.8393\n",
      "Epoch 16/20\n",
      "63/63 [==============================] - 1s 13ms/step - loss: 0.6073 - accuracy: 0.8528\n",
      "Epoch 17/20\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 0.5672 - accuracy: 0.8658\n",
      "Epoch 18/20\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.5307 - accuracy: 0.8772\n",
      "Epoch 19/20\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.4975 - accuracy: 0.8867\n",
      "Epoch 20/20\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 0.4673 - accuracy: 0.8960\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x21f0aff9580>"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "%tensorboard --logdir logs"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 2232), started 19:56:47 ago. (Use '!kill 2232' to kill it.)"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "weights = word2vec.get_layer('w2v_embedding').get_weights()[0]\r\n",
    "vocab = vectorize_layer.get_vocabulary()\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "out_v = io.open('vectors.tsv', 'w', encoding='utf-8')\r\n",
    "out_m = io.open('metadata.tsv', 'w', encoding='utf-8')\r\n",
    "\r\n",
    "for index, word in enumerate(vocab):\r\n",
    "  if index == 0:\r\n",
    "    continue  # skip <PAD>.\r\n",
    "  vec = weights[index]\r\n",
    "  out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\r\n",
    "  out_m.write(word + \"\\n\")\r\n",
    "out_v.close()\r\n",
    "out_m.close()\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "index2word = dict(enumerate(list(inverse_vocab)))\r\n",
    "word2index = {v: k for k, v in index2word.items()}\r\n",
    "\r\n",
    "text_vector_ds"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<_UnbatchDataset shapes: (10,), types: tf.int64>"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "def euclidean_dist(vec1, vec2):\r\n",
    "    return np.sqrt(np.sum((vec1-vec2)**2))\r\n",
    "\r\n",
    "def find_closest_neighbor(word_index, vectors):\r\n",
    "    min_dist = 10000 # to act like positive infinity\r\n",
    "    min_index = -1    \r\n",
    "    query_vector = vectors[word_index]    \r\n",
    "    for index, vector in enumerate(vectors):        \r\n",
    "        if euclidean_dist(vector, query_vector) < min_dist and not np.array_equal(vector, query_vector):            \r\n",
    "            min_dist = euclidean_dist(vector, query_vector)\r\n",
    "            min_index = index    \r\n",
    "    return index2word[min_index]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "find_closest_neighbor(word2index['king'], word2vec.get_layer('w2v_embedding').get_weights()[0])\r\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'xi'"
      ]
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "def find_closest(embeds, word, n=1): # n is for \"n closest words\"\r\n",
    "  n = n + 1 # This is becuse the most similar word is definatly that word itself. like the most similar word for \"apple\" is \"apple\". so we should look for top n+1 words\r\n",
    "  main_vec = embeds(word2index[word])\r\n",
    "\r\n",
    "  similarities = -tf.keras.losses.cosine_similarity(embeds.embeddings, main_vec)\r\n",
    "  top_n = tf.math.top_k(similarities, n).indices\r\n",
    "  words = [index2word[i] for i in top_n.numpy()]\r\n",
    "\r\n",
    "  return words[1:], tf.math.top_k(similarities, 3)[0][1]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "target = word2vec.target_embedding\r\n",
    "res, sim = find_closest(target, 'country')\r\n",
    "print('closest: ', res)\r\n",
    "print('top similarity: ', sim.numpy())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "closest:  ['measure']\n",
      "top similarity:  0.51464057\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "weights = word2vec.get_layer('w2v_embedding').get_weights()[0]\r\n",
    "print(\"length of all word2vec weights: (dimension) \",weights.shape, type(weights),\"\\n\",weights)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "length of all word2vec weights: (dimension)  (4096, 128) <class 'numpy.ndarray'> \n",
      " [[-0.03170488  0.04458102  0.02082629 ... -0.01657663  0.0432646\n",
      "   0.03816791]\n",
      " [-0.24896912  0.25002706  0.25575274 ...  0.06890602 -0.03906033\n",
      "  -0.00669807]\n",
      " [ 0.08956954 -0.13569097  0.46888766 ... -0.06426004  0.01015592\n",
      "  -0.17689146]\n",
      " ...\n",
      " [ 0.09986273  0.04257649 -0.04497757 ...  0.09604951 -0.06974712\n",
      "   0.02613098]\n",
      " [-0.30789903  0.00470538  0.20721006 ... -0.08699082  0.10174327\n",
      "   0.14791062]\n",
      " [-0.06157974  0.05344606 -0.11591852 ...  0.2676837  -0.21459925\n",
      "  -0.15642247]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "layer2 = word2vec.get_layer('embedding').get_weights()[0]\r\n",
    "print(\"layer2 weights: (dimension) \",layer2.shape, type(layer2),\"\\n\",layer2)\r\n",
    "\r\n",
    "vocab = vectorize_layer.get_vocabulary()\r\n",
    "print(\"vocabulary: (length/words)\",len(vocab))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "layer2 weights: (dimension)  (4096, 128) <class 'numpy.ndarray'> \n",
      " [[ 0.6983603  -0.7845663  -0.7150691  ...  0.70170385 -0.6963884\n",
      "   0.6602721 ]\n",
      " [ 0.3122846   0.0951447   0.21395807 ... -0.48337275  0.5315396\n",
      "  -0.3748534 ]\n",
      " [ 0.25121936  0.2995198   0.1888315  ...  0.76729554 -0.5334912\n",
      "   0.45754376]\n",
      " ...\n",
      " [ 0.22649363 -0.21330819  0.12587321 ... -0.23354936  0.06174172\n",
      "  -0.13220982]\n",
      " [-0.06439     0.00131847 -0.08911397 ...  0.30978835 -0.34610048\n",
      "  -0.01553277]\n",
      " [-0.05639622 -0.11941099 -0.07163206 ...  0.20494175 -0.17191157\n",
      "  -0.02553475]]\n",
      "vocabulary: (length/words) 4096\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "word2vec.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"word2_vec\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "w2v_embedding (Embedding)    multiple                  524288    \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        multiple                  524288    \n",
      "_________________________________________________________________\n",
      "dot (Dot)                    multiple                  0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            multiple                  0         \n",
      "=================================================================\n",
      "Total params: 1,048,576\n",
      "Trainable params: 1,048,576\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "#from keras.layers import dot\r\n",
    "#similarity = dot([target, context], axes=1, normalize=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Pretrained vs Custom"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# from gensim.models import Word2Vec\r\n",
    "# # from gensim.test.utils import datapath\r\n",
    "# from scipy.linalg import orthogonal_procrustes\r\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\r\n",
    "\r\n",
    "# # Corpus file path\r\n",
    "# file_path = \"\" # .txt path\r\n",
    "\r\n",
    "# model = Word2Vec(corpus_file=file_path,\r\n",
    "#                  min_count=100,\r\n",
    "#                  window=5,\r\n",
    "#                  size=300,\r\n",
    "#                  sample=1e-5,\r\n",
    "#                  negative=15,\r\n",
    "#                  alpha=0.025,\r\n",
    "#                  ns_exponent=0.75,\r\n",
    "#                  workers=8,\r\n",
    "#                  sg=1)\r\n",
    "\r\n",
    "# model2 = Word2Vec(corpus_file=file_path,\r\n",
    "#                  min_count=100,\r\n",
    "#                  window=15,\r\n",
    "#                  size=300,\r\n",
    "#                  sample=1e-8,\r\n",
    "#                  negative=5,\r\n",
    "#                  alpha=0.5,\r\n",
    "#                  ns_exponent=1,\r\n",
    "#                  workers=8,\r\n",
    "#                  sg=1)\r\n",
    "\r\n",
    "# # Get the vocab for each model\r\n",
    "# vocab_model = set(model.wv.vocab.keys())\r\n",
    "# vocab_model2 = set(model2.wv.vocab.keys())\r\n",
    "\r\n",
    "\r\n",
    "# # Find the common vocabulary\r\n",
    "# common_vocab = list(vocab_model2 & vocab_model)\r\n",
    "\r\n",
    "# # Make the orthogonal_procrustes alignment\r\n",
    "# model_matrix = model.wv.__getitem__(common_vocab)\r\n",
    "# model2_matrix = model2.wv.__getitem__(common_vocab)\r\n",
    "# M, _ = orthogonal_procrustes(model2_matrix, model_matrix)\r\n",
    "# model2_matrix_aligned = model2.wv.__getitem__(common_vocab).dot(M)\r\n",
    "\r\n",
    "# # Compute the cosine\r\n",
    "# cos_mat = cosine_similarity(model_matrix, model2_matrix_aligned)\r\n",
    "# vec_cosine = cos_mat.diagonal()\r\n",
    "# mean_cosine = vec_cosine.mean()\r\n",
    "# sd_cosine = vec_cosine.std()\r\n",
    "\r\n",
    "# print(\"Mean cosine = {0}, Std cosine = {1}\".format(mean_cosine, sd_cosine))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Versuch n°2 mit Gensim"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import re  # preprocessing\r\n",
    "import spacy  # preprocessing\r\n",
    "import pandas as pd  # data handling\r\n",
    "from time import time  # To time our operations\r\n",
    "from collections import defaultdict  # For word frequency\r\n",
    "\r\n",
    "\r\n",
    "import json # For import training data\r\n",
    "\r\n",
    "import logging  # Setting up the loggings to monitor gensim\r\n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "with open(r\"E:\\Users\\Lucas xD\\Downloads\\Products_Q_US_edited.json\", encoding=\"utf8\") as json_file:\r\n",
    "    data = json.load(json_file)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "df = pd.json_normalize(data)\r\n",
    "df.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  brand                                             colors gender  \\\n",
       "0  HUGO  [{'code': '001', 'name': 'Black', 'link': 'htt...    Men   \n",
       "1  BOSS  [{'code': '001', 'name': 'Black', 'link': 'htt...    Men   \n",
       "2  BOSS  [{'code': '001', 'name': 'Black', 'link': 'htt...    Men   \n",
       "3  BOSS  [{'code': '001', 'name': 'Black', 'link': 'htt...    Men   \n",
       "4  BOSS  [{'code': '410', 'name': 'Dark Blue', 'link': ...    Men   \n",
       "\n",
       "                                     longDescription  \\\n",
       "0  V-neck Solid Small embossed tonal logo detail ...   \n",
       "1  Cow skin Basket weave printed texture Tonal sq...   \n",
       "2  Lace-up Derby Italian leather upper Leather li...   \n",
       "3  A regular fit tuxedo in virgin wool.  <b>Jacke...   \n",
       "4  A slim-fit suit made from Italian Super 110s v...   \n",
       "\n",
       "                                               name     productId  \\\n",
       "0          Stretch Cotton V-Neck T-Shirt | Dredosos  hbna50261022   \n",
       "1                 Leather belt with embossed detail  hbna50262032   \n",
       "2         Italian Leather Derby Dress Shoe | Prindo  hbna50263062   \n",
       "3   Virgin Wool Tuxedo, Regular Fit | Stars/Glamour  hbna50194045   \n",
       "4  Italian Virgin Wool Suit, Slim Fit | Huge/Genius  hbna50263213   \n",
       "\n",
       "                                    shortDescription  \\\n",
       "0  This t-shirt by HUGO is crafted from cotton wi...   \n",
       "1  Upgrade your everyday collection with this tim...   \n",
       "2  Crafted from fine Italian calfskin with a prin...   \n",
       "3  This regular fit tuxedo by BOSS is crafted in ...   \n",
       "4  Our best-selling suit just got better with an ...   \n",
       "\n",
       "                                               sizes     styleName  \\\n",
       "0                             [XS, L, M, S, XL, XXL]                 \n",
       "1                   [34, 38, 40, 42, 44, 30, 32, 36]                 \n",
       "2  [6.5, 9, 11, 11.5, 12, 7, 7.5, 8, 8.5, 9.5, 10...    DressShoes   \n",
       "3  [34R, 42L, 44L, 46L, 48L, 36R, 36S, 38R, 38S, ...       Tuxedos   \n",
       "4  [44S, 48L, 42L, 44L, 34R, 36R, 38R, 40R, 40S, ...  Professional   \n",
       "\n",
       "                                            variants  \\\n",
       "0  [{'careInstructionCodes': ['000087', 'B1', 'C0...   \n",
       "1  [{'careInstructionCodes': [], 'colorCode': '00...   \n",
       "2  [{'careInstructionCodes': [], 'colorCode': '00...   \n",
       "3  [{'careInstructionCodes': ['00', 'B1', 'C0', '...   \n",
       "4  [{'careInstructionCodes': ['00', 'B1', 'C0', '...   \n",
       "\n",
       "                                               image   id  \n",
       "0  https://images.hugoboss.com/is/image/boss/hbna...  0.0  \n",
       "1  https://images.hugoboss.com/is/image/boss/hbna...  1.0  \n",
       "2  https://images.hugoboss.com/is/image/boss/hbna...  2.0  \n",
       "3  https://images.hugoboss.com/is/image/boss/hbna...  3.0  \n",
       "4  https://images.hugoboss.com/is/image/boss/hbna...  4.0  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brand</th>\n",
       "      <th>colors</th>\n",
       "      <th>gender</th>\n",
       "      <th>longDescription</th>\n",
       "      <th>name</th>\n",
       "      <th>productId</th>\n",
       "      <th>shortDescription</th>\n",
       "      <th>sizes</th>\n",
       "      <th>styleName</th>\n",
       "      <th>variants</th>\n",
       "      <th>image</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HUGO</td>\n",
       "      <td>[{'code': '001', 'name': 'Black', 'link': 'htt...</td>\n",
       "      <td>Men</td>\n",
       "      <td>V-neck Solid Small embossed tonal logo detail ...</td>\n",
       "      <td>Stretch Cotton V-Neck T-Shirt | Dredosos</td>\n",
       "      <td>hbna50261022</td>\n",
       "      <td>This t-shirt by HUGO is crafted from cotton wi...</td>\n",
       "      <td>[XS, L, M, S, XL, XXL]</td>\n",
       "      <td></td>\n",
       "      <td>[{'careInstructionCodes': ['000087', 'B1', 'C0...</td>\n",
       "      <td>https://images.hugoboss.com/is/image/boss/hbna...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BOSS</td>\n",
       "      <td>[{'code': '001', 'name': 'Black', 'link': 'htt...</td>\n",
       "      <td>Men</td>\n",
       "      <td>Cow skin Basket weave printed texture Tonal sq...</td>\n",
       "      <td>Leather belt with embossed detail</td>\n",
       "      <td>hbna50262032</td>\n",
       "      <td>Upgrade your everyday collection with this tim...</td>\n",
       "      <td>[34, 38, 40, 42, 44, 30, 32, 36]</td>\n",
       "      <td></td>\n",
       "      <td>[{'careInstructionCodes': [], 'colorCode': '00...</td>\n",
       "      <td>https://images.hugoboss.com/is/image/boss/hbna...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BOSS</td>\n",
       "      <td>[{'code': '001', 'name': 'Black', 'link': 'htt...</td>\n",
       "      <td>Men</td>\n",
       "      <td>Lace-up Derby Italian leather upper Leather li...</td>\n",
       "      <td>Italian Leather Derby Dress Shoe | Prindo</td>\n",
       "      <td>hbna50263062</td>\n",
       "      <td>Crafted from fine Italian calfskin with a prin...</td>\n",
       "      <td>[6.5, 9, 11, 11.5, 12, 7, 7.5, 8, 8.5, 9.5, 10...</td>\n",
       "      <td>DressShoes</td>\n",
       "      <td>[{'careInstructionCodes': [], 'colorCode': '00...</td>\n",
       "      <td>https://images.hugoboss.com/is/image/boss/hbna...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BOSS</td>\n",
       "      <td>[{'code': '001', 'name': 'Black', 'link': 'htt...</td>\n",
       "      <td>Men</td>\n",
       "      <td>A regular fit tuxedo in virgin wool.  &lt;b&gt;Jacke...</td>\n",
       "      <td>Virgin Wool Tuxedo, Regular Fit | Stars/Glamour</td>\n",
       "      <td>hbna50194045</td>\n",
       "      <td>This regular fit tuxedo by BOSS is crafted in ...</td>\n",
       "      <td>[34R, 42L, 44L, 46L, 48L, 36R, 36S, 38R, 38S, ...</td>\n",
       "      <td>Tuxedos</td>\n",
       "      <td>[{'careInstructionCodes': ['00', 'B1', 'C0', '...</td>\n",
       "      <td>https://images.hugoboss.com/is/image/boss/hbna...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BOSS</td>\n",
       "      <td>[{'code': '410', 'name': 'Dark Blue', 'link': ...</td>\n",
       "      <td>Men</td>\n",
       "      <td>A slim-fit suit made from Italian Super 110s v...</td>\n",
       "      <td>Italian Virgin Wool Suit, Slim Fit | Huge/Genius</td>\n",
       "      <td>hbna50263213</td>\n",
       "      <td>Our best-selling suit just got better with an ...</td>\n",
       "      <td>[44S, 48L, 42L, 44L, 34R, 36R, 38R, 40R, 40S, ...</td>\n",
       "      <td>Professional</td>\n",
       "      <td>[{'careInstructionCodes': ['00', 'B1', 'C0', '...</td>\n",
       "      <td>https://images.hugoboss.com/is/image/boss/hbna...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "df.info()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2777 entries, 0 to 2776\n",
      "Data columns (total 12 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   brand             2777 non-null   object \n",
      " 1   colors            2777 non-null   object \n",
      " 2   gender            2777 non-null   object \n",
      " 3   longDescription   2777 non-null   object \n",
      " 4   name              2777 non-null   object \n",
      " 5   productId         2777 non-null   object \n",
      " 6   shortDescription  2777 non-null   object \n",
      " 7   sizes             2777 non-null   object \n",
      " 8   styleName         2777 non-null   object \n",
      " 9   variants          2777 non-null   object \n",
      " 10  image             2777 non-null   object \n",
      " 11  id                2777 non-null   float64\n",
      "dtypes: float64(1), object(11)\n",
      "memory usage: 260.5+ KB\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "df = df.drop(columns=['brand', 'colors', 'gender', 'productId', 'sizes', 'styleName', 'variants', 'image', 'id'])\r\n",
    "df.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                     longDescription  \\\n",
       "0  V-neck Solid Small embossed tonal logo detail ...   \n",
       "1  Cow skin Basket weave printed texture Tonal sq...   \n",
       "2  Lace-up Derby Italian leather upper Leather li...   \n",
       "3  A regular fit tuxedo in virgin wool.  <b>Jacke...   \n",
       "4  A slim-fit suit made from Italian Super 110s v...   \n",
       "\n",
       "                                               name  \\\n",
       "0          Stretch Cotton V-Neck T-Shirt | Dredosos   \n",
       "1                 Leather belt with embossed detail   \n",
       "2         Italian Leather Derby Dress Shoe | Prindo   \n",
       "3   Virgin Wool Tuxedo, Regular Fit | Stars/Glamour   \n",
       "4  Italian Virgin Wool Suit, Slim Fit | Huge/Genius   \n",
       "\n",
       "                                    shortDescription  \n",
       "0  This t-shirt by HUGO is crafted from cotton wi...  \n",
       "1  Upgrade your everyday collection with this tim...  \n",
       "2  Crafted from fine Italian calfskin with a prin...  \n",
       "3  This regular fit tuxedo by BOSS is crafted in ...  \n",
       "4  Our best-selling suit just got better with an ...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longDescription</th>\n",
       "      <th>name</th>\n",
       "      <th>shortDescription</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>V-neck Solid Small embossed tonal logo detail ...</td>\n",
       "      <td>Stretch Cotton V-Neck T-Shirt | Dredosos</td>\n",
       "      <td>This t-shirt by HUGO is crafted from cotton wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cow skin Basket weave printed texture Tonal sq...</td>\n",
       "      <td>Leather belt with embossed detail</td>\n",
       "      <td>Upgrade your everyday collection with this tim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lace-up Derby Italian leather upper Leather li...</td>\n",
       "      <td>Italian Leather Derby Dress Shoe | Prindo</td>\n",
       "      <td>Crafted from fine Italian calfskin with a prin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A regular fit tuxedo in virgin wool.  &lt;b&gt;Jacke...</td>\n",
       "      <td>Virgin Wool Tuxedo, Regular Fit | Stars/Glamour</td>\n",
       "      <td>This regular fit tuxedo by BOSS is crafted in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A slim-fit suit made from Italian Super 110s v...</td>\n",
       "      <td>Italian Virgin Wool Suit, Slim Fit | Huge/Genius</td>\n",
       "      <td>Our best-selling suit just got better with an ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "df = df.stack().reset_index() # all in one column\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "df = df.drop(columns=['level_0', 'level_1'])\r\n",
    "df.columns = ['sentences']\r\n",
    "df.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                           sentences\n",
       "0  V-neck Solid Small embossed tonal logo detail ...\n",
       "1           Stretch Cotton V-Neck T-Shirt | Dredosos\n",
       "2  This t-shirt by HUGO is crafted from cotton wi...\n",
       "3  Cow skin Basket weave printed texture Tonal sq...\n",
       "4                  Leather belt with embossed detail"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>V-neck Solid Small embossed tonal logo detail ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Stretch Cotton V-Neck T-Shirt | Dredosos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This t-shirt by HUGO is crafted from cotton wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cow skin Basket weave printed texture Tonal sq...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Leather belt with embossed detail</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\", disable=['ner', 'parser']) # disabling Named Entity Recognition for speed"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "def cleaning(doc):\r\n",
    "    # Lemmatizes and removes stopwords\r\n",
    "    # doc needs to be a spacy Doc object\r\n",
    "    txt = [token.lemma_ for token in doc if not token.is_stop]\r\n",
    "    # Word2Vec uses context words to learn the vector representation of a target word,\r\n",
    "    # if a sentence is only one or two words long,\r\n",
    "    # the benefit for the training is very small\r\n",
    "    if len(txt) > 2:\r\n",
    "        return ' '.join(txt)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "brief_cleaning = (re.sub(\"[^A-Za-z']+\", ' ', str(row)).lower() for row in df['sentences']) # Removes non-alphabetic characters"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "t = time()\r\n",
    "\r\n",
    "txt = [cleaning(doc) for doc in nlp.pipe(brief_cleaning, batch_size=5000)]\r\n",
    "\r\n",
    "print('Time to clean up everything: {} mins'.format(round((time() - t) / 60, 2)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Time to clean up everything: 0.17 mins\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "txt[:5]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['v neck solid small emboss tonal logo detail hem stretch cotton',\n",
       " 'stretch cotton v neck t shirt dredosos',\n",
       " 't shirt hugo craft cotton hint stretch comfort small emboss tonal logo detail hem complete v neck style',\n",
       " 'cow skin basket weave print texture tonal square pin buckle logo print buckle width cm',\n",
       " 'leather belt emboss detail']"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "df_clean = pd.DataFrame({'clean': txt})\r\n",
    "df_clean = df_clean.dropna().drop_duplicates()\r\n",
    "df_clean.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(6723, 1)"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "from gensim.models.phrases import Phrases, Phraser # Phrases package to automatically detect common phrases (bigrams) from a list of sentences."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "sent = [row.split() for row in df_clean['clean']]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "phrases = Phrases(sent, min_count=30, progress_per=10000)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO - 08:38:34: collecting all words and their counts\n",
      "INFO - 08:38:34: PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "INFO - 08:38:34: collected 40667 token types (unigram + bigrams) from a corpus of 130173 words and 6723 sentences\n",
      "INFO - 08:38:34: merged Phrases<40667 vocab, min_count=30, threshold=10.0, max_vocab_size=40000000>\n",
      "INFO - 08:38:34: Phrases lifecycle event {'msg': 'built Phrases<40667 vocab, min_count=30, threshold=10.0, max_vocab_size=40000000> in 0.12s', 'datetime': '2021-07-09T08:38:34.235027', 'gensim': '4.0.1', 'python': '3.9.1 (tags/v3.9.1:1e5d33e, Dec  7 2020, 17:08:21) [MSC v.1927 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'created'}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "bigram = Phraser(phrases)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO - 08:38:37: exporting phrases from Phrases<40667 vocab, min_count=30, threshold=10.0, max_vocab_size=40000000>\n",
      "INFO - 08:38:37: FrozenPhrases lifecycle event {'msg': 'exported FrozenPhrases<144 phrases, min_count=30, threshold=10.0> from Phrases<40667 vocab, min_count=30, threshold=10.0, max_vocab_size=40000000> in 0.06s', 'datetime': '2021-07-09T08:38:37.432611', 'gensim': '4.0.1', 'python': '3.9.1 (tags/v3.9.1:1e5d33e, Dec  7 2020, 17:08:21) [MSC v.1927 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'created'}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "sentences = bigram[sent]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Most frequent words:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "word_freq = defaultdict(int)\r\n",
    "for sent in sentences:\r\n",
    "    for i in sent:\r\n",
    "        word_freq[i] += 1\r\n",
    "len(word_freq)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "3141"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "sorted(word_freq, key=word_freq.get, reverse=True)[:10]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['fit',\n",
       " 'pocket',\n",
       " 'cotton',\n",
       " 'boss',\n",
       " 'regular',\n",
       " 'craft',\n",
       " 'logo',\n",
       " 'feature',\n",
       " 'design',\n",
       " 'slim']"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "import multiprocessing\r\n",
    "\r\n",
    "from gensim.models import Word2Vec"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "source": [
    "cores = multiprocessing.cpu_count() # Count the number of cores in a computer\r\n",
    "w2v_model = Word2Vec(min_count=2,\r\n",
    "                     window=2,\r\n",
    "                     vector_size=100,\r\n",
    "\r\n",
    "                     min_alpha=0.0007, \r\n",
    "                     negative=20,\r\n",
    "                     workers=cores-1)\r\n",
    "\r\n",
    "#                  min_count=100,\r\n",
    "#                  window=5,\r\n",
    "#                  size=300,\r\n",
    "#                  sample=1e-5,\r\n",
    "#                  negative=15,\r\n",
    "#                  alpha=0.025,\r\n",
    "#                  ns_exponent=0.75,\r\n",
    "#                  workers=8,\r\n",
    "\r\n",
    "#                      min_count=100,\r\n",
    "#                      window=5,\r\n",
    "#                      vector_size=300,\r\n",
    "\r\n",
    "#                      min_alpha=0.025, \r\n",
    "#                      negative=15,\r\n",
    "#                      workers=cores-1"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO - 11:06:46: Word2Vec lifecycle event {'params': 'Word2Vec(vocab=0, vector_size=100, alpha=0.025)', 'datetime': '2021-07-09T11:06:46.926545', 'gensim': '4.0.1', 'python': '3.9.1 (tags/v3.9.1:1e5d33e, Dec  7 2020, 17:08:21) [MSC v.1927 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'created'}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Vocab..."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "source": [
    "w2v_model.build_vocab(sentences, progress_per=10000)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO - 11:06:47: collecting all words and their counts\n",
      "INFO - 11:06:47: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO - 11:06:47: collected 3141 word types from a corpus of 115658 raw words and 6723 sentences\n",
      "INFO - 11:06:47: Creating a fresh vocabulary\n",
      "INFO - 11:06:47: Word2Vec lifecycle event {'msg': 'effective_min_count=2 retains 2430 unique words (77.36389684813754%% of original 3141, drops 711)', 'datetime': '2021-07-09T11:06:47.858750', 'gensim': '4.0.1', 'python': '3.9.1 (tags/v3.9.1:1e5d33e, Dec  7 2020, 17:08:21) [MSC v.1927 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'prepare_vocab'}\n",
      "INFO - 11:06:47: Word2Vec lifecycle event {'msg': 'effective_min_count=2 leaves 114947 word corpus (99.38525653218973%% of original 115658, drops 711)', 'datetime': '2021-07-09T11:06:47.859750', 'gensim': '4.0.1', 'python': '3.9.1 (tags/v3.9.1:1e5d33e, Dec  7 2020, 17:08:21) [MSC v.1927 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'prepare_vocab'}\n",
      "INFO - 11:06:47: deleting the raw counts dictionary of 3141 items\n",
      "INFO - 11:06:47: sample=0.001 downsamples 65 most-common words\n",
      "INFO - 11:06:47: Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 94586.12532410408 word corpus (82.3%% of prior 114947)', 'datetime': '2021-07-09T11:06:47.870751', 'gensim': '4.0.1', 'python': '3.9.1 (tags/v3.9.1:1e5d33e, Dec  7 2020, 17:08:21) [MSC v.1927 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'prepare_vocab'}\n",
      "INFO - 11:06:47: estimated required memory for 2430 words and 100 dimensions: 3159000 bytes\n",
      "INFO - 11:06:47: resetting layer weights\n",
      "INFO - 11:06:47: Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2021-07-09T11:06:47.899752', 'gensim': '4.0.1', 'python': '3.9.1 (tags/v3.9.1:1e5d33e, Dec  7 2020, 17:08:21) [MSC v.1927 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'build_vocab'}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Training:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "source": [
    "w2v_model.wv.most_similar(positive=[\"boss\"])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('elevate', 0.6090481877326965),\n",
       " ('impeccable', 0.44833505153656006),\n",
       " ('occasion', 0.419290155172348),\n",
       " ('elegant', 0.37539181113243103),\n",
       " ('pare', 0.373048335313797),\n",
       " ('business', 0.36686018109321594),\n",
       " ('formal', 0.3624463379383087),\n",
       " ('workwear', 0.3576577603816986),\n",
       " ('effortless', 0.3560371398925781),\n",
       " ('italy', 0.3392908275127411)]"
      ]
     },
     "metadata": {},
     "execution_count": 55
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "source": [
    "w2v_model.wv.most_similar(positive=[\"hugo\"])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('smart_casual', 0.653635573387146),\n",
       " ('bold', 0.5222806334495544),\n",
       " ('red', 0.48366621136665344),\n",
       " ('iconic', 0.4434213638305664),\n",
       " ('urban', 0.43985819816589355),\n",
       " ('contemporary', 0.43972058215141296),\n",
       " ('chunky', 0.43887126445770264),\n",
       " ('trend', 0.4271756708621979),\n",
       " ('black', 0.4056681990623474),\n",
       " ('weekend', 0.42688432335853577)]"
      ]
     },
     "metadata": {},
     "execution_count": 109
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "source": [
    "w2v_model.wv.most_similar(positive=[\"watch\"])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('wristwatch', 0.6884447932243347),\n",
       " ('chronograph_watch', 0.6322919130325317),\n",
       " ('timepiece', 0.628886342048645),\n",
       " ('case', 0.5823776721954346),\n",
       " ('surround', 0.5522868037223816),\n",
       " ('mechanism', 0.5499434471130371),\n",
       " ('plating', 0.5467090606689453),\n",
       " ('marker', 0.5371400713920593),\n",
       " ('crown', 0.5342354774475098),\n",
       " ('yellow', 0.522142767906189)]"
      ]
     },
     "metadata": {},
     "execution_count": 73
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "source": [
    "w2v_model.wv.most_similar(positive=[\"casual\"])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('smart_casual', 0.6225508451461792),\n",
       " ('formal', 0.5969498753547668),\n",
       " ('lay', 0.5892535448074341),\n",
       " ('adaptable', 0.5338154435157776),\n",
       " ('host', 0.5331486463546753),\n",
       " ('modern', 0.5241866707801819),\n",
       " ('business', 0.5215544700622559),\n",
       " ('directional', 0.5197253823280334),\n",
       " ('everyday', 0.5166240930557251),\n",
       " ('athletic', 0.48097512125968933)]"
      ]
     },
     "metadata": {},
     "execution_count": 71
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ]
}