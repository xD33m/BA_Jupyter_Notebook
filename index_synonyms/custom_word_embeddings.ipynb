{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit ('.venv': venv)"
  },
  "interpreter": {
   "hash": "3fa4fbff26baf5372c1227c893813242fd1c5dcfd93352f945b61348dd56c099"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Gensim - Easy"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('eps', 0.2914133071899414), ('trees', 0.05541801080107689), ('minors', 0.04264770820736885), ('survey', -0.02176349051296711), ('interface', -0.1523357331752777)]\n"
     ]
    }
   ],
   "source": [
    "#Import a test data set provided in gensim to train amodel\n",
    "\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "#Build the model, by selecting the parameters.\n",
    "our_model=Word2Vec(sentences=common_texts, vector_size=10, window=5, min_count=1, workers=4)\n",
    "\n",
    "#Save the model\n",
    "our_model.save(\"tempmodel.w2v\")\n",
    "\n",
    "#Inspect the model by looking for the most similar words for a test word.\n",
    "print(our_model.wv.most_similar('computer',topn=5))\n",
    "\n",
    "#Let us see what the 10-dimensional vector for 'computer'looks like.\n",
    "#print(our_model['computer'])"
   ]
  },
  {
   "source": [
    "# Tensorflow, Keras - Advanced"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import re\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import tqdm\n",
    "\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Dot, Embedding, Flatten\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "sentence = \"The wide road shimmered in the hot sun\"\n",
    "tokens = list(sentence.lower().split())\n",
    "print(len(tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'<pad>': 0, 'the': 1, 'wide': 2, 'road': 3, 'shimmered': 4, 'in': 5, 'hot': 6, 'sun': 7}\n"
     ]
    }
   ],
   "source": [
    "vocab, index = {}, 1  # start indexing from 1\n",
    "vocab['<pad>'] = 0  # add a padding token\n",
    "for token in tokens:\n",
    "  if token not in vocab:\n",
    "    vocab[token] = index\n",
    "    index += 1\n",
    "vocab_size = len(vocab)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{0: '<pad>', 1: 'the', 2: 'wide', 3: 'road', 4: 'shimmered', 5: 'in', 6: 'hot', 7: 'sun'}\n"
     ]
    }
   ],
   "source": [
    "inverse_vocab = {index: token for token, index in vocab.items()}\n",
    "print(inverse_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1, 2, 3, 4, 5, 1, 6, 7]\n"
     ]
    }
   ],
   "source": [
    "example_sequence = [vocab[word] for word in tokens]\n",
    "print(example_sequence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "26\n"
     ]
    }
   ],
   "source": [
    "window_size = 2\n",
    "positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "      example_sequence,\n",
    "      vocabulary_size=vocab_size,\n",
    "      window_size=window_size,\n",
    "      negative_samples=0)\n",
    "print(len(positive_skip_grams))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(2, 3): (wide, road)\n(6, 5): (hot, in)\n(6, 7): (hot, sun)\n(5, 3): (in, road)\n(7, 6): (sun, hot)\n"
     ]
    }
   ],
   "source": [
    "for target, context in positive_skip_grams[:5]:\n",
    "  print(f\"({target}, {context}): ({inverse_vocab[target]}, {inverse_vocab[context]})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tf.Tensor([3 7 0 4], shape=(4,), dtype=int64)\n['road', 'sun', '<pad>', 'shimmered']\n"
     ]
    }
   ],
   "source": [
    "# Get target and context words for one positive skip-gram.\n",
    "target_word, context_word = positive_skip_grams[0]\n",
    "\n",
    "# Set the number of negative samples per positive context.\n",
    "num_ns = 4\n",
    "\n",
    "context_class = tf.reshape(tf.constant(context_word, dtype=\"int64\"), (1, 1))\n",
    "negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "    true_classes=context_class,  # class that should be sampled as 'positive'\n",
    "    num_true=1,  # each positive skip-gram has 1 positive context class\n",
    "    num_sampled=num_ns,  # number of negative context words to sample\n",
    "    unique=True,  # all the negative samples should be unique\n",
    "    range_max=vocab_size,  # pick index of the samples from [0, vocab_size]\n",
    "    seed=4,  # seed for reproducibility\n",
    "    name=\"negative_sampling\"  # name of this operation\n",
    ")\n",
    "print(negative_sampling_candidates)\n",
    "print([inverse_vocab[index.numpy()] for index in negative_sampling_candidates])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a dimension so you can use concatenation (on the next step).\n",
    "negative_sampling_candidates = tf.expand_dims(negative_sampling_candidates, 1)\n",
    "\n",
    "# Concat positive context word with negative sampled words.\n",
    "context = tf.concat([context_class, negative_sampling_candidates], 0)\n",
    "\n",
    "# Label first context word as 1 (positive) followed by num_ns 0s (negative).\n",
    "label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
    "\n",
    "# Reshape target to shape (1,) and context and label to (num_ns+1,).\n",
    "target = tf.squeeze(target_word)\n",
    "context = tf.squeeze(context)\n",
    "label = tf.squeeze(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "target_index    : 2\n",
      "target_word     : wide\n",
      "context_indices : [3 3 7 0 4]\n",
      "context_words   : ['road', 'road', 'sun', '<pad>', 'shimmered']\n",
      "label           : [1 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(f\"target_index    : {target}\")\n",
    "print(f\"target_word     : {inverse_vocab[target_word]}\")\n",
    "print(f\"context_indices : {context}\")\n",
    "print(f\"context_words   : {[inverse_vocab[c.numpy()] for c in context]}\")\n",
    "print(f\"label           : {label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "target  : tf.Tensor(2, shape=(), dtype=int32)\n",
      "context : tf.Tensor([3 3 7 0 4], shape=(5,), dtype=int64)\n",
      "label   : tf.Tensor([1 0 0 0 0], shape=(5,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(\"target  :\", target)\n",
    "print(\"context :\", context)\n",
    "print(\"label   :\", label)\n"
   ]
  },
  {
   "source": [
    "In Keras gibt's ne Funktion die genau das alles tut:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.00315225 0.00315225 0.00547597 0.00741556 0.00912817 0.01068435\n 0.01212381 0.01347162 0.01474487 0.0159558 ]\n"
     ]
    }
   ],
   "source": [
    "sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(size=10)\n",
    "print(sampling_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates skip-gram pairs with negative sampling for a list of sequences\n",
    "# (int-encoded sentences) based on window size, number of negative samples\n",
    "# and vocabulary size.\n",
    "def generate_training_data(sequences, window_size, num_ns, vocab_size, seed):\n",
    "  # Elements of each training example are appended to these lists.\n",
    "  targets, contexts, labels = [], [], []\n",
    "\n",
    "  # Build the sampling table for vocab_size tokens.\n",
    "  sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n",
    "\n",
    "  # Iterate over all sequences (sentences) in dataset.\n",
    "  for sequence in tqdm.tqdm(sequences):\n",
    "\n",
    "    # Generate positive skip-gram pairs for a sequence (sentence).\n",
    "    positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "          sequence,\n",
    "          vocabulary_size=vocab_size,\n",
    "          sampling_table=sampling_table,\n",
    "          window_size=window_size,\n",
    "          negative_samples=0)\n",
    "\n",
    "    # Iterate over each positive skip-gram pair to produce training examples\n",
    "    # with positive context word and negative samples.\n",
    "    for target_word, context_word in positive_skip_grams:\n",
    "      context_class = tf.expand_dims(\n",
    "          tf.constant([context_word], dtype=\"int64\"), 1)\n",
    "      negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "          true_classes=context_class,\n",
    "          num_true=1,\n",
    "          num_sampled=num_ns,\n",
    "          unique=True,\n",
    "          range_max=vocab_size,\n",
    "          seed=seed,\n",
    "          name=\"negative_sampling\")\n",
    "\n",
    "      # Build context and label vectors (for one target word)\n",
    "      negative_sampling_candidates = tf.expand_dims(\n",
    "          negative_sampling_candidates, 1)\n",
    "\n",
    "      context = tf.concat([context_class, negative_sampling_candidates], 0)\n",
    "      label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
    "\n",
    "      # Append each element from the training example to global lists.\n",
    "      targets.append(target_word)\n",
    "      contexts.append(context)\n",
    "      labels.append(label)\n",
    "\n",
    "  return targets, contexts, labels\n"
   ]
  },
  {
   "source": [
    "## Erstellen eines Word-Embeddings-Models"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Preprocessing für Word Embeddings:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n"
     ]
    }
   ],
   "source": [
    "with open(path_to_file) as f: \n",
    "  lines = f.read().splitlines()\n",
    "for line in lines[:20]:\n",
    "  print(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_ds = tf.data.TextLineDataset(path_to_file).filter(lambda x: tf.cast(tf.strings.length(x), bool))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, create a custom standardization function to lowercase the text and\n",
    "# remove punctuation.\n",
    "def custom_standardization(input_data):\n",
    "  lowercase = tf.strings.lower(input_data)\n",
    "  return tf.strings.regex_replace(lowercase,\n",
    "                                  '[%s]' % re.escape(string.punctuation), '')\n",
    "\n",
    "\n",
    "# Define the vocabulary size and number of words in a sequence.\n",
    "vocab_size = 4096\n",
    "sequence_length = 10\n",
    "\n",
    "# Use the text vectorization layer to normalize, split, and map strings to\n",
    "# integers. Set output_sequence_length length to pad all samples to same length.\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_layer.adapt(text_ds.batch(1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['', '[UNK]', 'the', 'and', 'to', 'i', 'of', 'you', 'my', 'a', 'that', 'in', 'is', 'not', 'for', 'with', 'me', 'it', 'be', 'your']\n"
     ]
    }
   ],
   "source": [
    "# Save the created vocabulary for reference.\n",
    "inverse_vocab = vectorize_layer.get_vocabulary()\n",
    "print(inverse_vocab[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the data in text_ds.\n",
    "text_vector_ds = text_ds.batch(1024).prefetch(tf.data.AUTOTUNE).map(vectorize_layer).unbatch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "32777\n"
     ]
    }
   ],
   "source": [
    "sequences = list(text_vector_ds.as_numpy_iterator())\n",
    "print(len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[ 89 270   0   0   0   0   0   0   0   0] => ['first', 'citizen', '', '', '', '', '', '', '', '']\n[138  36 982 144 673 125  16 106   0   0] => ['before', 'we', 'proceed', 'any', 'further', 'hear', 'me', 'speak', '', '']\n[34  0  0  0  0  0  0  0  0  0] => ['all', '', '', '', '', '', '', '', '', '']\n[106 106   0   0   0   0   0   0   0   0] => ['speak', 'speak', '', '', '', '', '', '', '', '']\n[ 89 270   0   0   0   0   0   0   0   0] => ['first', 'citizen', '', '', '', '', '', '', '', '']\n"
     ]
    }
   ],
   "source": [
    "for seq in sequences[:5]:\n",
    "  print(f\"{seq} => {[inverse_vocab[i] for i in seq]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 32777/32777 [00:08<00:00, 4057.85it/s]65292 65292 65292\n",
      "\n"
     ]
    }
   ],
   "source": [
    "targets, contexts, labels = generate_training_data(\n",
    "    sequences=sequences,\n",
    "    window_size=2,\n",
    "    num_ns=4,\n",
    "    vocab_size=vocab_size,\n",
    "    seed=4)\n",
    "print(len(targets), len(contexts), len(labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<BatchDataset shapes: (((1024,), (1024, 5, 1)), (1024, 5)), types: ((tf.int32, tf.int64), tf.int64)>\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 1024\n",
    "BUFFER_SIZE = 10000\n",
    "dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<PrefetchDataset shapes: (((1024,), (1024, 5, 1)), (1024, 5)), types: ((tf.int32, tf.int64), tf.int64)>\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec(Model):\n",
    "  def __init__(self, vocab_size, embedding_dim):\n",
    "    super(Word2Vec, self).__init__()\n",
    "    self.target_embedding = Embedding(vocab_size,\n",
    "                                      embedding_dim,\n",
    "                                      input_length=1,\n",
    "                                      name=\"w2v_embedding\")\n",
    "    self.context_embedding = Embedding(vocab_size,\n",
    "                                       embedding_dim,\n",
    "                                       input_length=num_ns+1)\n",
    "    self.dots = Dot(axes=(3, 2))\n",
    "    self.flatten = Flatten()\n",
    "\n",
    "  def call(self, pair):\n",
    "    target, context = pair\n",
    "    word_emb = self.target_embedding(target)\n",
    "    context_emb = self.context_embedding(context)\n",
    "    dots = self.dots([context_emb, word_emb])\n",
    "    return self.flatten(dots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 128\n",
    "word2vec = Word2Vec(vocab_size, embedding_dim)\n",
    "word2vec.compile(optimizer='adam',\n",
    "                 loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                 metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/20\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 1.6082 - accuracy: 0.2329\n",
      "Epoch 2/20\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 1.5886 - accuracy: 0.5538\n",
      "Epoch 3/20\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 1.5411 - accuracy: 0.5969\n",
      "Epoch 4/20\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 1.4595 - accuracy: 0.5751\n",
      "Epoch 5/20\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 1.3621 - accuracy: 0.5846\n",
      "Epoch 6/20\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 1.2649 - accuracy: 0.6110\n",
      "Epoch 7/20\n",
      "63/63 [==============================] - 1s 13ms/step - loss: 1.1740 - accuracy: 0.6442\n",
      "Epoch 8/20\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 1.0897 - accuracy: 0.6785\n",
      "Epoch 9/20\n",
      "63/63 [==============================] - 1s 14ms/step - loss: 1.0116 - accuracy: 0.7103\n",
      "Epoch 10/20\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 0.9390 - accuracy: 0.7383\n",
      "Epoch 11/20\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.8717 - accuracy: 0.7632\n",
      "Epoch 12/20\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.8095 - accuracy: 0.7856\n",
      "Epoch 13/20\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 0.7522 - accuracy: 0.8057\n",
      "Epoch 14/20\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.6995 - accuracy: 0.8237\n",
      "Epoch 15/20\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.6513 - accuracy: 0.8393\n",
      "Epoch 16/20\n",
      "63/63 [==============================] - 1s 13ms/step - loss: 0.6073 - accuracy: 0.8528\n",
      "Epoch 17/20\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 0.5672 - accuracy: 0.8658\n",
      "Epoch 18/20\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.5307 - accuracy: 0.8772\n",
      "Epoch 19/20\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.4975 - accuracy: 0.8867\n",
      "Epoch 20/20\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 0.4673 - accuracy: 0.8960\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x21f0aff9580>"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "word2vec.fit(dataset, epochs=20, callbacks=[tensorboard_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Reusing TensorBoard on port 6006 (pid 2232), started 19:56:47 ago. (Use '!kill 2232' to kill it.)"
     },
     "metadata": {}
    }
   ],
   "source": [
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = word2vec.get_layer('w2v_embedding').get_weights()[0]\n",
    "vocab = vectorize_layer.get_vocabulary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_v = io.open('vectors.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('metadata.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for index, word in enumerate(vocab):\n",
    "  if index == 0:\n",
    "    continue  # skip <PAD>.\n",
    "  vec = weights[index]\n",
    "  out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "  out_m.write(word + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<_UnbatchDataset shapes: (10,), types: tf.int64>"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "index2word = dict(enumerate(list(inverse_vocab)))\n",
    "word2index = {v: k for k, v in index2word.items()}\n",
    "\n",
    "text_vector_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_dist(vec1, vec2):\n",
    "    return np.sqrt(np.sum((vec1-vec2)**2))\n",
    "\n",
    "def find_closest_neighbor(word_index, vectors):\n",
    "    min_dist = 10000 # to act like positive infinity\n",
    "    min_index = -1    \n",
    "    query_vector = vectors[word_index]    \n",
    "    for index, vector in enumerate(vectors):        \n",
    "        if euclidean_dist(vector, query_vector) < min_dist and not np.array_equal(vector, query_vector):            \n",
    "            min_dist = euclidean_dist(vector, query_vector)\n",
    "            min_index = index    \n",
    "    return index2word[min_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'xi'"
      ]
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "find_closest_neighbor(word2index['king'], word2vec.get_layer('w2v_embedding').get_weights()[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest(embeds, word, n=1): # n is for \"n closest words\"\n",
    "  n = n + 1 # This is becuse the most similar word is definatly that word itself. like the most similar word for \"apple\" is \"apple\". so we should look for top n+1 words\n",
    "  main_vec = embeds(word2index[word])\n",
    "\n",
    "  similarities = -tf.keras.losses.cosine_similarity(embeds.embeddings, main_vec)\n",
    "  top_n = tf.math.top_k(similarities, n).indices\n",
    "  words = [index2word[i] for i in top_n.numpy()]\n",
    "\n",
    "  return words[1:], tf.math.top_k(similarities, 3)[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "closest:  ['measure']\ntop similarity:  0.51464057\n"
     ]
    }
   ],
   "source": [
    "target = word2vec.target_embedding\n",
    "res, sim = find_closest(target, 'country')\n",
    "print('closest: ', res)\n",
    "print('top similarity: ', sim.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "length of all word2vec weights: (dimension)  (4096, 128) <class 'numpy.ndarray'> \n [[-0.03170488  0.04458102  0.02082629 ... -0.01657663  0.0432646\n   0.03816791]\n [-0.24896912  0.25002706  0.25575274 ...  0.06890602 -0.03906033\n  -0.00669807]\n [ 0.08956954 -0.13569097  0.46888766 ... -0.06426004  0.01015592\n  -0.17689146]\n ...\n [ 0.09986273  0.04257649 -0.04497757 ...  0.09604951 -0.06974712\n   0.02613098]\n [-0.30789903  0.00470538  0.20721006 ... -0.08699082  0.10174327\n   0.14791062]\n [-0.06157974  0.05344606 -0.11591852 ...  0.2676837  -0.21459925\n  -0.15642247]]\n"
     ]
    }
   ],
   "source": [
    "weights = word2vec.get_layer('w2v_embedding').get_weights()[0]\n",
    "print(\"length of all word2vec weights: (dimension) \",weights.shape, type(weights),\"\\n\",weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "layer2 weights: (dimension)  (4096, 128) <class 'numpy.ndarray'> \n [[ 0.6983603  -0.7845663  -0.7150691  ...  0.70170385 -0.6963884\n   0.6602721 ]\n [ 0.3122846   0.0951447   0.21395807 ... -0.48337275  0.5315396\n  -0.3748534 ]\n [ 0.25121936  0.2995198   0.1888315  ...  0.76729554 -0.5334912\n   0.45754376]\n ...\n [ 0.22649363 -0.21330819  0.12587321 ... -0.23354936  0.06174172\n  -0.13220982]\n [-0.06439     0.00131847 -0.08911397 ...  0.30978835 -0.34610048\n  -0.01553277]\n [-0.05639622 -0.11941099 -0.07163206 ...  0.20494175 -0.17191157\n  -0.02553475]]\nvocabulary: (length/words) 4096\n"
     ]
    }
   ],
   "source": [
    "layer2 = word2vec.get_layer('embedding').get_weights()[0]\n",
    "print(\"layer2 weights: (dimension) \",layer2.shape, type(layer2),\"\\n\",layer2)\n",
    "\n",
    "vocab = vectorize_layer.get_vocabulary()\n",
    "print(\"vocabulary: (length/words)\",len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"word2_vec\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nw2v_embedding (Embedding)    multiple                  524288    \n_________________________________________________________________\nembedding (Embedding)        multiple                  524288    \n_________________________________________________________________\ndot (Dot)                    multiple                  0         \n_________________________________________________________________\nflatten (Flatten)            multiple                  0         \n=================================================================\nTotal params: 1,048,576\nTrainable params: 1,048,576\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "word2vec.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-4159a48876c9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdot\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0msimilarity\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\lucas\\Documents\\Github\\BA_Jupyter_Notebook\\.venv\\lib\\site-packages\\keras\\layers\\merge.py\u001b[0m in \u001b[0;36mdot\u001b[1;34m(inputs, axes, normalize, **kwargs)\u001b[0m\n\u001b[0;32m    962\u001b[0m       \u001b[0mA\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdot\u001b[0m \u001b[0mproduct\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msamples\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    963\u001b[0m   \"\"\"\n\u001b[1;32m--> 964\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mDot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\lucas\\Documents\\Github\\BA_Jupyter_Notebook\\.venv\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    997\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    998\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 999\u001b[1;33m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1000\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1001\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_autocast\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\lucas\\Documents\\Github\\BA_Jupyter_Notebook\\.venv\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m_maybe_build\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2599\u001b[0m         \u001b[1;31m# operations.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2600\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaybe_init_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2601\u001b[1;33m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint:disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2602\u001b[0m       \u001b[1;31m# We must set also ensure that the layer is marked as built, and the build\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2603\u001b[0m       \u001b[1;31m# shape is stored since user defined build functions may not be calling\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\lucas\\Documents\\Github\\BA_Jupyter_Notebook\\.venv\\lib\\site-packages\\keras\\utils\\tf_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(instance, input_shape)\u001b[0m\n\u001b[0;32m    256\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minput_shape\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m       \u001b[0minput_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_shapes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_tuples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 258\u001b[1;33m     \u001b[0moutput_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    259\u001b[0m     \u001b[1;31m# Return shapes from `fn` as TensorShapes.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0moutput_shape\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\lucas\\Documents\\Github\\BA_Jupyter_Notebook\\.venv\\lib\\site-packages\\keras\\layers\\merge.py\u001b[0m in \u001b[0;36mbuild\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m    656\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m     \u001b[1;31m# Used purely for shape validation.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 658\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    659\u001b[0m       raise ValueError('A `Dot` layer should be called '\n\u001b[0;32m    660\u001b[0m                        'on a list of 2 inputs.')\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "#from keras.layers import dot\n",
    "#similarity = dot([target, context], axes=1, normalize=True)"
   ]
  },
  {
   "source": [
    "### Pretrained vs Custom"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.models import Word2Vec\n",
    "# # from gensim.test.utils import datapath\n",
    "# from scipy.linalg import orthogonal_procrustes\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# # Corpus file path\n",
    "# file_path = \"\" # .txt path\n",
    "\n",
    "# model = Word2Vec(corpus_file=file_path,\n",
    "#                  min_count=100,\n",
    "#                  window=5,\n",
    "#                  size=300,\n",
    "#                  sample=1e-5,\n",
    "#                  negative=15,\n",
    "#                  alpha=0.025,\n",
    "#                  ns_exponent=0.75,\n",
    "#                  workers=8,\n",
    "#                  sg=1)\n",
    "\n",
    "# model2 = Word2Vec(corpus_file=file_path,\n",
    "#                  min_count=100,\n",
    "#                  window=15,\n",
    "#                  size=300,\n",
    "#                  sample=1e-8,\n",
    "#                  negative=5,\n",
    "#                  alpha=0.5,\n",
    "#                  ns_exponent=1,\n",
    "#                  workers=8,\n",
    "#                  sg=1)\n",
    "\n",
    "# # Get the vocab for each model\n",
    "# vocab_model = set(model.wv.vocab.keys())\n",
    "# vocab_model2 = set(model2.wv.vocab.keys())\n",
    "\n",
    "\n",
    "# # Find the common vocabulary\n",
    "# common_vocab = list(vocab_model2 & vocab_model)\n",
    "\n",
    "# # Make the orthogonal_procrustes alignment\n",
    "# model_matrix = model.wv.__getitem__(common_vocab)\n",
    "# model2_matrix = model2.wv.__getitem__(common_vocab)\n",
    "# M, _ = orthogonal_procrustes(model2_matrix, model_matrix)\n",
    "# model2_matrix_aligned = model2.wv.__getitem__(common_vocab).dot(M)\n",
    "\n",
    "# # Compute the cosine\n",
    "# cos_mat = cosine_similarity(model_matrix, model2_matrix_aligned)\n",
    "# vec_cosine = cos_mat.diagonal()\n",
    "# mean_cosine = vec_cosine.mean()\n",
    "# sd_cosine = vec_cosine.std()\n",
    "\n",
    "# print(\"Mean cosine = {0}, Std cosine = {1}\".format(mean_cosine, sd_cosine))"
   ]
  },
  {
   "source": [
    "# Versuch n°2 mit Gensim"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re  # preprocessing\n",
    "import spacy  # preprocessing\n",
    "import pandas as pd  # data handling\n",
    "from time import time  # To time our operations\n",
    "from collections import defaultdict  # For word frequency\n",
    "\n",
    "\n",
    "import json # For import training data\n",
    "\n",
    "import logging  # Setting up the loggings to monitor gensim\n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"E:\\Users\\Lucas xD\\Downloads\\Products_Q_US_edited.json\", encoding=\"utf8\") as json_file:\n",
    "    data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  brand                                             colors gender  \\\n",
       "0  HUGO  [{'code': '001', 'name': 'Black', 'link': 'htt...    Men   \n",
       "1  BOSS  [{'code': '001', 'name': 'Black', 'link': 'htt...    Men   \n",
       "2  BOSS  [{'code': '001', 'name': 'Black', 'link': 'htt...    Men   \n",
       "3  BOSS  [{'code': '001', 'name': 'Black', 'link': 'htt...    Men   \n",
       "4  BOSS  [{'code': '410', 'name': 'Dark Blue', 'link': ...    Men   \n",
       "\n",
       "                                     longDescription  \\\n",
       "0  V-neck Solid Small embossed tonal logo detail ...   \n",
       "1  Cow skin Basket weave printed texture Tonal sq...   \n",
       "2  Lace-up Derby Italian leather upper Leather li...   \n",
       "3  A regular fit tuxedo in virgin wool.  <b>Jacke...   \n",
       "4  A slim-fit suit made from Italian Super 110s v...   \n",
       "\n",
       "                                               name     productId  \\\n",
       "0          Stretch Cotton V-Neck T-Shirt | Dredosos  hbna50261022   \n",
       "1                 Leather belt with embossed detail  hbna50262032   \n",
       "2         Italian Leather Derby Dress Shoe | Prindo  hbna50263062   \n",
       "3   Virgin Wool Tuxedo, Regular Fit | Stars/Glamour  hbna50194045   \n",
       "4  Italian Virgin Wool Suit, Slim Fit | Huge/Genius  hbna50263213   \n",
       "\n",
       "                                    shortDescription  \\\n",
       "0  This t-shirt by HUGO is crafted from cotton wi...   \n",
       "1  Upgrade your everyday collection with this tim...   \n",
       "2  Crafted from fine Italian calfskin with a prin...   \n",
       "3  This regular fit tuxedo by BOSS is crafted in ...   \n",
       "4  Our best-selling suit just got better with an ...   \n",
       "\n",
       "                                               sizes     styleName  \\\n",
       "0                             [XS, L, M, S, XL, XXL]                 \n",
       "1                   [34, 38, 40, 42, 44, 30, 32, 36]                 \n",
       "2  [6.5, 9, 11, 11.5, 12, 7, 7.5, 8, 8.5, 9.5, 10...    DressShoes   \n",
       "3  [34R, 42L, 44L, 46L, 48L, 36R, 36S, 38R, 38S, ...       Tuxedos   \n",
       "4  [44S, 48L, 42L, 44L, 34R, 36R, 38R, 40R, 40S, ...  Professional   \n",
       "\n",
       "                                            variants  \\\n",
       "0  [{'careInstructionCodes': ['000087', 'B1', 'C0...   \n",
       "1  [{'careInstructionCodes': [], 'colorCode': '00...   \n",
       "2  [{'careInstructionCodes': [], 'colorCode': '00...   \n",
       "3  [{'careInstructionCodes': ['00', 'B1', 'C0', '...   \n",
       "4  [{'careInstructionCodes': ['00', 'B1', 'C0', '...   \n",
       "\n",
       "                                               image   id  \n",
       "0  https://images.hugoboss.com/is/image/boss/hbna...  0.0  \n",
       "1  https://images.hugoboss.com/is/image/boss/hbna...  1.0  \n",
       "2  https://images.hugoboss.com/is/image/boss/hbna...  2.0  \n",
       "3  https://images.hugoboss.com/is/image/boss/hbna...  3.0  \n",
       "4  https://images.hugoboss.com/is/image/boss/hbna...  4.0  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>brand</th>\n      <th>colors</th>\n      <th>gender</th>\n      <th>longDescription</th>\n      <th>name</th>\n      <th>productId</th>\n      <th>shortDescription</th>\n      <th>sizes</th>\n      <th>styleName</th>\n      <th>variants</th>\n      <th>image</th>\n      <th>id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>HUGO</td>\n      <td>[{'code': '001', 'name': 'Black', 'link': 'htt...</td>\n      <td>Men</td>\n      <td>V-neck Solid Small embossed tonal logo detail ...</td>\n      <td>Stretch Cotton V-Neck T-Shirt | Dredosos</td>\n      <td>hbna50261022</td>\n      <td>This t-shirt by HUGO is crafted from cotton wi...</td>\n      <td>[XS, L, M, S, XL, XXL]</td>\n      <td></td>\n      <td>[{'careInstructionCodes': ['000087', 'B1', 'C0...</td>\n      <td>https://images.hugoboss.com/is/image/boss/hbna...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>BOSS</td>\n      <td>[{'code': '001', 'name': 'Black', 'link': 'htt...</td>\n      <td>Men</td>\n      <td>Cow skin Basket weave printed texture Tonal sq...</td>\n      <td>Leather belt with embossed detail</td>\n      <td>hbna50262032</td>\n      <td>Upgrade your everyday collection with this tim...</td>\n      <td>[34, 38, 40, 42, 44, 30, 32, 36]</td>\n      <td></td>\n      <td>[{'careInstructionCodes': [], 'colorCode': '00...</td>\n      <td>https://images.hugoboss.com/is/image/boss/hbna...</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>BOSS</td>\n      <td>[{'code': '001', 'name': 'Black', 'link': 'htt...</td>\n      <td>Men</td>\n      <td>Lace-up Derby Italian leather upper Leather li...</td>\n      <td>Italian Leather Derby Dress Shoe | Prindo</td>\n      <td>hbna50263062</td>\n      <td>Crafted from fine Italian calfskin with a prin...</td>\n      <td>[6.5, 9, 11, 11.5, 12, 7, 7.5, 8, 8.5, 9.5, 10...</td>\n      <td>DressShoes</td>\n      <td>[{'careInstructionCodes': [], 'colorCode': '00...</td>\n      <td>https://images.hugoboss.com/is/image/boss/hbna...</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>BOSS</td>\n      <td>[{'code': '001', 'name': 'Black', 'link': 'htt...</td>\n      <td>Men</td>\n      <td>A regular fit tuxedo in virgin wool.  &lt;b&gt;Jacke...</td>\n      <td>Virgin Wool Tuxedo, Regular Fit | Stars/Glamour</td>\n      <td>hbna50194045</td>\n      <td>This regular fit tuxedo by BOSS is crafted in ...</td>\n      <td>[34R, 42L, 44L, 46L, 48L, 36R, 36S, 38R, 38S, ...</td>\n      <td>Tuxedos</td>\n      <td>[{'careInstructionCodes': ['00', 'B1', 'C0', '...</td>\n      <td>https://images.hugoboss.com/is/image/boss/hbna...</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>BOSS</td>\n      <td>[{'code': '410', 'name': 'Dark Blue', 'link': ...</td>\n      <td>Men</td>\n      <td>A slim-fit suit made from Italian Super 110s v...</td>\n      <td>Italian Virgin Wool Suit, Slim Fit | Huge/Genius</td>\n      <td>hbna50263213</td>\n      <td>Our best-selling suit just got better with an ...</td>\n      <td>[44S, 48L, 42L, 44L, 34R, 36R, 38R, 40R, 40S, ...</td>\n      <td>Professional</td>\n      <td>[{'careInstructionCodes': ['00', 'B1', 'C0', '...</td>\n      <td>https://images.hugoboss.com/is/image/boss/hbna...</td>\n      <td>4.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "source": [
    "df = pd.json_normalize(data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 2777 entries, 0 to 2776\nData columns (total 12 columns):\n #   Column            Non-Null Count  Dtype  \n---  ------            --------------  -----  \n 0   brand             2777 non-null   object \n 1   colors            2777 non-null   object \n 2   gender            2777 non-null   object \n 3   longDescription   2777 non-null   object \n 4   name              2777 non-null   object \n 5   productId         2777 non-null   object \n 6   shortDescription  2777 non-null   object \n 7   sizes             2777 non-null   object \n 8   styleName         2777 non-null   object \n 9   variants          2777 non-null   object \n 10  image             2777 non-null   object \n 11  id                2777 non-null   float64\ndtypes: float64(1), object(11)\nmemory usage: 260.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                     longDescription  \\\n",
       "0  V-neck Solid Small embossed tonal logo detail ...   \n",
       "1  Cow skin Basket weave printed texture Tonal sq...   \n",
       "2  Lace-up Derby Italian leather upper Leather li...   \n",
       "3  A regular fit tuxedo in virgin wool.  <b>Jacke...   \n",
       "4  A slim-fit suit made from Italian Super 110s v...   \n",
       "\n",
       "                                               name  \\\n",
       "0          Stretch Cotton V-Neck T-Shirt | Dredosos   \n",
       "1                 Leather belt with embossed detail   \n",
       "2         Italian Leather Derby Dress Shoe | Prindo   \n",
       "3   Virgin Wool Tuxedo, Regular Fit | Stars/Glamour   \n",
       "4  Italian Virgin Wool Suit, Slim Fit | Huge/Genius   \n",
       "\n",
       "                                    shortDescription  \n",
       "0  This t-shirt by HUGO is crafted from cotton wi...  \n",
       "1  Upgrade your everyday collection with this tim...  \n",
       "2  Crafted from fine Italian calfskin with a prin...  \n",
       "3  This regular fit tuxedo by BOSS is crafted in ...  \n",
       "4  Our best-selling suit just got better with an ...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>longDescription</th>\n      <th>name</th>\n      <th>shortDescription</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>V-neck Solid Small embossed tonal logo detail ...</td>\n      <td>Stretch Cotton V-Neck T-Shirt | Dredosos</td>\n      <td>This t-shirt by HUGO is crafted from cotton wi...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Cow skin Basket weave printed texture Tonal sq...</td>\n      <td>Leather belt with embossed detail</td>\n      <td>Upgrade your everyday collection with this tim...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Lace-up Derby Italian leather upper Leather li...</td>\n      <td>Italian Leather Derby Dress Shoe | Prindo</td>\n      <td>Crafted from fine Italian calfskin with a prin...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>A regular fit tuxedo in virgin wool.  &lt;b&gt;Jacke...</td>\n      <td>Virgin Wool Tuxedo, Regular Fit | Stars/Glamour</td>\n      <td>This regular fit tuxedo by BOSS is crafted in ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>A slim-fit suit made from Italian Super 110s v...</td>\n      <td>Italian Virgin Wool Suit, Slim Fit | Huge/Genius</td>\n      <td>Our best-selling suit just got better with an ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 50
    }
   ],
   "source": [
    "df = df.drop(columns=['brand', 'colors', 'gender', 'productId', 'sizes', 'styleName', 'variants', 'image', 'id'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.stack().reset_index() # all in one column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                           sentences\n",
       "0  V-neck Solid Small embossed tonal logo detail ...\n",
       "1           Stretch Cotton V-Neck T-Shirt | Dredosos\n",
       "2  This t-shirt by HUGO is crafted from cotton wi...\n",
       "3  Cow skin Basket weave printed texture Tonal sq...\n",
       "4                  Leather belt with embossed detail"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentences</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>V-neck Solid Small embossed tonal logo detail ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Stretch Cotton V-Neck T-Shirt | Dredosos</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>This t-shirt by HUGO is crafted from cotton wi...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Cow skin Basket weave printed texture Tonal sq...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Leather belt with embossed detail</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "source": [
    "df = df.drop(columns=['level_0', 'level_1'])\n",
    "df.columns = ['sentences']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\", disable=['ner', 'parser']) # disabling Named Entity Recognition for speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning(doc):\n",
    "    # Lemmatizes and removes stopwords\n",
    "    # doc needs to be a spacy Doc object\n",
    "    txt = [token.lemma_ for token in doc if not token.is_stop]\n",
    "    # Word2Vec uses context words to learn the vector representation of a target word,\n",
    "    # if a sentence is only one or two words long,\n",
    "    # the benefit for the training is very small\n",
    "    if len(txt) > 2:\n",
    "        return ' '.join(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "brief_cleaning = (re.sub(\"[^A-Za-z']+\", ' ', str(row)).lower() for row in df['sentences']) # Removes non-alphabetic characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Time to clean up everything: 0.19 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "\n",
    "txt = [cleaning(doc) for doc in nlp.pipe(brief_cleaning, batch_size=5000)]\n",
    "\n",
    "print('Time to clean up everything: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['v neck solid small emboss tonal logo detail hem stretch cotton',\n",
       " 'stretch cotton v neck t shirt dredosos',\n",
       " 't shirt hugo craft cotton hint stretch comfort small emboss tonal logo detail hem complete v neck style',\n",
       " 'cow skin basket weave print texture tonal square pin buckle logo print buckle width cm',\n",
       " 'leather belt emboss detail']"
      ]
     },
     "metadata": {},
     "execution_count": 57
    }
   ],
   "source": [
    "txt[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(6723, 1)"
      ]
     },
     "metadata": {},
     "execution_count": 58
    }
   ],
   "source": [
    "df_clean = pd.DataFrame({'clean': txt})\n",
    "df_clean = df_clean.dropna().drop_duplicates()\n",
    "df_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases, Phraser # Phrases package to automatically detect common phrases (bigrams) from a list of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = [row.split() for row in df_clean['clean']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO - 10:47:41: collecting all words and their counts\n",
      "INFO - 10:47:41: PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "INFO - 10:47:41: collected 40667 token types (unigram + bigrams) from a corpus of 130173 words and 6723 sentences\n",
      "INFO - 10:47:41: merged Phrases<40667 vocab, min_count=30, threshold=10.0, max_vocab_size=40000000>\n",
      "INFO - 10:47:41: Phrases lifecycle event {'msg': 'built Phrases<40667 vocab, min_count=30, threshold=10.0, max_vocab_size=40000000> in 0.13s', 'datetime': '2021-07-08T10:47:41.520830', 'gensim': '4.0.1', 'python': '3.9.1 (tags/v3.9.1:1e5d33e, Dec  7 2020, 17:08:21) [MSC v.1927 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "phrases = Phrases(sent, min_count=30, progress_per=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO - 10:48:32: exporting phrases from Phrases<40667 vocab, min_count=30, threshold=10.0, max_vocab_size=40000000>\n",
      "INFO - 10:48:32: FrozenPhrases lifecycle event {'msg': 'exported FrozenPhrases<144 phrases, min_count=30, threshold=10.0> from Phrases<40667 vocab, min_count=30, threshold=10.0, max_vocab_size=40000000> in 0.06s', 'datetime': '2021-07-08T10:48:32.890021', 'gensim': '4.0.1', 'python': '3.9.1 (tags/v3.9.1:1e5d33e, Dec  7 2020, 17:08:21) [MSC v.1927 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "bigram = Phraser(phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = bigram[sent]"
   ]
  },
  {
   "source": [
    "#### Most frequent words:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "3141"
      ]
     },
     "metadata": {},
     "execution_count": 65
    }
   ],
   "source": [
    "word_freq = defaultdict(int)\n",
    "for sent in sentences:\n",
    "    for i in sent:\n",
    "        word_freq[i] += 1\n",
    "len(word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['fit',\n",
       " 'pocket',\n",
       " 'cotton',\n",
       " 'boss',\n",
       " 'regular',\n",
       " 'craft',\n",
       " 'logo',\n",
       " 'feature',\n",
       " 'design',\n",
       " 'slim']"
      ]
     },
     "metadata": {},
     "execution_count": 66
    }
   ],
   "source": [
    "sorted(word_freq, key=word_freq.get, reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO - 11:06:06: Word2Vec lifecycle event {'params': 'Word2Vec(vocab=0, vector_size=100, alpha=0.03)', 'datetime': '2021-07-08T11:06:06.817138', 'gensim': '4.0.1', 'python': '3.9.1 (tags/v3.9.1:1e5d33e, Dec  7 2020, 17:08:21) [MSC v.1927 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "cores = multiprocessing.cpu_count() # Count the number of cores in a computer\n",
    "w2v_model = Word2Vec(min_count=10,\n",
    "                     window=2,\n",
    "                     vector_size=100,\n",
    "                     sample=6e-5, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007, \n",
    "                     negative=20,\n",
    "                     workers=cores-1)"
   ]
  },
  {
   "source": [
    "Vocab..."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO - 11:08:53: collecting all words and their counts\n",
      "INFO - 11:08:53: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO - 11:08:53: collected 3141 word types from a corpus of 115658 raw words and 6723 sentences\n",
      "INFO - 11:08:53: Creating a fresh vocabulary\n",
      "INFO - 11:08:53: Word2Vec lifecycle event {'msg': 'effective_min_count=10 retains 1206 unique words (38.39541547277937%% of original 3141, drops 1935)', 'datetime': '2021-07-08T11:08:53.819342', 'gensim': '4.0.1', 'python': '3.9.1 (tags/v3.9.1:1e5d33e, Dec  7 2020, 17:08:21) [MSC v.1927 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'prepare_vocab'}\n",
      "INFO - 11:08:53: Word2Vec lifecycle event {'msg': 'effective_min_count=10 leaves 109974 word corpus (95.0855107299106%% of original 115658, drops 5684)', 'datetime': '2021-07-08T11:08:53.824342', 'gensim': '4.0.1', 'python': '3.9.1 (tags/v3.9.1:1e5d33e, Dec  7 2020, 17:08:21) [MSC v.1927 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'prepare_vocab'}\n",
      "INFO - 11:08:53: deleting the raw counts dictionary of 3141 items\n",
      "INFO - 11:08:53: sample=6e-05 downsamples 916 most-common words\n",
      "INFO - 11:08:53: Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 31085.389924066403 word corpus (28.3%% of prior 109974)', 'datetime': '2021-07-08T11:08:53.843342', 'gensim': '4.0.1', 'python': '3.9.1 (tags/v3.9.1:1e5d33e, Dec  7 2020, 17:08:21) [MSC v.1927 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'prepare_vocab'}\n",
      "WARNING - 11:08:53: sorting after vectors have been allocated is expensive & error-prone\n",
      "INFO - 11:08:53: estimated required memory for 1206 words and 100 dimensions: 1567800 bytes\n",
      "INFO - 11:08:53: resetting layer weights\n",
      "INFO - 11:08:53: Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2021-07-08T11:08:53.861342', 'gensim': '4.0.1', 'python': '3.9.1 (tags/v3.9.1:1e5d33e, Dec  7 2020, 17:08:21) [MSC v.1927 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'build_vocab'}\n"
     ]
    }
   ],
   "source": [
    "w2v_model.build_vocab(sentences, progress_per=10000)"
   ]
  },
  {
   "source": [
    "Training:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING - 11:09:13: Effective 'alpha' higher than previous training cycles\n",
      "INFO - 11:09:13: Word2Vec lifecycle event {'msg': 'training model with 3 workers on 1206 vocabulary and 100 features, using sg=0 hs=0 sample=6e-05 negative=20 window=2', 'datetime': '2021-07-08T11:09:13.156139', 'gensim': '4.0.1', 'python': '3.9.1 (tags/v3.9.1:1e5d33e, Dec  7 2020, 17:08:21) [MSC v.1927 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'train'}\n",
      "INFO - 11:09:13: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:09:13: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:09:13: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:09:13: EPOCH - 1 : training on 115658 raw words (31021 effective words) took 0.1s, 253167 effective words/s\n",
      "INFO - 11:09:13: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:09:13: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:09:13: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:09:13: EPOCH - 2 : training on 115658 raw words (30959 effective words) took 0.1s, 247524 effective words/s\n",
      "INFO - 11:09:13: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:09:13: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:09:13: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:09:13: EPOCH - 3 : training on 115658 raw words (30959 effective words) took 0.1s, 238342 effective words/s\n",
      "INFO - 11:09:13: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:09:13: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:09:13: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:09:13: EPOCH - 4 : training on 115658 raw words (31095 effective words) took 0.1s, 280946 effective words/s\n",
      "INFO - 11:09:13: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:09:13: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:09:13: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:09:13: EPOCH - 5 : training on 115658 raw words (31226 effective words) took 0.1s, 236234 effective words/s\n",
      "INFO - 11:09:14: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:09:14: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:09:14: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:09:14: EPOCH - 6 : training on 115658 raw words (30855 effective words) took 0.1s, 389709 effective words/s\n",
      "INFO - 11:09:14: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:09:14: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:09:14: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:09:14: EPOCH - 7 : training on 115658 raw words (31043 effective words) took 0.1s, 258069 effective words/s\n",
      "INFO - 11:09:14: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:09:14: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:09:14: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:09:14: EPOCH - 8 : training on 115658 raw words (30926 effective words) took 0.1s, 258192 effective words/s\n",
      "INFO - 11:09:14: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:09:14: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:09:14: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:09:14: EPOCH - 9 : training on 115658 raw words (31224 effective words) took 0.1s, 248585 effective words/s\n",
      "INFO - 11:09:14: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:09:14: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:09:14: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:09:14: EPOCH - 10 : training on 115658 raw words (31047 effective words) took 0.1s, 213905 effective words/s\n",
      "INFO - 11:09:14: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:09:14: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:09:14: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:09:14: EPOCH - 11 : training on 115658 raw words (30886 effective words) took 0.2s, 201140 effective words/s\n",
      "INFO - 11:09:14: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:09:14: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:09:14: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:09:14: EPOCH - 12 : training on 115658 raw words (31160 effective words) took 0.1s, 248349 effective words/s\n",
      "INFO - 11:09:15: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:09:15: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:09:15: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:09:15: EPOCH - 13 : training on 115658 raw words (31232 effective words) took 0.2s, 191832 effective words/s\n",
      "INFO - 11:09:15: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:09:15: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:09:15: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:09:15: EPOCH - 14 : training on 115658 raw words (30948 effective words) took 0.1s, 254319 effective words/s\n",
      "INFO - 11:09:15: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:09:15: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:09:15: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:09:15: EPOCH - 15 : training on 115658 raw words (30907 effective words) took 0.1s, 270801 effective words/s\n",
      "INFO - 11:09:15: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:09:15: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:09:15: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:09:15: EPOCH - 16 : training on 115658 raw words (30979 effective words) took 0.1s, 222356 effective words/s\n",
      "INFO - 11:09:15: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:09:15: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:09:15: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:09:15: EPOCH - 17 : training on 115658 raw words (31147 effective words) took 0.1s, 250300 effective words/s\n",
      "INFO - 11:09:15: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:09:15: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:09:15: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:09:15: EPOCH - 18 : training on 115658 raw words (31082 effective words) took 0.2s, 202479 effective words/s\n",
      "INFO - 11:09:16: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:09:16: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:09:16: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:09:16: EPOCH - 19 : training on 115658 raw words (31186 effective words) took 0.1s, 255887 effective words/s\n",
      "INFO - 11:09:16: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:09:16: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:09:16: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:09:16: EPOCH - 20 : training on 115658 raw words (31180 effective words) took 0.1s, 226263 effective words/s\n",
      "INFO - 11:09:16: Word2Vec lifecycle event {'msg': 'training on 2313160 raw words (621062 effective words) took 3.0s, 204622 effective words/s', 'datetime': '2021-07-08T11:09:16.192138', 'gensim': '4.0.1', 'python': '3.9.1 (tags/v3.9.1:1e5d33e, Dec  7 2020, 17:08:21) [MSC v.1927 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'train'}\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(621062, 2313160)"
      ]
     },
     "metadata": {},
     "execution_count": 107
    }
   ],
   "source": [
    "w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=20, report_delay=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('elevate', 0.5828986763954163),\n",
       " ('impeccable', 0.5804067254066467),\n",
       " ('occasion', 0.5768791437149048),\n",
       " ('elegant', 0.5364821553230286),\n",
       " ('pare', 0.5162294507026672),\n",
       " ('business', 0.49626004695892334),\n",
       " ('formal', 0.49442365765571594),\n",
       " ('workwear', 0.4815402626991272),\n",
       " ('effortless', 0.471557080745697), \n",
       " ('italy', 0.4412034749984741)]"
      ]
     },
     "metadata": {},
     "execution_count": 108
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"boss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('smart_casual', 0.653635573387146),\n",
       " ('bold', 0.5222806334495544),\n",
       " ('red', 0.48366621136665344),\n",
       " ('iconic', 0.4434213638305664),\n",
       " ('urban', 0.43985819816589355),\n",
       " ('contemporary', 0.43972058215141296),\n",
       " ('chunky', 0.43887126445770264),\n",
       " ('trend', 0.4271756708621979),\n",
       " ('black', 0.4056681990623474),\n",
       " ('weekend', 0.42688432335853577)]"
      ]
     },
     "metadata": {},
     "execution_count": 109
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"hugo\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('crisp', 0.5842987895011902),\n",
       " ('elevate', 0.5828986763954163),\n",
       " ('impeccable', 0.5804067254066467),\n",
       " ('occasion', 0.5768791437149048),\n",
       " ('elegant', 0.5364821553230286),\n",
       " ('pare', 0.5162294507026672),\n",
       " ('business', 0.49626004695892334),\n",
       " ('formal', 0.49442365765571594),\n",
       " ('workwear', 0.4815402626991272),\n",
       " ('effortless', 0.471557080745697)]"
      ]
     },
     "metadata": {},
     "execution_count": 112
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"professional\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('duty', 0.5042375326156616),\n",
       " ('foundation', 0.48831605911254883),\n",
       " ('give', 0.4623507857322693),\n",
       " ('smart_casual', 0.453635573387146),\n",
       " ('contemporary', 0.45182058215141296),\n",
       " ('lay', 0.4511401355266571),\n",
       " ('versatile', 0.4445495903491974),\n",
       " ('new_season', 0.44348201155662537),\n",
       " ('crew_neckline', 0.4307824969291687),\n",
       " ('weekend', 0.42688432335853577)]"
      ]
     },
     "metadata": {},
     "execution_count": 113
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"casual\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'w2v_model' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-7552d4f405a2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mw2v_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"casual\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'w2v_model' is not defined"
     ]
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"casual\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}