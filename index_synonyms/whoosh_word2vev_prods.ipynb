{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit ('.venv': venv)"
  },
  "interpreter": {
   "hash": "3fa4fbff26baf5372c1227c893813242fd1c5dcfd93352f945b61348dd56c099"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Imports"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# index creation\r\n",
    "from whoosh.fields import Schema, TEXT, KEYWORD, ID, STORED, DATETIME\r\n",
    "from whoosh.index import create_in\r\n",
    "from whoosh.analysis import StemmingAnalyzer, NgramFilter, StopFilter\r\n",
    "from whoosh.qparser import MultifieldParser\r\n",
    "from whoosh import scoring\r\n",
    "import whoosh\r\n",
    "\r\n",
    "# data import\r\n",
    "import json\r\n",
    "\r\n",
    "from gensim.models import KeyedVectors\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "# preprocessing\r\n",
    "from nltk.tokenize import word_tokenize\r\n",
    "from nltk.tag import pos_tag\r\n",
    "from nltk.corpus import wordnet as wn"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Import pretrained word embeddings (Google's Word2Vec)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "word2vec_model_path = 'E:/Users/Lucas xD/Downloads/GoogleNews-vectors-negative300.bin'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "from gensim.models import KeyedVectors\r\n",
    "\r\n",
    "try: # nich mehrmals in Speicher laden... sind 3gb\r\n",
    "    model\r\n",
    "except NameError:\r\n",
    "    model = KeyedVectors.load_word2vec_format(word2vec_model_path, binary=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "model.most_similar('suit')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('suits', 0.7935055494308472),\n",
       " ('lawsuit', 0.7260671854019165),\n",
       " ('Suit', 0.5952914357185364),\n",
       " ('Atta_chakki_delivery', 0.5889373421669006),\n",
       " ('lawsuits', 0.5749934315681458),\n",
       " ('countersuit', 0.5632992386817932),\n",
       " ('polka_dot_clown', 0.5329123735427856),\n",
       " ('His_showmanship_rhinestone', 0.5297678709030151),\n",
       " ('complaint', 0.5216640830039978),\n",
       " ('lawsuit_alleging', 0.514050304889679)]"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Synonyms generation with a dictionary (WordNet)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# nltk.download('punkt')\r\n",
    "# nltk.download('averaged_perceptron_tagger')\r\n",
    "\r\n",
    "def tag(sentence):\r\n",
    "  words = word_tokenize(sentence)\r\n",
    "  words = pos_tag(words)\r\n",
    "  return words\r\n",
    "\r\n",
    "def paraphraseable(tag):\r\n",
    "    return tag.startswith('NN')\r\n",
    "#  return tag.startswith('NN') or tag == 'VB' or tag.startswith('JJ')\r\n",
    "\r\n",
    "def pos(tag):\r\n",
    " if tag.startswith('NN'):\r\n",
    "  return wn.NOUN\r\n",
    " elif tag.startswith('V'):\r\n",
    "  return wn.VERB\r\n",
    "\r\n",
    "#################\r\n",
    "# Synonym words #\r\n",
    "#################\r\n",
    "\r\n",
    "def synonyms(word, tag):\r\n",
    "    synsets = wn.synsets(word, pos(tag))\r\n",
    "    if len(synsets) > 1:\r\n",
    "          first_syn = synsets[0] # only take closed match (frequent synonyms)\r\n",
    "          lemma_lists = first_syn.lemmas()\r\n",
    "          lemmas = [lemma.name() for lemma in lemma_lists if lemma.name() != word]\r\n",
    "    else:\r\n",
    "          return []\r\n",
    "    return lemmas\r\n",
    "\r\n",
    "def synonymIfExists(sentence):\r\n",
    " for (word, t) in tag(sentence):\r\n",
    "   if paraphraseable(t):\r\n",
    "    syns = synonyms(word, t)\r\n",
    "    if syns:\r\n",
    "     if len(syns) >= 1:\r\n",
    "      yield [word, syns[0]] # keep only one\r\n",
    "      continue\r\n",
    "   yield [word]\r\n",
    "\r\n",
    "def get_synonyms(sentence):\r\n",
    "  words = [word for word in synonymIfExists(sentence)]\r\n",
    "  single_list = [item for sublist in words for item in sublist]\r\n",
    " \r\n",
    "  return single_list\r\n",
    "\r\n",
    "#################\r\n",
    "# Similar words #\r\n",
    "#################\r\n",
    "\r\n",
    "def sim_words(word):\r\n",
    "    sim_words_arr = []\r\n",
    "    try:\r\n",
    "      words = model.most_similar(word) # pretrained or custom w2v model <---- evtl. beide dazunehmen\r\n",
    "    except KeyError: # no similar words found\r\n",
    "      return sim_words_arr\r\n",
    "    for word in words:\r\n",
    "        if word[1] > 0.75: # cosine similarity must be at least 75% \r\n",
    "            sim_words_arr.append(word[0])\r\n",
    "\r\n",
    "    return sim_words_arr\r\n",
    "\r\n",
    "def similarIfExists(sentence):\r\n",
    "  for (word, t) in tag(sentence):\r\n",
    "   if paraphraseable(t):\r\n",
    "    sim = sim_words(word)\r\n",
    "    if sim:\r\n",
    "     if len(sim) >= 1:\r\n",
    "      yield [word, sim[0]] # keep only one\r\n",
    "      continue\r\n",
    "   yield [word]\r\n",
    "\r\n",
    "def get_similar_words(sentence):\r\n",
    "    words = [word for word in similarIfExists(sentence)]\r\n",
    "    single_list = [item for sublist in words for item in sublist]\r\n",
    "    return single_list\r\n",
    "\r\n",
    "def add_syns_and_similar_words(sentence):\r\n",
    "      syns = get_synonyms(sentence)\r\n",
    "      sims = get_similar_words(sentence)\r\n",
    "      result = syns + sims\r\n",
    "      \r\n",
    "      result = ','.join(set(result))\r\n",
    "      return result\r\n",
    "      \r\n",
    "\r\n",
    "print(get_synonyms(\"casual winter cloth\"))\r\n",
    "print(get_synonyms(\"fitting necktie for a casual meeting\"))\r\n",
    "print(get_synonyms(\"summer T-shirt\"))\r\n",
    "print(get_synonyms(\"elastic trouser\"))\r\n",
    "print(get_synonyms(\"pretty little skirt\"))\r\n",
    "\r\n",
    "print(get_similar_words(\"pretty little skirt\"))\r\n",
    "print(get_similar_words(\"casual winter cloth\"))\r\n",
    "print(get_similar_words(\"fitting necktie for a casual meeting\"))\r\n",
    "print(get_similar_words(\"summer T-shirt\"))\r\n",
    "\r\n",
    "print(add_syns_and_similar_words(\"elastic trouser\"))\r\n",
    "print(add_syns_and_similar_words(\"pretty little skirt\"))\r\n",
    "print(add_syns_and_similar_words(\"casual winter cloth\"))\r\n",
    "print(add_syns_and_similar_words(\"fitting necktie for a casual meeting\"))\r\n",
    "print(add_syns_and_similar_words(\"summer T-shirt\"))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['casual', 'winter', 'cloth']\n",
      "['fitting', 'necktie', 'for', 'a', 'casual', 'meeting', 'group_meeting']\n",
      "['summer', 'summertime', 'T-shirt']\n",
      "['elastic', 'trouser', 'pant']\n",
      "['pretty', 'little', 'skirt']\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['pretty', 'little', 'skirt', 'skirts']\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['casual', 'winter', 'cloth']\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['fitting', 'necktie', 'for', 'a', 'casual', 'meeting', 'meetings']\n",
      "['summer', 'spring', 'T-shirt']\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "elastic,trouser,pant\n",
      "little,skirt,pretty,skirts\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "winter,cloth,casual\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "for,casual,group_meeting,fitting,meeting,meetings,a,necktie\n",
      "summer,spring,T-shirt,summertime"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "def preprocess_query(search_term):\r\n",
    "        stem = StemmingAnalyzer(stoplist=frozenset([\r\n",
    "            'and', 'is', 'it', 'an', 'as', 'at', 'have', 'in', 'yet', 'if',\r\n",
    "            'from', 'for', 'when', 'by', 'to', 'you', 'be', 'we', 'that',\r\n",
    "            'may', 'not', 'with', 'tbd', 'a', 'on', 'your', 'this', 'of', 'us',\r\n",
    "            'will', 'can', 'the', 'or', 'are'\r\n",
    "        ]),\r\n",
    "                                minsize=3)\r\n",
    "        return [token.text for token in stem(search_term)]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "print(','.join(preprocess_query('fitting,necktie,tie,for,a,casual,occasional,meeting,coming_together')))\r\n",
    "print(','.join(preprocess_query('elastic,pliant,trouser,pant')))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training word embeddings with product data (Tensorflow, Keras)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "source": [
    "import io\r\n",
    "import re\r\n",
    "import string\r\n",
    "import tensorflow as tf\r\n",
    "import tqdm\r\n",
    "\r\n",
    "from tensorflow.keras import Model\r\n",
    "from tensorflow.keras.layers import Dot, Embedding, Flatten\r\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\r\n",
    "\r\n",
    "import numpy as np"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testing concepts"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "source": [
    "sentence = \"The wide road shimmered in the hot sun\"\r\n",
    "tokens = list(sentence.lower().split())\r\n",
    "print(len(tokens))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "8\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "source": [
    "vocab, index = {}, 1  # start indexing from 1\r\n",
    "vocab['<pad>'] = 0  # add a padding token\r\n",
    "for token in tokens:\r\n",
    "  if token not in vocab:\r\n",
    "    vocab[token] = index\r\n",
    "    index += 1\r\n",
    "vocab_size = len(vocab)\r\n",
    "print(vocab)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'<pad>': 0, 'the': 1, 'wide': 2, 'road': 3, 'shimmered': 4, 'in': 5, 'hot': 6, 'sun': 7}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "source": [
    "inverse_vocab = {index: token for token, index in vocab.items()}\r\n",
    "print(inverse_vocab)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{0: '<pad>', 1: 'the', 2: 'wide', 3: 'road', 4: 'shimmered', 5: 'in', 6: 'hot', 7: 'sun'}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "source": [
    "example_sequence = [vocab[word] for word in tokens]\r\n",
    "print(example_sequence)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1, 2, 3, 4, 5, 1, 6, 7]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "source": [
    "window_size = 2\r\n",
    "positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\r\n",
    "      example_sequence,\r\n",
    "      vocabulary_size=vocab_size,\r\n",
    "      window_size=window_size,\r\n",
    "      negative_samples=0)\r\n",
    "print(len(positive_skip_grams))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "26\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "source": [
    "for target, context in positive_skip_grams[:5]:\r\n",
    "  print(f\"({target}, {context}): ({inverse_vocab[target]}, {inverse_vocab[context]})\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(4, 1): (shimmered, the)\n",
      "(5, 6): (in, hot)\n",
      "(5, 3): (in, road)\n",
      "(5, 1): (in, the)\n",
      "(3, 5): (road, in)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "source": [
    "# Get target and context words for one positive skip-gram.\r\n",
    "target_word, context_word = positive_skip_grams[0]\r\n",
    "\r\n",
    "# Set the number of negative samples per positive context.\r\n",
    "num_ns = 4\r\n",
    "\r\n",
    "context_class = tf.reshape(tf.constant(context_word, dtype=\"int64\"), (1, 1))\r\n",
    "negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\r\n",
    "    true_classes=context_class,  # class that should be sampled as 'positive'\r\n",
    "    num_true=1,  # each positive skip-gram has 1 positive context class\r\n",
    "    num_sampled=num_ns,  # number of negative context words to sample\r\n",
    "    unique=True,  # all the negative samples should be unique\r\n",
    "    range_max=vocab_size,  # pick index of the samples from [0, vocab_size]\r\n",
    "    seed=4,  # seed for reproducibility\r\n",
    "    name=\"negative_sampling\"  # name of this operation\r\n",
    ")\r\n",
    "print(negative_sampling_candidates)\r\n",
    "print([inverse_vocab[index.numpy()] for index in negative_sampling_candidates])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tf.Tensor([3 7 0 4], shape=(4,), dtype=int64)\n",
      "['road', 'sun', '<pad>', 'shimmered']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "source": [
    "# Add a dimension so you can use concatenation (on the next step).\r\n",
    "negative_sampling_candidates = tf.expand_dims(negative_sampling_candidates, 1)\r\n",
    "\r\n",
    "# Concat positive context word with negative sampled words.\r\n",
    "context = tf.concat([context_class, negative_sampling_candidates], 0)\r\n",
    "\r\n",
    "# Label first context word as 1 (positive) followed by num_ns 0s (negative).\r\n",
    "label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\r\n",
    "\r\n",
    "# Reshape target to shape (1,) and context and label to (num_ns+1,).\r\n",
    "target = tf.squeeze(target_word)\r\n",
    "context = tf.squeeze(context)\r\n",
    "label = tf.squeeze(label)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "source": [
    "print(f\"target_index    : {target}\")\r\n",
    "print(f\"target_word     : {inverse_vocab[target_word]}\")\r\n",
    "print(f\"context_indices : {context}\")\r\n",
    "print(f\"context_words   : {[inverse_vocab[c.numpy()] for c in context]}\")\r\n",
    "print(f\"label           : {label}\")\r\n",
    "\r\n",
    "print(\"target  :\", target)\r\n",
    "print(\"context :\", context)\r\n",
    "print(\"label   :\", label)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "target_index    : 4\n",
      "target_word     : shimmered\n",
      "context_indices : [1 3 7 0 4]\n",
      "context_words   : ['the', 'road', 'sun', '<pad>', 'shimmered']\n",
      "label           : [1 0 0 0 0]\n",
      "target  : tf.Tensor(4, shape=(), dtype=int32)\n",
      "context : tf.Tensor([1 3 7 0 4], shape=(5,), dtype=int64)\n",
      "label   : tf.Tensor([1 0 0 0 0], shape=(5,), dtype=int64)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import training data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "source": [
    "import json\r\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "source": [
    "with open(r\"E:\\Users\\Lucas xD\\Downloads\\Products_Q_US_edited.json\", encoding=\"utf8\") as json_file:\r\n",
    "    data = json.load(json_file)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "source": [
    "df = pd.json_normalize(data)\r\n",
    "df.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  brand                                             colors gender  \\\n",
       "0  HUGO  [{'code': '001', 'name': 'Black', 'link': 'htt...    Men   \n",
       "1  BOSS  [{'code': '001', 'name': 'Black', 'link': 'htt...    Men   \n",
       "2  BOSS  [{'code': '001', 'name': 'Black', 'link': 'htt...    Men   \n",
       "3  BOSS  [{'code': '001', 'name': 'Black', 'link': 'htt...    Men   \n",
       "4  BOSS  [{'code': '410', 'name': 'Dark Blue', 'link': ...    Men   \n",
       "\n",
       "                                     longDescription  \\\n",
       "0  V-neck Solid Small embossed tonal logo detail ...   \n",
       "1  Cow skin Basket weave printed texture Tonal sq...   \n",
       "2  Lace-up Derby Italian leather upper Leather li...   \n",
       "3  A regular fit tuxedo in virgin wool.  <b>Jacke...   \n",
       "4  A slim-fit suit made from Italian Super 110s v...   \n",
       "\n",
       "                                               name     productId  \\\n",
       "0          Stretch Cotton V-Neck T-Shirt | Dredosos  hbna50261022   \n",
       "1                 Leather belt with embossed detail  hbna50262032   \n",
       "2         Italian Leather Derby Dress Shoe | Prindo  hbna50263062   \n",
       "3   Virgin Wool Tuxedo, Regular Fit | Stars/Glamour  hbna50194045   \n",
       "4  Italian Virgin Wool Suit, Slim Fit | Huge/Genius  hbna50263213   \n",
       "\n",
       "                                    shortDescription  \\\n",
       "0  This t-shirt by HUGO is crafted from cotton wi...   \n",
       "1  Upgrade your everyday collection with this tim...   \n",
       "2  Crafted from fine Italian calfskin with a prin...   \n",
       "3  This regular fit tuxedo by BOSS is crafted in ...   \n",
       "4  Our best-selling suit just got better with an ...   \n",
       "\n",
       "                                               sizes     styleName  \\\n",
       "0                             [XS, L, M, S, XL, XXL]                 \n",
       "1                   [34, 38, 40, 42, 44, 30, 32, 36]                 \n",
       "2  [6.5, 9, 11, 11.5, 12, 7, 7.5, 8, 8.5, 9.5, 10...    DressShoes   \n",
       "3  [34R, 42L, 44L, 46L, 48L, 36R, 36S, 38R, 38S, ...       Tuxedos   \n",
       "4  [44S, 48L, 42L, 44L, 34R, 36R, 38R, 40R, 40S, ...  Professional   \n",
       "\n",
       "                                            variants  \\\n",
       "0  [{'careInstructionCodes': ['000087', 'B1', 'C0...   \n",
       "1  [{'careInstructionCodes': [], 'colorCode': '00...   \n",
       "2  [{'careInstructionCodes': [], 'colorCode': '00...   \n",
       "3  [{'careInstructionCodes': ['00', 'B1', 'C0', '...   \n",
       "4  [{'careInstructionCodes': ['00', 'B1', 'C0', '...   \n",
       "\n",
       "                                               image   id  \n",
       "0  https://images.hugoboss.com/is/image/boss/hbna...  0.0  \n",
       "1  https://images.hugoboss.com/is/image/boss/hbna...  1.0  \n",
       "2  https://images.hugoboss.com/is/image/boss/hbna...  2.0  \n",
       "3  https://images.hugoboss.com/is/image/boss/hbna...  3.0  \n",
       "4  https://images.hugoboss.com/is/image/boss/hbna...  4.0  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brand</th>\n",
       "      <th>colors</th>\n",
       "      <th>gender</th>\n",
       "      <th>longDescription</th>\n",
       "      <th>name</th>\n",
       "      <th>productId</th>\n",
       "      <th>shortDescription</th>\n",
       "      <th>sizes</th>\n",
       "      <th>styleName</th>\n",
       "      <th>variants</th>\n",
       "      <th>image</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HUGO</td>\n",
       "      <td>[{'code': '001', 'name': 'Black', 'link': 'htt...</td>\n",
       "      <td>Men</td>\n",
       "      <td>V-neck Solid Small embossed tonal logo detail ...</td>\n",
       "      <td>Stretch Cotton V-Neck T-Shirt | Dredosos</td>\n",
       "      <td>hbna50261022</td>\n",
       "      <td>This t-shirt by HUGO is crafted from cotton wi...</td>\n",
       "      <td>[XS, L, M, S, XL, XXL]</td>\n",
       "      <td></td>\n",
       "      <td>[{'careInstructionCodes': ['000087', 'B1', 'C0...</td>\n",
       "      <td>https://images.hugoboss.com/is/image/boss/hbna...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BOSS</td>\n",
       "      <td>[{'code': '001', 'name': 'Black', 'link': 'htt...</td>\n",
       "      <td>Men</td>\n",
       "      <td>Cow skin Basket weave printed texture Tonal sq...</td>\n",
       "      <td>Leather belt with embossed detail</td>\n",
       "      <td>hbna50262032</td>\n",
       "      <td>Upgrade your everyday collection with this tim...</td>\n",
       "      <td>[34, 38, 40, 42, 44, 30, 32, 36]</td>\n",
       "      <td></td>\n",
       "      <td>[{'careInstructionCodes': [], 'colorCode': '00...</td>\n",
       "      <td>https://images.hugoboss.com/is/image/boss/hbna...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BOSS</td>\n",
       "      <td>[{'code': '001', 'name': 'Black', 'link': 'htt...</td>\n",
       "      <td>Men</td>\n",
       "      <td>Lace-up Derby Italian leather upper Leather li...</td>\n",
       "      <td>Italian Leather Derby Dress Shoe | Prindo</td>\n",
       "      <td>hbna50263062</td>\n",
       "      <td>Crafted from fine Italian calfskin with a prin...</td>\n",
       "      <td>[6.5, 9, 11, 11.5, 12, 7, 7.5, 8, 8.5, 9.5, 10...</td>\n",
       "      <td>DressShoes</td>\n",
       "      <td>[{'careInstructionCodes': [], 'colorCode': '00...</td>\n",
       "      <td>https://images.hugoboss.com/is/image/boss/hbna...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BOSS</td>\n",
       "      <td>[{'code': '001', 'name': 'Black', 'link': 'htt...</td>\n",
       "      <td>Men</td>\n",
       "      <td>A regular fit tuxedo in virgin wool.  &lt;b&gt;Jacke...</td>\n",
       "      <td>Virgin Wool Tuxedo, Regular Fit | Stars/Glamour</td>\n",
       "      <td>hbna50194045</td>\n",
       "      <td>This regular fit tuxedo by BOSS is crafted in ...</td>\n",
       "      <td>[34R, 42L, 44L, 46L, 48L, 36R, 36S, 38R, 38S, ...</td>\n",
       "      <td>Tuxedos</td>\n",
       "      <td>[{'careInstructionCodes': ['00', 'B1', 'C0', '...</td>\n",
       "      <td>https://images.hugoboss.com/is/image/boss/hbna...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BOSS</td>\n",
       "      <td>[{'code': '410', 'name': 'Dark Blue', 'link': ...</td>\n",
       "      <td>Men</td>\n",
       "      <td>A slim-fit suit made from Italian Super 110s v...</td>\n",
       "      <td>Italian Virgin Wool Suit, Slim Fit | Huge/Genius</td>\n",
       "      <td>hbna50263213</td>\n",
       "      <td>Our best-selling suit just got better with an ...</td>\n",
       "      <td>[44S, 48L, 42L, 44L, 34R, 36R, 38R, 40R, 40S, ...</td>\n",
       "      <td>Professional</td>\n",
       "      <td>[{'careInstructionCodes': ['00', 'B1', 'C0', '...</td>\n",
       "      <td>https://images.hugoboss.com/is/image/boss/hbna...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 300
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "source": [
    "df.info()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2777 entries, 0 to 2776\n",
      "Data columns (total 12 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   brand             2777 non-null   object \n",
      " 1   colors            2777 non-null   object \n",
      " 2   gender            2777 non-null   object \n",
      " 3   longDescription   2777 non-null   object \n",
      " 4   name              2777 non-null   object \n",
      " 5   productId         2777 non-null   object \n",
      " 6   shortDescription  2777 non-null   object \n",
      " 7   sizes             2777 non-null   object \n",
      " 8   styleName         2777 non-null   object \n",
      " 9   variants          2777 non-null   object \n",
      " 10  image             2777 non-null   object \n",
      " 11  id                2777 non-null   float64\n",
      "dtypes: float64(1), object(11)\n",
      "memory usage: 260.5+ KB\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "source": [
    "df = df.drop(columns=['brand', 'colors', 'gender', 'productId', 'sizes', 'styleName', 'variants', 'image', 'id'])\r\n",
    "df.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                     longDescription  \\\n",
       "0  V-neck Solid Small embossed tonal logo detail ...   \n",
       "1  Cow skin Basket weave printed texture Tonal sq...   \n",
       "2  Lace-up Derby Italian leather upper Leather li...   \n",
       "3  A regular fit tuxedo in virgin wool.  <b>Jacke...   \n",
       "4  A slim-fit suit made from Italian Super 110s v...   \n",
       "\n",
       "                                               name  \\\n",
       "0          Stretch Cotton V-Neck T-Shirt | Dredosos   \n",
       "1                 Leather belt with embossed detail   \n",
       "2         Italian Leather Derby Dress Shoe | Prindo   \n",
       "3   Virgin Wool Tuxedo, Regular Fit | Stars/Glamour   \n",
       "4  Italian Virgin Wool Suit, Slim Fit | Huge/Genius   \n",
       "\n",
       "                                    shortDescription  \n",
       "0  This t-shirt by HUGO is crafted from cotton wi...  \n",
       "1  Upgrade your everyday collection with this tim...  \n",
       "2  Crafted from fine Italian calfskin with a prin...  \n",
       "3  This regular fit tuxedo by BOSS is crafted in ...  \n",
       "4  Our best-selling suit just got better with an ...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longDescription</th>\n",
       "      <th>name</th>\n",
       "      <th>shortDescription</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>V-neck Solid Small embossed tonal logo detail ...</td>\n",
       "      <td>Stretch Cotton V-Neck T-Shirt | Dredosos</td>\n",
       "      <td>This t-shirt by HUGO is crafted from cotton wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cow skin Basket weave printed texture Tonal sq...</td>\n",
       "      <td>Leather belt with embossed detail</td>\n",
       "      <td>Upgrade your everyday collection with this tim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lace-up Derby Italian leather upper Leather li...</td>\n",
       "      <td>Italian Leather Derby Dress Shoe | Prindo</td>\n",
       "      <td>Crafted from fine Italian calfskin with a prin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A regular fit tuxedo in virgin wool.  &lt;b&gt;Jacke...</td>\n",
       "      <td>Virgin Wool Tuxedo, Regular Fit | Stars/Glamour</td>\n",
       "      <td>This regular fit tuxedo by BOSS is crafted in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A slim-fit suit made from Italian Super 110s v...</td>\n",
       "      <td>Italian Virgin Wool Suit, Slim Fit | Huge/Genius</td>\n",
       "      <td>Our best-selling suit just got better with an ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 302
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "source": [
    "df = df.stack().reset_index() # all in one column"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "source": [
    "df = df.drop(columns=['level_0', 'level_1'])\r\n",
    "df.columns = ['sentences']\r\n",
    "df.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                           sentences\n",
       "0  V-neck Solid Small embossed tonal logo detail ...\n",
       "1           Stretch Cotton V-Neck T-Shirt | Dredosos\n",
       "2  This t-shirt by HUGO is crafted from cotton wi...\n",
       "3  Cow skin Basket weave printed texture Tonal sq...\n",
       "4                  Leather belt with embossed detail"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>V-neck Solid Small embossed tonal logo detail ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Stretch Cotton V-Neck T-Shirt | Dredosos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This t-shirt by HUGO is crafted from cotton wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cow skin Basket weave printed texture Tonal sq...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Leather belt with embossed detail</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 304
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "source": [
    "def pre_process(text):\r\n",
    "    # lowercase\r\n",
    "    text=text.lower()\r\n",
    "    \r\n",
    "    #remove tags\r\n",
    "    text=re.sub(\"</?.*?>\",\" <> \",text)\r\n",
    "    \r\n",
    "    # remove special characters and digits\r\n",
    "    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\r\n",
    "    \r\n",
    "    return text"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "source": [
    "df['sentences'] = df['sentences'].apply(lambda x:pre_process(x))\r\n",
    "df.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                           sentences\n",
       "0  v neck solid small embossed tonal logo detail ...\n",
       "1             stretch cotton v neck t shirt dredosos\n",
       "2  this t shirt by hugo is crafted from cotton wi...\n",
       "3  cow skin basket weave printed texture tonal sq...\n",
       "4                  leather belt with embossed detail"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>v neck solid small embossed tonal logo detail ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>stretch cotton v neck t shirt dredosos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>this t shirt by hugo is crafted from cotton wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cow skin basket weave printed texture tonal sq...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>leather belt with embossed detail</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 379
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "source": [
    "sentences_arr = df['sentences'].tolist()\r\n",
    "out = io.open('product_text.txt', 'w', encoding='utf-8')\r\n",
    "\r\n",
    "for sentence in sentences_arr:\r\n",
    "  out.write(sentence + \"\\n\")\r\n",
    "out.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocessing in one function"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "source": [
    "# Generates skip-gram pairs with negative sampling for a list of sequences\r\n",
    "# (int-encoded sentences) based on window size, number of negative samples\r\n",
    "# and vocabulary size.\r\n",
    "def generate_training_data(sequences, window_size, num_ns, vocab_size, seed):\r\n",
    "  # Elements of each training example are appended to these lists.\r\n",
    "  targets, contexts, labels = [], [], []\r\n",
    "\r\n",
    "  # Build the sampling table for vocab_size tokens.\r\n",
    "  sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\r\n",
    "\r\n",
    "  # Iterate over all sequences (sentences) in dataset.\r\n",
    "  for sequence in tqdm.tqdm(sequences):\r\n",
    "\r\n",
    "    # Generate positive skip-gram pairs for a sequence (sentence).\r\n",
    "    positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\r\n",
    "          sequence,\r\n",
    "          vocabulary_size=vocab_size,\r\n",
    "          sampling_table=sampling_table,\r\n",
    "          window_size=window_size,\r\n",
    "          negative_samples=0)\r\n",
    "\r\n",
    "    # Iterate over each positive skip-gram pair to produce training examples\r\n",
    "    # with positive context word and negative samples.\r\n",
    "    for target_word, context_word in positive_skip_grams:\r\n",
    "      context_class = tf.expand_dims(\r\n",
    "          tf.constant([context_word], dtype=\"int64\"), 1)\r\n",
    "      negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\r\n",
    "          true_classes=context_class,\r\n",
    "          num_true=1,\r\n",
    "          num_sampled=num_ns,\r\n",
    "          unique=True,\r\n",
    "          range_max=vocab_size,\r\n",
    "          seed=seed,\r\n",
    "          name=\"negative_sampling\")\r\n",
    "\r\n",
    "      # Build context and label vectors (for one target word)\r\n",
    "      negative_sampling_candidates = tf.expand_dims(\r\n",
    "          negative_sampling_candidates, 1)\r\n",
    "\r\n",
    "      context = tf.concat([context_class, negative_sampling_candidates], 0)\r\n",
    "      label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\r\n",
    "\r\n",
    "      # Append each element from the training example to global lists.\r\n",
    "      targets.append(target_word)\r\n",
    "      contexts.append(context)\r\n",
    "      labels.append(label)\r\n",
    "\r\n",
    "  return targets, contexts, labels\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "source": [
    "# path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\r\n",
    "path_to_file = r\"C:\\Users\\lucas\\Documents\\Github\\BA_Jupyter_Notebook\\index_synonyms\\product_text.txt\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "source": [
    "with open(path_to_file, encoding=\"utf8\") as f: \r\n",
    "  lines = f.read().splitlines()\r\n",
    "for line in lines[:5]:\r\n",
    "  print(line)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "v neck solid small embossed tonal logo detail above hem stretch cotton\n",
      "stretch cotton v neck t shirt dredosos\n",
      "this t shirt by hugo is crafted from cotton with a hint of stretch for comfort a small embossed tonal logo detail above the hem completes this v neck style \n",
      "cow skin basket weave printed texture tonal square pin buckle logo printed at buckle width in cm \n",
      "leather belt with embossed detail\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "source": [
    "text_ds = tf.data.TextLineDataset(path_to_file).filter(lambda x: tf.cast(tf.strings.length(x), bool))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "source": [
    "# custom standardization function to lowercase the text and remove punctuation & numbers.\r\n",
    "def custom_standardization(input_data):\r\n",
    "  text_nonum = input_data\r\n",
    "  if isinstance(input_data, str):\r\n",
    "    text_nonum = text_nonum = re.sub(r'\\d+', '', input_data) # remove numbers\r\n",
    "  if type(text_nonum) == int or type(text_nonum) == float:\r\n",
    "        return ''\r\n",
    "  lowercase = tf.strings.lower(text_nonum)\r\n",
    "  return tf.strings.regex_replace(lowercase,\r\n",
    "                                  '[%s]' % re.escape(string.punctuation), '')\r\n",
    "\r\n",
    "\r\n",
    "# vocabulary size and number of words in a sequence.\r\n",
    "vocab_size = 4096\r\n",
    "sequence_length = 10\r\n",
    "\r\n",
    "# text vectorization layer to normalize, split, and map strings to\r\n",
    "# integers. Set output_sequence_length length to pad all samples to same length.\r\n",
    "vectorize_layer = TextVectorization(\r\n",
    "    standardize=custom_standardization,\r\n",
    "    max_tokens=vocab_size,\r\n",
    "    output_mode='int',\r\n",
    "    output_sequence_length=sequence_length)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "source": [
    "vectorize_layer.adapt(text_ds.batch(1024))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "source": [
    "# Save the created vocabulary for reference.\r\n",
    "inverse_vocab = vectorize_layer.get_vocabulary()\r\n",
    "print(inverse_vocab[:20])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['', '[UNK]', 'a', 'with', 'in', 'fit', 'and', 'the', 'this', 'for', 'by', 'pockets', 'boss', 'to', 'length', 'inches', 'of', 'cm', 'regular', 'slim']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "source": [
    "# Vectorize the data in text_ds.\r\n",
    "text_vector_ds = text_ds.batch(1024).prefetch(tf.data.AUTOTUNE).map(vectorize_layer).unbatch() # unbatch tut alle arr in arr \"abflachen\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "source": [
    "sequences = list(text_vector_ds.as_numpy_iterator())\r\n",
    "print(len(sequences))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "8061\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "source": [
    "for seq in sequences[:5]:\r\n",
    "  print(f\"{seq} => {[inverse_vocab[i] for i in seq]}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[ 351   59  430  810  244  226   27  166 1337  210] => ['v', 'neck', 'solid', 'small', 'embossed', 'tonal', 'logo', 'detail', 'above', 'hem']\n",
      "[  31   24  351   59   81   23 3524    0    0    0] => ['stretch', 'cotton', 'v', 'neck', 't', 'shirt', 'dredosos', '', '', '']\n",
      "[  8  81  23  10  38  20  28 102  24   3] => ['this', 't', 'shirt', 'by', 'hugo', 'is', 'crafted', 'from', 'cotton', 'with']\n",
      "[2726  797 2796  330  124  377  226  331  541  257] => ['cow', 'skin', 'basket', 'weave', 'printed', 'texture', 'tonal', 'square', 'pin', 'buckle']\n",
      "[ 41 164   3 244 166   0   0   0   0   0] => ['leather', 'belt', 'with', 'embossed', 'detail', '', '', '', '', '']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "source": [
    "targets, contexts, labels = generate_training_data(\r\n",
    "    sequences=sequences,\r\n",
    "    window_size=2,\r\n",
    "    num_ns=4,\r\n",
    "    vocab_size=vocab_size,\r\n",
    "    seed=4)\r\n",
    "print(len(targets), len(contexts), len(labels))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 8061/8061 [00:02<00:00, 3988.94it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "17791 17791 17791\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "source": [
    "BATCH_SIZE = 1024\r\n",
    "BUFFER_SIZE = 10000\r\n",
    "dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\r\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\r\n",
    "print(dataset)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<BatchDataset shapes: (((1024,), (1024, 5, 1)), (1024, 5)), types: ((tf.int32, tf.int64), tf.int64)>\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "source": [
    "dataset = dataset.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\r\n",
    "print(dataset)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<PrefetchDataset shapes: (((1024,), (1024, 5, 1)), (1024, 5)), types: ((tf.int32, tf.int64), tf.int64)>\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "source": [
    "num_ns = 4\r\n",
    "\r\n",
    "class Word2Vec(Model):\r\n",
    "  def __init__(self, vocab_size, embedding_dim):\r\n",
    "    super(Word2Vec, self).__init__()\r\n",
    "    self.target_embedding = Embedding(vocab_size,\r\n",
    "                                      embedding_dim,\r\n",
    "                                      input_length=1,\r\n",
    "                                      name=\"w2v_embedding\")\r\n",
    "    self.context_embedding = Embedding(vocab_size,\r\n",
    "                                      embedding_dim,\r\n",
    "                                      input_length=num_ns+1)\r\n",
    "    self.dots = Dot(axes=(3, 2))\r\n",
    "    self.flatten = Flatten()\r\n",
    "\r\n",
    "  def call(self, pair):\r\n",
    "    target, context = pair\r\n",
    "    word_emb = self.target_embedding(target)\r\n",
    "    context_emb = self.context_embedding(context)\r\n",
    "    dots = self.dots([context_emb, word_emb])\r\n",
    "    return self.flatten(dots)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "source": [
    "embedding_dim = 64\r\n",
    "word2vec = Word2Vec(vocab_size, embedding_dim)\r\n",
    "word2vec.compile(optimizer='adam',\r\n",
    "                 loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\r\n",
    "                 metrics=['accuracy'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "word2vec.fit(dataset, epochs=40, callbacks=[tensorboard_callback])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "source": [
    "from tensorboard.plugins import projector\r\n",
    "%tensorboard --logdir logs"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "UsageError: Line magic function `%tensorboard` not found.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "source": [
    "weights = word2vec.get_layer('w2v_embedding').get_weights()[0]\r\n",
    "vocab = vectorize_layer.get_vocabulary()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "source": [
    "out_v = io.open('vectors.tsv', 'w', encoding='utf-8')\r\n",
    "out_m = io.open('metadata.tsv', 'w', encoding='utf-8')\r\n",
    "\r\n",
    "for index, word in enumerate(vocab):\r\n",
    "  if index == 0:\r\n",
    "    continue  # skip <PAD>.\r\n",
    "  vec = weights[index]\r\n",
    "  out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\r\n",
    "  out_m.write(word + \"\\n\")\r\n",
    "out_v.close()\r\n",
    "out_m.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "source": [
    "index2word = dict(enumerate(list(inverse_vocab)))\r\n",
    "word2index = {v: k for k, v in index2word.items()}\r\n",
    "\r\n",
    "text_vector_ds"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<_UnbatchDataset shapes: (10,), types: tf.int64>"
      ]
     },
     "metadata": {},
     "execution_count": 399
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "source": [
    "def find_closest(embeds, word, n=3): # n is for \"n closest words\"\r\n",
    "  n = n + 1 # This is becuse the most similar word is definatly that word itself. like the most similar word for \"apple\" is \"apple\". so we should look for top n+1 words\r\n",
    "  main_vec = embeds(word2index[word])\r\n",
    "\r\n",
    "  similarities = -tf.keras.losses.cosine_similarity(embeds.embeddings, main_vec)\r\n",
    "  top_n = tf.math.top_k(similarities, n).indices\r\n",
    "  words = [index2word[i] for i in top_n.numpy()]\r\n",
    "\r\n",
    "  return words[0:], tf.math.top_k(similarities, 3)[0][1]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "source": [
    "target = word2vec.target_embedding\r\n",
    "res, sim = find_closest(target, 'red')\r\n",
    "print('closest: ', res)\r\n",
    "print('top similarity: ', sim.numpy())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "closest:  ['red', 'velvet', 'hemp', 'fluoro']\n",
      "top similarity:  0.7950691\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "source": [
    "word2vec.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"word2_vec_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "w2v_embedding (Embedding)    multiple                  262144    \n",
      "_________________________________________________________________\n",
      "embedding_6 (Embedding)      multiple                  262144    \n",
      "_________________________________________________________________\n",
      "dot_6 (Dot)                  multiple                  0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          multiple                  0         \n",
      "=================================================================\n",
      "Total params: 524,288\n",
      "Trainable params: 524,288\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TF-IDF most common words"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocessing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "import json\r\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "def pre_process(text):\r\n",
    "    # lowercase\r\n",
    "    text=text.lower()\r\n",
    "    \r\n",
    "    #remove tags\r\n",
    "    text=re.sub(\"</?.*?>\",\" <> \",text)\r\n",
    "    \r\n",
    "    # remove special characters and digits\r\n",
    "    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\r\n",
    "\r\n",
    "    text = add_syns_and_similar_words(text)\r\n",
    "    \r\n",
    "    return text"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "source": [
    "# with open(r\"E:\\Users\\Lucas xD\\Downloads\\Products_Q_US_edited.json\", encoding=\"utf8\") as json_file:\r\n",
    "#     data = json.load(json_file)\r\n",
    "\r\n",
    "# df = pd.json_normalize(data)\r\n",
    "# df['name'] = df['name'].apply(lambda x:pre_process(x))\r\n",
    "# df.to_csv('prods_w_syn_sim.csv', index=False)\r\n",
    "# df_names.head(3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "source": [],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0     stretch cotton v neck t shirt dredosos\n",
       "1          leather belt with embossed detail\n",
       "2    italian leather derby dress shoe prindo\n",
       "Name: name, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 578
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "source": [
    "for row in df['name'].iloc[:3]:\r\n",
    "    print(row)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "volt,vs,neck,shirt,cervix,t,dredosos,shirts,cotton_fiber,v,cotton,thymine,stretch\n",
      "embossed,belt,leather,belts,detail,item,with\n",
      "dress,frock,dresses,prindo,italian,derby,shoes,shoe,leather\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### sklearn"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "source": [
    "# from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
    "\r\n",
    "# n_features = 1000\r\n",
    "\r\n",
    "# corpus = sentences_arr\r\n",
    "# vectorizer = TfidfVectorizer(max_df=0.95, min_df=2,\r\n",
    "#                             max_features=n_features, stop_words='english')\r\n",
    "# X = vectorizer.fit_transform(corpus)\r\n",
    "# print(vectorizer.get_feature_names()[:10])\r\n",
    "\r\n",
    "# print(X.shape)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['abstract', 'accent', 'accented', 'accents', 'accessories', 'accessory', 'acetate', 'active', 'adaptable', 'add']\n",
      "(8331, 1000)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create Schema with analyzers & filters (Whoosh)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "df_syn = pd.read_csv('prods_w_syn_sim.csv')\r\n",
    "df_syn.head() \r\n",
    "for row in df_syn['name'].iloc[:100]:\r\n",
    "    print(row)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "volt,vs,neck,shirt,cervix,t,dredosos,shirts,cotton_fiber,v,cotton,thymine,stretch\n",
      "embossed,belt,leather,belts,detail,item,with\n",
      "dress,frock,dresses,prindo,italian,derby,shoes,shoe,leather\n",
      "fit,wool,glamour,woolen,regular,virgin,star,tuxedo,stars\n",
      "fit,wool,slim,suits,woolen,suit_of_clothes,italian,virgin,genius,huge,suit,mastermind\n",
      "metal,check,belt,tab,detailed,leather,belts,metallic_element,connio\n",
      "shift,dress,sleeveless,frock,wool,dresses,woolen,italian,in,displacement,shifts,stretch\n",
      "wool,woolen,jackets,narrow,jacket,lapels,with,stretch\n",
      "and,gifts,trimness,gunmetal,hardware,suspenders,trim,leather,boxed,gift,with\n",
      "wool,woolen,blazer,peak,extremum,lapels,with,stretch\n",
      "curved,wool,lapels,woolen,blazer,with,stretch\n",
      "fit,soft,vs,shirt,regular,tantrum,volt,neck,cervix,t,in,shirts,cotton_fiber,v,cotton,thymine\n",
      "pants,fit,cropped,slim,trousers,bloomers,tantrum\n",
      "wool,leg,woolen,bloomers,boot,italian,virgin,pants,in,trousers,stretch\n",
      "necktie,tie,silk,italian\n",
      "shirts,dress,frock,dresses,shirt,cotton_fiber,cotton\n",
      "fit,shirt,regular,Polo,polo,fine,piqué,in,shirts,tantrum\n",
      "derbies,dress,frock,dresses,derby,shoes,shoe,leather,carmons\n",
      "fit,wool,slim,suits,woolen,suit_of_clothes,virgin,genius,huge,suit,mastermind\n",
      "necktie,tie,silk,slim,italian\n",
      "sharp,fit,wool,James,suits,woolen,suit_of_clothes,italian,regular,the,suit,james\n",
      "Jacquard,pure,pocket,silk,foursquare,in,jacquard,square\n",
      "two,belt,vegetable,tone_of_voice,in,leather,belts,tone,tanned\n",
      "bow,italian,pure,necktie,tie,silk,in,made\n",
      "metal,belt,logo,logos,leather,belts,metallic_element,senol,with\n",
      "Jacquard,pure,pocket,foursquare,in,cotton,jacquard,cotton_fiber,square\n",
      "cummerbund,bowknot,and,bow,necktie,tie,silk,set,setting\n",
      "bow,italian,pure,necktie,tie,silk,in,made\n",
      "cap,twill_weave,logo,logos,with,embroidered,twill,in,Baseball,cotton_fiber,caps,cotton,baseball,baseball_game\n",
      "black,fit,denim_jeans,denim,jean,in,jeans,slim,stay\n",
      "mercerized,fit,shirt,regular,t,in,shirts,cotton_fiber,cotton,thymine,tantrum\n",
      "belts,reversible,double,belt,leather,buckle,with\n",
      "contrary,fit,logo,regular,sweatshirt,logos,sweat_shirt,cotton_fiber,reverse,ignition_interlock,interlock,cotton,with,tantrum\n",
      "mercerized,shirt,long,sleeved,with,t,ribbing,in,shirts,cotton_fiber,cotton,thymine\n",
      "fit,hem,bloomers,hems,pants,slim,trousers,zipped,with\n",
      "fit,blend,mock,placket,blends,slim,cotton_fiber,blouse,cotton,with\n",
      "fit,darted,item,seam,slim,detail,blouse,with,tantrum\n",
      "docs,signature,s,workbag,italian,calfskin,doc,doctor\n",
      "fit,shirt,contrast,regular,with,t,direct_contrast,shirts,detail,item,thymine,tantrum\n",
      "belts,reversible,belt,leather,buckle,pin,with\n",
      "signature,bag,ns,zip,nothing,bags,journalist,calfskin,reporter\n",
      "fit,wool,skirt,woolen,pencils,skirts,slim,pencil,stretch\n",
      "signature,wallet,pebbled,calfskin,coins,coin\n",
      "clip,logo,logos,cartridge_holder,with,tie,fastening,hole,engraved,fix\n",
      "signature,aggregation,collection,billfold,wallet,in,palmellato,leather,collections\n",
      "four,smooth,slot,card,in,cards,leather,holder\n",
      "sneaker,akeen,and,leather,gym_shoe,mesh\n",
      "technical,cap,piqué,in,Baseball,caps,baseball,baseball_game\n",
      "shirts,fit,in,slim,shirt,poplin,stretch\n",
      "fit,wool,woolen,bloomers,virgin,pants,extra,in,slim,trousers\n",
      "fit,contrast,bottoms,bottom,jog,jogging,trims,trim,direct_contrast,slim,with,tantrum\n",
      "necktie,tie,fine,silk,in,jacquard,Jacquard\n",
      "organiser,traveling,organizer,signature_d,zip,nothing,travel,leather,trav\n",
      "necktie,tie,wool,slim,woolen,virgin\n",
      "delaware,fit,denim_jeans,jean,Delaware,jeans,slim,cotton_fiber,cotton,stretch\n",
      "briefs,of,pugilist,jersey,way,battalion,one,boxer,pack,three,Jockey_shorts,in,manner,heavyweight_boxer,cotton_fiber,cotton\n",
      "underwear,battalion,of,T_shirts,pack,three,pure,t,in,shirts,cotton_fiber,cotton,thymine\n",
      "signature,grained,aggregation,collection,card,in,palmellato,cards,leather,collections,holder\n",
      "fit,wool,jacket,woolen,jackets,virgin,in,slim,tantrum\n",
      "underwear,neck,battalion,of,T_shirts,cervix,pack,three,t,in,shirts,cotton_fiber,v,cotton,thymine\n",
      "briefs,battalion,of,pack,Jockey_shorts,in,cotton_fiber,triple,cotton,stretch\n",
      "battalion,of,pack,short_pants,in,cotton,cotton_fiber,triple,trunks,stretch\n",
      "briefs,battalion,of,pugilist,boxer,pack,Jockey_shorts,in,heavyweight_boxer,cotton_fiber,triple,cotton,stretch\n",
      "chinos,fit,chino,in,slim,cotton_fiber,cotton,gabardine,stretch\n",
      "chinos,fit,chino,regular,in,cotton_fiber,stretch,cotton,gabardine,tantrum\n",
      "necktie,tie,silk,stripe,band,slim,embroidered\n",
      "belts,brushed,belt,leather,gunmetal,buckle,with\n",
      "nan\n",
      "fit,denim_jeans,denim,jean,skinny,jeans,in,coldblack,stretch\n",
      "fit,wool,trouser,woolen,virgin,pants,extra,in,slim,poplin,trousers,tantrum\n",
      "fit,shirt,dry,regular,piqué,polo,Polo,quick,technologies,technology,shirts,engineering,with,tantrum\n",
      "fit,wool,woolen,bloomers,pure,virgin,pants,in,slim,trousers\n",
      "single,fit,New_Jersey,shirt,regular,jersey,t,in,shirts,jerseys,thymine,tantrum\n",
      "fit,wool,waistcoat,woolen,virgin,in,slim,tantrum\n",
      "clip,signature,aggregation,collection,leather,in,palmellato,money,collections,cartridge_holder\n",
      "silk,blend,neckline,blends,gathered,blouse,with\n",
      "skirt,pencils,madea,skirts,crepe,crepe_paper,pencil,textured\n",
      "fit,concern,shirt,shirts,in,slim,poplin,cotton_fiber,business,cotton,tantrum\n",
      "fit,wool,inside_information,jacket,woolen,jackets,virgin,silk,details,slim,with\n",
      "fit,wool,woolen,bloomers,virgin,trims,pants,silk,trim,in,slim,trousers,formal,with,tantrum\n",
      "Oxford,oxford,piping,patents,patent,place,grosgrain,shoes,neckband,leather,collar,patent_of_invention,sneakers,with\n",
      "briefs,waistband,logo,four,logos,Jockey_shorts,way,microfiber,in,manner,stretch\n",
      "sateen,fit,dress,frock,dresses,shirt,extra,in,slim,shirts,cotton_fiber,cotton\n",
      "t_test\n",
      "fit,wool,jacket,woolen,jackets,virgin,extra,in,slim,poplin,stretch,tantrum\n",
      "fit,shirt,washed,Polo,polo,piqué,in,slim,shirts,cotton_fiber,cotton\n",
      "crew,wool,neck,woolen,cervix,virgin,jumper,sweaters,in,sweater\n",
      "us,t_test\n",
      "fit,wool,stitching,woolen,jackets,regular,virgin,jacket,amf,with,tantrum\n",
      "fit,wool,coat,and,woolen,regular,cashmere\n",
      "blend,jackets,zip,nothing,blends,jacket,cotton_fiber,cotton\n",
      "fit,wool,jacket,woolen,jackets,pure,extra,in,slim,tantrum\n",
      "japple,hooded,jackets,jacket\n",
      "axial_rotation,mercerized,wool,neck,roll,woolen,cervix,jumper,sweaters,merino_wool,in,merino,sweater\n",
      "wool,leg,concern,woolen,bloomers,virgin,pants,in,straight,trousers,business\n",
      "chinos,fit,brushed,chino,casual,in,slim,cotton_fiber,cotton,stretch\n",
      "chinos,fit,brushed,chino,regular,casual,in,cotton_fiber,cotton,stretch\n",
      "fit,wool,woolen,jackets,regular,italian,in,jacket,stretch,tantrum\n",
      "fit,spread,shirt,spreading,collar,pure,neckband,in,slim,shirts,cotton_fiber,cotton,with\n",
      "pr,dress,frock,derb,dresses,italian,derby,shoes,hannover,shoe,leather,praseodymium\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "schema = Schema(brand=TEXT(),\r\n",
    "                colors=TEXT(),\r\n",
    "                gender=TEXT(),\r\n",
    "                longDescription=TEXT(analyzer=StemmingAnalyzer() | NgramFilter(minsize=2, maxsize=4) | StopFilter()),\r\n",
    "                name=TEXT(analyzer=StemmingAnalyzer() | NgramFilter(minsize=2, maxsize=4) | StopFilter(), field_boost=3.0, stored=True),\r\n",
    "                productId=TEXT(),\r\n",
    "                shortDescription=TEXT(analyzer=StemmingAnalyzer() | NgramFilter(minsize=2, maxsize=4) | StopFilter()),\r\n",
    "                sizes=TEXT(),\r\n",
    "                styleName=TEXT(),\r\n",
    "                variants=TEXT(),\r\n",
    "                image=TEXT(),\r\n",
    "                id=ID(stored=True)\r\n",
    "                )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create the index"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "index_path = './w2v_index'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "create_new_index = True\r\n",
    "if(create_new_index):\r\n",
    "    index = create_in(index_path, schema)\r\n",
    "else:    \r\n",
    "    index = whoosh.index.open_dir(index_path)\r\n",
    "\r\n",
    "writer = index.writer()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "for num, doc in df_syn.iterrows():\r\n",
    "    writer.add_document(\r\n",
    "            brand=str(doc['brand']),\r\n",
    "            colors=str(doc['colors']),\r\n",
    "            gender=str(doc['gender']),\r\n",
    "            longDescription=str(doc['longDescription']),\r\n",
    "            name=str(doc['name']),\r\n",
    "            productId=str(doc['productId']),\r\n",
    "            shortDescription=str(doc['shortDescription']),\r\n",
    "            sizes=str(doc['sizes']),\r\n",
    "            styleName=str(doc['styleName']),\r\n",
    "            variants=str(doc['variants']),\r\n",
    "            image=str(doc['image']),\r\n",
    "            id=str(doc['id'])\r\n",
    "    )\r\n",
    "         \r\n",
    "writer.commit()"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Testing the index"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "\r\n",
    "use_synonyms = True\r\n",
    "\r\n",
    "search_term = \"wintertime\"\r\n",
    "\r\n",
    "if(use_synonyms):\r\n",
    "    try:\r\n",
    "        similarity_list = model.most_similar(search_term, topn=1)\r\n",
    "        similar_words = [sim_tuple[0] for sim_tuple in similarity_list]\r\n",
    "    except KeyError:\r\n",
    "        similar_words = []\r\n",
    "    keywords = \" OR \".join([search_term] + similar_words)\r\n",
    "\r\n",
    "    print(\"Results with Word2Vec:\")\r\n",
    "    print(f\"Similar words used: {similar_words}\")\r\n",
    "    with index.searcher(weighting=scoring.TF_IDF()) as searcher:\r\n",
    "        query = MultifieldParser([\"name\", \"longDescription\", 'shortDescription'], index.schema).parse(keywords)\r\n",
    "        results = searcher.search(query)\r\n",
    "        for docnum, score in results.items():\r\n",
    "            print(docnum, score)\r\n",
    "        print(results)\r\n",
    "else:\r\n",
    "    keywords  = search_term\r\n",
    "\r\n",
    "# results = []\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "print(\"________________________\\n\")\r\n",
    "print(\"Results without Word2Vec:\")\r\n",
    "with index.searcher(weighting=scoring.TF_IDF()) as searcher:\r\n",
    "    query = MultifieldParser([\"name\", \"longDescription\", \"shortDescription\"], index.schema).parse(search_term)\r\n",
    "    results = searcher.search(query)\r\n",
    "    for docnum, score in results.items():\r\n",
    "        print(docnum, score)\r\n",
    "    for doc in results:\r\n",
    "        print(doc)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Results with Word2Vec:\n",
      "Similar words used: ['summertime']\n",
      "1344 33.30499334186929\n",
      "1871 33.30499334186929\n",
      "2057 33.30499334186929\n",
      "<Top 3 Results for Or([And([Term('name', 'wint'), Term('name', 'inte'), Term('name', 'nter'), Term('name', 'tert'), Term('name', 'erti'), Term('name', 'rtim')]), And([Term('longDescription', 'wint'), Term('longDescription', 'inte'), Term('longDescription', 'nter'), Term('longDescription', 'tert'), Term('longDescription', 'erti'), Term('longDescription', 'rtim')]), And([Term('shortDescription', 'wint'), Term('shortDescription', 'inte'), Term('shortDescription', 'nter'), Term('shortDescription', 'tert'), Term('shortDescription', 'erti'), Term('shortDescription', 'rtim')]), And([Term('name', 'summ'), Term('name', 'umme'), Term('name', 'mmer'), Term('name', 'mert'), Term('name', 'erti'), Term('name', 'rtim')]), And([Term('longDescription', 'summ'), Term('longDescription', 'umme'), Term('longDescription', 'mmer'), Term('longDescription', 'mert'), Term('longDescription', 'erti'), Term('longDescription', 'rtim')]), And([Term('shortDescription', 'summ'), Term('shortDescription', 'umme'), Term('shortDescription', 'mmer'), Term('shortDescription', 'mert'), Term('shortDescription', 'erti'), Term('shortDescription', 'rtim')])]) runtime=0.011651000000028944>\n",
      "________________________\n",
      "\n",
      "Results without Word2Vec:\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ]
}