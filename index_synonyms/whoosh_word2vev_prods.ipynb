{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit ('.venv': venv)"
  },
  "interpreter": {
   "hash": "3fa4fbff26baf5372c1227c893813242fd1c5dcfd93352f945b61348dd56c099"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Imports"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "# index creation\r\n",
    "from whoosh.fields import Schema, TEXT, KEYWORD, ID, STORED, DATETIME\r\n",
    "from whoosh.index import create_in\r\n",
    "from whoosh.analysis import StemmingAnalyzer, NgramFilter, StopFilter\r\n",
    "from whoosh.qparser import MultifieldParser\r\n",
    "from whoosh import scoring\r\n",
    "import whoosh\r\n",
    "\r\n",
    "# data import\r\n",
    "import json\r\n",
    "\r\n",
    "from gensim.models import KeyedVectors\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "# preprocessing\r\n",
    "from nltk.tokenize import word_tokenize\r\n",
    "from nltk.tag import pos_tag\r\n",
    "from nltk.corpus import wordnet as wn"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Synonyms generation with a dictionary (WordNet)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "source": [
    "# nltk.download('punkt')\r\n",
    "# nltk.download('averaged_perceptron_tagger')\r\n",
    "\r\n",
    "def tag(sentence):\r\n",
    "  words = word_tokenize(sentence)\r\n",
    "  words = pos_tag(words)\r\n",
    "  return words\r\n",
    "\r\n",
    "def paraphraseable(tag):\r\n",
    " return tag.startswith('NN') or tag == 'VB' or tag.startswith('JJ')\r\n",
    "\r\n",
    "def pos(tag):\r\n",
    " if tag.startswith('NN'):\r\n",
    "  return wn.NOUN\r\n",
    " elif tag.startswith('V'):\r\n",
    "  return wn.VERB\r\n",
    "\r\n",
    "def synonyms(word, tag):\r\n",
    "    lemma_lists = [ss.lemmas() for ss in wn.synsets(word, pos(tag))]\r\n",
    "    lemmas = [lemma.name() for lemma in sum(lemma_lists, []) if lemma.name() != word]\r\n",
    "    return set(lemmas)\r\n",
    "\r\n",
    "def synonymIfExists(sentence):\r\n",
    " for (word, t) in tag(sentence):\r\n",
    "   if paraphraseable(t):\r\n",
    "    syns = synonyms(word, t)\r\n",
    "    if syns:\r\n",
    "     if len(syns) >= 1:\r\n",
    "      yield [word, list(syns)[0]] # keep only one\r\n",
    "      continue\r\n",
    "   yield [word]\r\n",
    "\r\n",
    "def get_synonyms(sentence):\r\n",
    "  words = [word for word in synonymIfExists(sentence)]\r\n",
    "  single_list = [item for sublist in words for item in sublist]\r\n",
    "  str_list = ','.join(single_list)\r\n",
    "  return str_list\r\n",
    "\r\n",
    "\r\n",
    "print(get_synonyms(\"casual winter cloth\"))\r\n",
    "print(get_synonyms(\"fitting necktie for a casual meeting\"))\r\n",
    "print(get_synonyms(\"summer T-shirt\"))\r\n",
    "print(get_synonyms(\"elastic trouser\"))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "casual,occasional,winter,wintertime,cloth,textile\n",
      "fitting,necktie,tie,for,a,casual,occasional,meeting,coming_together\n",
      "summer,summertime,T-shirt,jersey\n",
      "elastic,pliant,trouser,pant\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "source": [
    "def preprocess_query(search_term):\r\n",
    "        stem = StemmingAnalyzer(stoplist=frozenset([\r\n",
    "            'and', 'is', 'it', 'an', 'as', 'at', 'have', 'in', 'yet', 'if',\r\n",
    "            'from', 'for', 'when', 'by', 'to', 'you', 'be', 'we', 'that',\r\n",
    "            'may', 'not', 'with', 'tbd', 'a', 'on', 'your', 'this', 'of', 'us',\r\n",
    "            'will', 'can', 'the', 'or', 'are'\r\n",
    "        ]),\r\n",
    "                                minsize=3)\r\n",
    "        return [token.text for token in stem(search_term)]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "source": [
    "print(','.join(preprocess_query('fitting,necktie,tie,for,a,casual,occasional,meeting,coming_together')))\r\n",
    "print(','.join(preprocess_query('elastic,pliant,trouser,pant')))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "fitt,neckti,tie,casual,occasion,meet,coming_togeth\n",
      "elast,pliant,trouser,pant\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training word embeddings with product data (Tensorflow, Keras)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "import io\r\n",
    "import re\r\n",
    "import string\r\n",
    "import tensorflow as tf\r\n",
    "import tqdm\r\n",
    "\r\n",
    "from tensorflow.keras import Model\r\n",
    "from tensorflow.keras.layers import Dot, Embedding, Flatten\r\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\r\n",
    "\r\n",
    "import numpy as np"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "sentence = \"The wide road shimmered in the hot sun\"\r\n",
    "tokens = list(sentence.lower().split())\r\n",
    "print(len(tokens))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "8\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "vocab, index = {}, 1  # start indexing from 1\r\n",
    "vocab['<pad>'] = 0  # add a padding token\r\n",
    "for token in tokens:\r\n",
    "  if token not in vocab:\r\n",
    "    vocab[token] = index\r\n",
    "    index += 1\r\n",
    "vocab_size = len(vocab)\r\n",
    "print(vocab)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'<pad>': 0, 'the': 1, 'wide': 2, 'road': 3, 'shimmered': 4, 'in': 5, 'hot': 6, 'sun': 7}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "inverse_vocab = {index: token for token, index in vocab.items()}\r\n",
    "print(inverse_vocab)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{0: '<pad>', 1: 'the', 2: 'wide', 3: 'road', 4: 'shimmered', 5: 'in', 6: 'hot', 7: 'sun'}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "example_sequence = [vocab[word] for word in tokens]\r\n",
    "print(example_sequence)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1, 2, 3, 4, 5, 1, 6, 7]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "window_size = 2\r\n",
    "positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\r\n",
    "      example_sequence,\r\n",
    "      vocabulary_size=vocab_size,\r\n",
    "      window_size=window_size,\r\n",
    "      negative_samples=0)\r\n",
    "print(len(positive_skip_grams))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "26\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "for target, context in positive_skip_grams[:5]:\r\n",
    "  print(f\"({target}, {context}): ({inverse_vocab[target]}, {inverse_vocab[context]})\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(3, 1): (road, the)\n",
      "(1, 4): (the, shimmered)\n",
      "(7, 1): (sun, the)\n",
      "(4, 5): (shimmered, in)\n",
      "(2, 4): (wide, shimmered)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "# Get target and context words for one positive skip-gram.\r\n",
    "target_word, context_word = positive_skip_grams[0]\r\n",
    "\r\n",
    "# Set the number of negative samples per positive context.\r\n",
    "num_ns = 4\r\n",
    "\r\n",
    "context_class = tf.reshape(tf.constant(context_word, dtype=\"int64\"), (1, 1))\r\n",
    "negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\r\n",
    "    true_classes=context_class,  # class that should be sampled as 'positive'\r\n",
    "    num_true=1,  # each positive skip-gram has 1 positive context class\r\n",
    "    num_sampled=num_ns,  # number of negative context words to sample\r\n",
    "    unique=True,  # all the negative samples should be unique\r\n",
    "    range_max=vocab_size,  # pick index of the samples from [0, vocab_size]\r\n",
    "    seed=4,  # seed for reproducibility\r\n",
    "    name=\"negative_sampling\"  # name of this operation\r\n",
    ")\r\n",
    "print(negative_sampling_candidates)\r\n",
    "print([inverse_vocab[index.numpy()] for index in negative_sampling_candidates])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tf.Tensor([3 7 0 4], shape=(4,), dtype=int64)\n",
      "['road', 'sun', '<pad>', 'shimmered']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "# Add a dimension so you can use concatenation (on the next step).\r\n",
    "negative_sampling_candidates = tf.expand_dims(negative_sampling_candidates, 1)\r\n",
    "\r\n",
    "# Concat positive context word with negative sampled words.\r\n",
    "context = tf.concat([context_class, negative_sampling_candidates], 0)\r\n",
    "\r\n",
    "# Label first context word as 1 (positive) followed by num_ns 0s (negative).\r\n",
    "label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\r\n",
    "\r\n",
    "# Reshape target to shape (1,) and context and label to (num_ns+1,).\r\n",
    "target = tf.squeeze(target_word)\r\n",
    "context = tf.squeeze(context)\r\n",
    "label = tf.squeeze(label)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "print(f\"target_index    : {target}\")\r\n",
    "print(f\"target_word     : {inverse_vocab[target_word]}\")\r\n",
    "print(f\"context_indices : {context}\")\r\n",
    "print(f\"context_words   : {[inverse_vocab[c.numpy()] for c in context]}\")\r\n",
    "print(f\"label           : {label}\")\r\n",
    "\r\n",
    "print(\"target  :\", target)\r\n",
    "print(\"context :\", context)\r\n",
    "print(\"label   :\", label)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "target_index    : 3\n",
      "target_word     : road\n",
      "context_indices : [1 3 7 0 4]\n",
      "context_words   : ['the', 'road', 'sun', '<pad>', 'shimmered']\n",
      "label           : [1 0 0 0 0]\n",
      "target  : tf.Tensor(3, shape=(), dtype=int32)\n",
      "context : tf.Tensor([1 3 7 0 4], shape=(5,), dtype=int64)\n",
      "label   : tf.Tensor([1 0 0 0 0], shape=(5,), dtype=int64)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocessing in one function"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "# Generates skip-gram pairs with negative sampling for a list of sequences\r\n",
    "# (int-encoded sentences) based on window size, number of negative samples\r\n",
    "# and vocabulary size.\r\n",
    "def generate_training_data(sequences, window_size, num_ns, vocab_size, seed):\r\n",
    "  # Elements of each training example are appended to these lists.\r\n",
    "  targets, contexts, labels = [], [], []\r\n",
    "\r\n",
    "  # Build the sampling table for vocab_size tokens.\r\n",
    "  sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\r\n",
    "\r\n",
    "  # Iterate over all sequences (sentences) in dataset.\r\n",
    "  for sequence in tqdm.tqdm(sequences):\r\n",
    "\r\n",
    "    # Generate positive skip-gram pairs for a sequence (sentence).\r\n",
    "    positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\r\n",
    "          sequence,\r\n",
    "          vocabulary_size=vocab_size,\r\n",
    "          sampling_table=sampling_table,\r\n",
    "          window_size=window_size,\r\n",
    "          negative_samples=0)\r\n",
    "\r\n",
    "    # Iterate over each positive skip-gram pair to produce training examples\r\n",
    "    # with positive context word and negative samples.\r\n",
    "    for target_word, context_word in positive_skip_grams:\r\n",
    "      context_class = tf.expand_dims(\r\n",
    "          tf.constant([context_word], dtype=\"int64\"), 1)\r\n",
    "      negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\r\n",
    "          true_classes=context_class,\r\n",
    "          num_true=1,\r\n",
    "          num_sampled=num_ns,\r\n",
    "          unique=True,\r\n",
    "          range_max=vocab_size,\r\n",
    "          seed=seed,\r\n",
    "          name=\"negative_sampling\")\r\n",
    "\r\n",
    "      # Build context and label vectors (for one target word)\r\n",
    "      negative_sampling_candidates = tf.expand_dims(\r\n",
    "          negative_sampling_candidates, 1)\r\n",
    "\r\n",
    "      context = tf.concat([context_class, negative_sampling_candidates], 0)\r\n",
    "      label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\r\n",
    "\r\n",
    "      # Append each element from the training example to global lists.\r\n",
    "      targets.append(target_word)\r\n",
    "      contexts.append(context)\r\n",
    "      labels.append(label)\r\n",
    "\r\n",
    "  return targets, contexts, labels\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "with open(path_to_file) as f: \r\n",
    "  lines = f.read().splitlines()\r\n",
    "for line in lines[:20]:\r\n",
    "  print(line)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "text_ds = tf.data.TextLineDataset(path_to_file).filter(lambda x: tf.cast(tf.strings.length(x), bool))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "# Now, create a custom standardization function to lowercase the text and\r\n",
    "# remove punctuation.\r\n",
    "def custom_standardization(input_data):\r\n",
    "  lowercase = tf.strings.lower(input_data)\r\n",
    "  return tf.strings.regex_replace(lowercase,\r\n",
    "                                  '[%s]' % re.escape(string.punctuation), '')\r\n",
    "\r\n",
    "\r\n",
    "# Define the vocabulary size and number of words in a sequence.\r\n",
    "vocab_size = 4096\r\n",
    "sequence_length = 10\r\n",
    "\r\n",
    "# Use the text vectorization layer to normalize, split, and map strings to\r\n",
    "# integers. Set output_sequence_length length to pad all samples to same length.\r\n",
    "vectorize_layer = TextVectorization(\r\n",
    "    standardize=custom_standardization,\r\n",
    "    max_tokens=vocab_size,\r\n",
    "    output_mode='int',\r\n",
    "    output_sequence_length=sequence_length)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "vectorize_layer.adapt(text_ds.batch(1024))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "# Save the created vocabulary for reference.\r\n",
    "inverse_vocab = vectorize_layer.get_vocabulary()\r\n",
    "print(inverse_vocab[:20])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['', '[UNK]', 'the', 'and', 'to', 'i', 'of', 'you', 'my', 'a', 'that', 'in', 'is', 'not', 'for', 'with', 'me', 'it', 'be', 'your']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "# Vectorize the data in text_ds.\r\n",
    "text_vector_ds = text_ds.batch(1024).prefetch(tf.data.AUTOTUNE).map(vectorize_layer).unbatch()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "sequences = list(text_vector_ds.as_numpy_iterator())\r\n",
    "print(len(sequences))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "32777\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "for seq in sequences[:5]:\r\n",
    "  print(f\"{seq} => {[inverse_vocab[i] for i in seq]}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[ 89 270   0   0   0   0   0   0   0   0] => ['first', 'citizen', '', '', '', '', '', '', '', '']\n",
      "[138  36 982 144 673 125  16 106   0   0] => ['before', 'we', 'proceed', 'any', 'further', 'hear', 'me', 'speak', '', '']\n",
      "[34  0  0  0  0  0  0  0  0  0] => ['all', '', '', '', '', '', '', '', '', '']\n",
      "[106 106   0   0   0   0   0   0   0   0] => ['speak', 'speak', '', '', '', '', '', '', '', '']\n",
      "[ 89 270   0   0   0   0   0   0   0   0] => ['first', 'citizen', '', '', '', '', '', '', '', '']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "targets, contexts, labels = generate_training_data(\r\n",
    "    sequences=sequences,\r\n",
    "    window_size=2,\r\n",
    "    num_ns=4,\r\n",
    "    vocab_size=vocab_size,\r\n",
    "    seed=4)\r\n",
    "print(len(targets), len(contexts), len(labels))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 32777/32777 [00:07<00:00, 4498.63it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "65011 65011 65011\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "BATCH_SIZE = 1024\r\n",
    "BUFFER_SIZE = 10000\r\n",
    "dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\r\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\r\n",
    "print(dataset)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<BatchDataset shapes: (((1024,), (1024, 5, 1)), (1024, 5)), types: ((tf.int32, tf.int64), tf.int64)>\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "dataset = dataset.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\r\n",
    "print(dataset)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<PrefetchDataset shapes: (((1024,), (1024, 5, 1)), (1024, 5)), types: ((tf.int32, tf.int64), tf.int64)>\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "class Word2Vec(Model):\r\n",
    "  def __init__(self, vocab_size, embedding_dim):\r\n",
    "    super(Word2Vec, self).__init__()\r\n",
    "    self.target_embedding = Embedding(vocab_size,\r\n",
    "                                      embedding_dim,\r\n",
    "                                      input_length=1,\r\n",
    "                                      name=\"w2v_embedding\")\r\n",
    "    self.context_embedding = Embedding(vocab_size,\r\n",
    "                                      embedding_dim,\r\n",
    "                                      input_length=num_ns+1)\r\n",
    "    self.dots = Dot(axes=(3, 2))\r\n",
    "    self.flatten = Flatten()\r\n",
    "\r\n",
    "  def call(self, pair):\r\n",
    "    target, context = pair\r\n",
    "    word_emb = self.target_embedding(target)\r\n",
    "    context_emb = self.context_embedding(context)\r\n",
    "    dots = self.dots([context_emb, word_emb])\r\n",
    "    return self.flatten(dots)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "embedding_dim = 128\r\n",
    "word2vec = Word2Vec(vocab_size, embedding_dim)\r\n",
    "word2vec.compile(optimizer='adam',\r\n",
    "                 loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\r\n",
    "                 metrics=['accuracy'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "word2vec.fit(dataset, epochs=20, callbacks=[tensorboard_callback])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/20\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 1.6084 - accuracy: 0.2285\n",
      "Epoch 2/20\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 1.5894 - accuracy: 0.5534\n",
      "Epoch 3/20\n",
      "63/63 [==============================] - 1s 13ms/step - loss: 1.5423 - accuracy: 0.6041\n",
      "Epoch 4/20\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 1.4598 - accuracy: 0.5752\n",
      "Epoch 5/20\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 1.3617 - accuracy: 0.5790\n",
      "Epoch 6/20\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 1.2650 - accuracy: 0.6048\n",
      "Epoch 7/20\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 1.1750 - accuracy: 0.6383\n",
      "Epoch 8/20\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 1.0917 - accuracy: 0.6720\n",
      "Epoch 9/20\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 1.0141 - accuracy: 0.7061\n",
      "Epoch 10/20\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.9420 - accuracy: 0.7358\n",
      "Epoch 11/20\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 0.8749 - accuracy: 0.7628\n",
      "Epoch 12/20\n",
      "63/63 [==============================] - 1s 13ms/step - loss: 0.8127 - accuracy: 0.7860\n",
      "Epoch 13/20\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.7553 - accuracy: 0.8058\n",
      "Epoch 14/20\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 0.7026 - accuracy: 0.8230\n",
      "Epoch 15/20\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.6543 - accuracy: 0.8377\n",
      "Epoch 16/20\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.6102 - accuracy: 0.8524\n",
      "Epoch 17/20\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.5699 - accuracy: 0.8643\n",
      "Epoch 18/20\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.5333 - accuracy: 0.8759\n",
      "Epoch 19/20\n",
      "63/63 [==============================] - 1s 13ms/step - loss: 0.5000 - accuracy: 0.8853\n",
      "Epoch 20/20\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.4697 - accuracy: 0.8944\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x28516d8eca0>"
      ]
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "source": [
    "%tensorboard --logdir logs"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "UsageError: Line magic function `%tensorboard` not found.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "source": [
    "weights = word2vec.get_layer('w2v_embedding').get_weights()[0]\r\n",
    "vocab = vectorize_layer.get_vocabulary()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "source": [
    "out_v = io.open('vectors.tsv', 'w', encoding='utf-8')\r\n",
    "out_m = io.open('metadata.tsv', 'w', encoding='utf-8')\r\n",
    "\r\n",
    "for index, word in enumerate(vocab):\r\n",
    "  if index == 0:\r\n",
    "    continue  # skip <PAD>.\r\n",
    "  vec = weights[index]\r\n",
    "  out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\r\n",
    "  out_m.write(word + \"\\n\")\r\n",
    "out_v.close()\r\n",
    "out_m.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "source": [
    "index2word = dict(enumerate(list(inverse_vocab)))\r\n",
    "word2index = {v: k for k, v in index2word.items()}\r\n",
    "\r\n",
    "text_vector_ds"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<_UnbatchDataset shapes: (10,), types: tf.int64>"
      ]
     },
     "metadata": {},
     "execution_count": 60
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "source": [
    "def find_closest(embeds, word, n=1): # n is for \"n closest words\"\r\n",
    "  n = n + 1 # This is becuse the most similar word is definatly that word itself. like the most similar word for \"apple\" is \"apple\". so we should look for top n+1 words\r\n",
    "  main_vec = embeds(word2index[word])\r\n",
    "\r\n",
    "  similarities = -tf.keras.losses.cosine_similarity(embeds.embeddings, main_vec)\r\n",
    "  top_n = tf.math.top_k(similarities, n).indices\r\n",
    "  words = [index2word[i] for i in top_n.numpy()]\r\n",
    "\r\n",
    "  return words[1:], tf.math.top_k(similarities, 3)[0][1]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "source": [
    "target = word2vec.target_embedding\r\n",
    "res, sim = find_closest(target, 'summer')\r\n",
    "print('closest: ', res)\r\n",
    "print('top similarity: ', sim.numpy())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "closest:  ['rebellion']\n",
      "top similarity:  0.5394914\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "source": [
    "word2vec.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"word2_vec\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "w2v_embedding (Embedding)    multiple                  524288    \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        multiple                  524288    \n",
      "_________________________________________________________________\n",
      "dot (Dot)                    multiple                  0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            multiple                  0         \n",
      "=================================================================\n",
      "Total params: 1,048,576\n",
      "Trainable params: 1,048,576\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Import pretrained word embeddings (Google's Word2Vec)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "source": [
    "word2vec_model_path = 'E:/Users/Lucas xD/Downloads/GoogleNews-vectors-negative300.bin'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "source": [
    "try: # nich mehrmals in Speicher laden... sind 3gb\r\n",
    "    model\r\n",
    "except NameError:\r\n",
    "    model = KeyedVectors.load_word2vec_format(word2vec_model_path, binary=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "source": [
    "model.most_similar('HUGO')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('MARGO', 0.5448382496833801),\n",
       " ('ESPINOSA', 0.540989875793457),\n",
       " ('ALFREDO', 0.5401614904403687),\n",
       " ('FELIX', 0.5358700156211853),\n",
       " ('JEFFREYS', 0.5335772037506104),\n",
       " ('PABLO', 0.5334649682044983),\n",
       " ('ALBERTO', 0.5314393043518066),\n",
       " ('LINDGREN', 0.5300692915916443),\n",
       " ('CLARA', 0.5292963981628418),\n",
       " ('RICARDO', 0.5291734337806702)]"
      ]
     },
     "metadata": {},
     "execution_count": 81
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create Schema with analyzers & filters (Whoosh)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "source": [
    "dataset_path = \"E:/Users/Lucas xD/Downloads/Products_Q_US_edited.json\"\r\n",
    "index_path = \"./hb_index\"\r\n",
    "\r\n",
    "json_data = json.load(open(dataset_path,'r'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "source": [
    "schema = Schema(brand=TEXT(),\r\n",
    "                colors=TEXT(),\r\n",
    "                gender=TEXT(),\r\n",
    "                longDescription=TEXT(analyzer=StemmingAnalyzer() | NgramFilter(minsize=2, maxsize=4) | StopFilter()),\r\n",
    "                name=TEXT(analyzer=StemmingAnalyzer() | NgramFilter(minsize=2, maxsize=4) | StopFilter(), field_boost=3.0),\r\n",
    "                productId=TEXT(),\r\n",
    "                shortDescription=TEXT(analyzer=StemmingAnalyzer() | NgramFilter(minsize=2, maxsize=4) | StopFilter()),\r\n",
    "                sizes=TEXT(),\r\n",
    "                styleName=TEXT(),\r\n",
    "                variants=TEXT(),\r\n",
    "                image=TEXT(),\r\n",
    "                id=ID(stored=True)\r\n",
    "                )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create the index"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "source": [
    "create_new_index = True\r\n",
    "if(create_new_index):\r\n",
    "    index = create_in(index_path, schema)\r\n",
    "else:    \r\n",
    "    index = whoosh.index.open_dir(index_path)\r\n",
    "\r\n",
    "writer = index.writer()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "source": [
    "for doc in json_data:\r\n",
    "    writer.add_document(\r\n",
    "            brand=str(doc['brand']),\r\n",
    "            colors=str(doc['colors']),\r\n",
    "            gender=str(doc['gender']),\r\n",
    "            longDescription=str(doc['longDescription']),\r\n",
    "            name=str(doc['name']),\r\n",
    "            productId=str(doc['productId']),\r\n",
    "            shortDescription=str(doc['shortDescription']),\r\n",
    "            sizes=str(doc['sizes']),\r\n",
    "            styleName=str(doc['styleName']),\r\n",
    "            variants=str(doc['variants']),\r\n",
    "            image=str(doc['image']),\r\n",
    "            id=str(doc['id'])\r\n",
    "    )\r\n",
    "         \r\n",
    "writer.commit()"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Testing the index"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "source": [
    "\r\n",
    "use_synonyms = True\r\n",
    "\r\n",
    "search_term = \"dress\"\r\n",
    "\r\n",
    "if(use_synonyms):\r\n",
    "    try:\r\n",
    "        similarity_list = model.most_similar(search_term, topn=1)\r\n",
    "        similar_words = [sim_tuple[0] for sim_tuple in similarity_list]\r\n",
    "    except KeyError:\r\n",
    "        similar_words = []\r\n",
    "    keywords = \" OR \".join([search_term] + similar_words)\r\n",
    "\r\n",
    "    print(\"Results with Word2Vec:\")\r\n",
    "    print(f\"Similar words used: {similar_words}\")\r\n",
    "    with index.searcher(weighting=scoring.TF_IDF()) as searcher:\r\n",
    "        query = MultifieldParser([\"name\", \"longDescription\", 'shortDescription'], index.schema).parse(keywords)\r\n",
    "        results = searcher.search(query)\r\n",
    "        for docnum, score in results.items():\r\n",
    "            print(docnum, score)\r\n",
    "        print(results)\r\n",
    "else:\r\n",
    "    keywords  = search_term\r\n",
    "\r\n",
    "# results = []\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "print(\"________________________\\n\")\r\n",
    "print(\"Results without Word2Vec:\")\r\n",
    "with index.searcher(weighting=scoring.TF_IDF()) as searcher:\r\n",
    "    query = MultifieldParser([\"name\", \"longDescription\", \"shortDescription\"], index.schema).parse(search_term)\r\n",
    "    results = searcher.search(query)\r\n",
    "    for docnum, score in results.items():\r\n",
    "        print(docnum, score)\r\n",
    "    for doc in results:\r\n",
    "        print(doc)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Results with Word2Vec:\n",
      "Similar words used: ['dresses']\n",
      "519 46.841051088653984\n",
      "536 46.841051088653984\n",
      "992 46.841051088653984\n",
      "1679 46.841051088653984\n",
      "2 39.648702347024916\n",
      "6 39.648702347024916\n",
      "17 39.648702347024916\n",
      "215 39.648702347024916\n",
      "255 39.648702347024916\n",
      "278 39.648702347024916\n",
      "<Top 10 Results for Or([And([Term('name', 'dres'), Term('name', 'ress')]), And([Term('longDescription', 'dres'), Term('longDescription', 'ress')]), And([Term('shortDescription', 'dres'), Term('shortDescription', 'ress')])]) runtime=0.012073500001861248>\n",
      "________________________\n",
      "\n",
      "Results without Word2Vec:\n",
      "519 46.841051088653984\n",
      "536 46.841051088653984\n",
      "992 46.841051088653984\n",
      "1679 46.841051088653984\n",
      "2 39.648702347024916\n",
      "6 39.648702347024916\n",
      "17 39.648702347024916\n",
      "215 39.648702347024916\n",
      "255 39.648702347024916\n",
      "278 39.648702347024916\n",
      "<Hit {'id': '519.0'}>\n",
      "<Hit {'id': '536.0'}>\n",
      "<Hit {'id': '992.0'}>\n",
      "<Hit {'id': '1679.0'}>\n",
      "<Hit {'id': '2.0'}>\n",
      "<Hit {'id': '6.0'}>\n",
      "<Hit {'id': '17.0'}>\n",
      "<Hit {'id': '215.0'}>\n",
      "<Hit {'id': '255.0'}>\n",
      "<Hit {'id': '278.0'}>\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ]
}