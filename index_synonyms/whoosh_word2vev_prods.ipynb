{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit ('.venv': venv)"
  },
  "interpreter": {
   "hash": "3fa4fbff26baf5372c1227c893813242fd1c5dcfd93352f945b61348dd56c099"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from whoosh.fields import Schema, TEXT, KEYWORD, ID, STORED, DATETIME\n",
    "from whoosh.index import create_in\n",
    "from whoosh.analysis import StemmingAnalyzer, NgramFilter, StopFilter\n",
    "from whoosh.qparser import MultifieldParser\n",
    "from whoosh import scoring\n",
    "import whoosh\n",
    "import json\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "import numpy as np"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "dataset_path = \"E:/Users/Lucas xD/Downloads/Products_Q_US_edited.json\"\n",
    "index_path = \"./hb_index\"\n",
    "word2vec_model_path = 'E:/Users/Lucas xD/Downloads/GoogleNews-vectors-negative300.bin'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "try: # nich mehrmals in Speicher laden... sind 3gb\n",
    "    model\n",
    "except NameError:\n",
    "    model = KeyedVectors.load_word2vec_format(word2vec_model_path, binary=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "json_data = json.load(open(dataset_path,'r'))\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "schema = Schema(brand=TEXT(),\n",
    "                colors=TEXT(),\n",
    "                gender=TEXT(),\n",
    "                longDescription=TEXT(analyzer=StemmingAnalyzer() | NgramFilter(minsize=2, maxsize=4) | StopFilter()),\n",
    "                name=TEXT(analyzer=StemmingAnalyzer() | NgramFilter(minsize=2, maxsize=4) | StopFilter(), field_boost=3.0),\n",
    "                productId=TEXT(),\n",
    "                shortDescription=TEXT(analyzer=StemmingAnalyzer() | NgramFilter(minsize=2, maxsize=4) | StopFilter()),\n",
    "                sizes=TEXT(),\n",
    "                styleName=TEXT(),\n",
    "                variants=TEXT(),\n",
    "                image=TEXT(),\n",
    "                id=ID(stored=True)\n",
    "                )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "create_new_index = True\n",
    "if(create_new_index):\n",
    "    index = create_in(index_path, schema)\n",
    "else:    \n",
    "    index = whoosh.index.open_dir(index_path)\n",
    "\n",
    "writer = index.writer()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "for doc in json_data:\n",
    "    writer.add_document(\n",
    "            brand=str(doc['brand']),\n",
    "            colors=str(doc['colors']),\n",
    "            gender=str(doc['gender']),\n",
    "            longDescription=str(doc['longDescription']),\n",
    "            name=str(doc['name']),\n",
    "            productId=str(doc['productId']),\n",
    "            shortDescription=str(doc['shortDescription']),\n",
    "            sizes=str(doc['sizes']),\n",
    "            styleName=str(doc['styleName']),\n",
    "            variants=str(doc['variants']),\n",
    "            image=str(doc['image']),\n",
    "            id=str(doc['id'])\n",
    "    )\n",
    "         \n",
    "writer.commit()"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "\r\n",
    "use_synonyms = True\r\n",
    "\r\n",
    "search_term = \"dre\"\r\n",
    "\r\n",
    "if(use_synonyms):\r\n",
    "    try:\r\n",
    "        similarity_list = model.most_similar(search_term, topn=1)\r\n",
    "        similar_words = [sim_tuple[0] for sim_tuple in similarity_list]\r\n",
    "    except KeyError:\r\n",
    "        similar_words = []\r\n",
    "    keywords = \" OR \".join([search_term] + similar_words)\r\n",
    "\r\n",
    "    print(\"Results with Word2Vec:\")\r\n",
    "    print(f\"Similar words used: {similar_words}\")\r\n",
    "    with index.searcher(weighting=scoring.TF_IDF()) as searcher:\r\n",
    "        query = MultifieldParser([\"name\", \"longDescription\", 'shortDescription'], index.schema).parse(keywords)\r\n",
    "        results = searcher.search(query)\r\n",
    "        for docnum, score in results.items():\r\n",
    "            print(docnum, score)\r\n",
    "        print(results)\r\n",
    "else:\r\n",
    "    keywords  = search_term\r\n",
    "\r\n",
    "# results = []\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "print(\"________________________\\n\")\r\n",
    "print(\"Results without Word2Vec:\")\r\n",
    "with index.searcher(weighting=scoring.TF_IDF()) as searcher:\r\n",
    "    query = MultifieldParser([\"name\", \"longDescription\", \"shortDescription\"], index.schema).parse(search_term)\r\n",
    "    results = searcher.search(query)\r\n",
    "    for docnum, score in results.items():\r\n",
    "        print(docnum, score)\r\n",
    "    for doc in results:\r\n",
    "        print(doc)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Results with Word2Vec:\n",
      "Similar words used: ['duine']\n",
      "519 24.138011707535227\n",
      "536 24.138011707535227\n",
      "992 24.138011707535227\n",
      "1679 24.138011707535227\n",
      "2 20.302635421035188\n",
      "6 20.302635421035188\n",
      "17 20.302635421035188\n",
      "215 20.302635421035188\n",
      "255 20.302635421035188\n",
      "278 20.302635421035188\n",
      "<Top 10 Results for Or([Term('name', 'dre'), Term('longDescription', 'dre'), Term('shortDescription', 'dre'), Term('name', 'duin'), Term('longDescription', 'duin'), Term('shortDescription', 'duin')]) runtime=0.002712100000053397>\n",
      "________________________\n",
      "\n",
      "Results without Word2Vec:\n",
      "519 24.138011707535227\n",
      "536 24.138011707535227\n",
      "992 24.138011707535227\n",
      "1679 24.138011707535227\n",
      "2 20.302635421035188\n",
      "6 20.302635421035188\n",
      "17 20.302635421035188\n",
      "215 20.302635421035188\n",
      "255 20.302635421035188\n",
      "278 20.302635421035188\n",
      "<Hit {'id': '519.0'}>\n",
      "<Hit {'id': '536.0'}>\n",
      "<Hit {'id': '992.0'}>\n",
      "<Hit {'id': '1679.0'}>\n",
      "<Hit {'id': '2.0'}>\n",
      "<Hit {'id': '6.0'}>\n",
      "<Hit {'id': '17.0'}>\n",
      "<Hit {'id': '215.0'}>\n",
      "<Hit {'id': '255.0'}>\n",
      "<Hit {'id': '278.0'}>\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def preprocess_query(search_term):\r\n",
    "        stem = StemmingAnalyzer(stoplist=frozenset([\r\n",
    "            'and', 'is', 'it', 'an', 'as', 'at', 'have', 'in', 'yet', 'if',\r\n",
    "            'from', 'for', 'when', 'by', 'to', 'you', 'be', 'we', 'that',\r\n",
    "            'may', 'not', 'with', 'tbd', 'a', 'on', 'your', 'this', 'of', 'us',\r\n",
    "            'will', 'can', 'the', 'or', 'are'\r\n",
    "        ]),\r\n",
    "                                minsize=3)\r\n",
    "        return [token.text for token in stem(search_term)]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "from whoosh import query\r\n",
    "from whoosh.lang.wordnet import Thesaurus\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "\r\n",
    "query = preprocess_query('summer parfume')\r\n",
    "\r\n",
    "\r\n",
    "sny_file = open(\"E:/Users/Lucas xD/Downloads/prolog/wn_s.pl\")\r\n",
    "tesaurus = Thesaurus.from_file(sny_file)\r\n",
    "synonyms_arr = []\r\n",
    "for token in query:\r\n",
    "    all_synonyms = tesaurus.synonyms(token)\r\n",
    "    if len(all_synonyms) > 0:\r\n",
    "        synonyms_arr.append(all_synonyms[0])\r\n",
    "        \r\n",
    "    print(synonyms_arr, token)\r\n",
    "    # if len(synonyms_arr) > 0: \r\n",
    "        # synonyms_arr.append(synonyms_arr) # only keep best match\r\n",
    "        # synonym_string = synonym_string = \" OR \".join(synonyms_arr)\r\n",
    "    # synonyms_arr.append(token)\r\n",
    "    # print('1', synonyms_arr)\r\n",
    "    # synonyms_arr.append(\" OR \".join(synonyms_arr))\r\n",
    "    # print('2', synonyms_arr)\r\n",
    "\r\n",
    "\r\n",
    "# synonym_string = \" \".join(synonyms_arr)\r\n",
    "print(query)\r\n",
    "fullQuery = np.concatenate((query, synonyms_arr))\r\n",
    "synonym_string = \" OR \".join(fullQuery)\r\n",
    "# synonym_string = \" OR \".join(synonyms_arr)\r\n",
    "synonym_string"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['summertime'] summer\n",
      "['summertime'] parfum\n",
      "['summer', 'parfum']\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'summer OR parfum OR summertime'"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "from whoosh import query\r\n",
    "from nltk.corpus import wordnet\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "import nltk\r\n",
    "nltk.download('wordnet')\r\n",
    "\r\n",
    "query = preprocess_query('casual fit')\r\n",
    "\r\n",
    "def get_synonyms(word):\r\n",
    "    synonyms = []\r\n",
    "    synsets = wordnet.synsets(word)\r\n",
    "    if (len(synsets) == 0):\r\n",
    "        return []\r\n",
    "    synset = synsets[0]\r\n",
    "    lemma_names = synset.lemma_names()\r\n",
    "    for lemma_name in lemma_names:\r\n",
    "        lemma_name = lemma_name.lower().replace('_', ' ')\r\n",
    "        if (lemma_name != word and lemma_name not in synonyms):\r\n",
    "            synonyms.append(lemma_name)\r\n",
    "    return synonyms\r\n",
    "\r\n",
    "\r\n",
    "synonyms_arr = []\r\n",
    "for token in query:\r\n",
    "    synonyms = get_synonyms(token)\r\n",
    "    if len(synonyms) > 0: synonyms_arr.append(synonyms[0]) # only keep best match\r\n",
    "\r\n",
    "\r\n",
    "# synonym_string = \" \".join(synonyms_arr)\r\n",
    "\r\n",
    "fullQuery = np.concatenate((query, synonyms_arr))\r\n",
    "synonym_string = \" OR \".join(fullQuery)\r\n",
    "# synonym_string = \" OR \".join(synonyms_arr)\r\n",
    "synonym_string"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\lucas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'casual OR fit OR insouciant OR tantrum'"
      ]
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "source": [
    "synsets1 = wordnet.synsets('casual')[0]\r\n",
    "synsets2 = wordnet.synsets('insouciant')[0]\r\n",
    "\r\n",
    "print(synsets1.wup_similarity(synsets2))\r\n",
    "print(synsets1.path_similarity(synsets2))\r\n",
    "print(synsets1.lch_similarity(synsets2))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1.0\n",
      "1.0\n",
      "0.6931471805599453\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "source": [
    "from nltk.tokenize import word_tokenize\r\n",
    "from nltk.tag import pos_tag\r\n",
    "from nltk.corpus import wordnet as wn\r\n",
    "\r\n",
    "# nltk.download('punkt')\r\n",
    "# nltk.download('averaged_perceptron_tagger')\r\n",
    "\r\n",
    "def tag(sentence):\r\n",
    " words = word_tokenize(sentence)\r\n",
    " words = pos_tag(words)\r\n",
    " return words\r\n",
    "\r\n",
    "def paraphraseable(tag):\r\n",
    " return tag.startswith('NN') or tag == 'VB' or tag.startswith('JJ')\r\n",
    "\r\n",
    "def pos(tag):\r\n",
    " if tag.startswith('NN'):\r\n",
    "  return wn.NOUN\r\n",
    " elif tag.startswith('V'):\r\n",
    "  return wn.VERB\r\n",
    "\r\n",
    "def synonyms(word, tag):\r\n",
    "    lemma_lists = [ss.lemmas() for ss in wn.synsets(word, pos(tag))]\r\n",
    "    lemmas = [lemma.name() for lemma in sum(lemma_lists, []) if lemma.name() != word]\r\n",
    "    return set(lemmas)\r\n",
    "\r\n",
    "def synonymIfExists(sentence):\r\n",
    " for (word, t) in tag(sentence):\r\n",
    "   if paraphraseable(t):\r\n",
    "    syns = synonyms(word, t)\r\n",
    "    if syns:\r\n",
    "     if len(syns) >= 1:\r\n",
    "      yield [word, list(syns)[0]] # keep only one\r\n",
    "      continue\r\n",
    "   yield [word, []]\r\n",
    "\r\n",
    "def get_synonyms(sentence):\r\n",
    " return [word for word in synonymIfExists(sentence)]\r\n",
    "\r\n",
    "\r\n",
    "print(get_synonyms(\"casual winter cloth\"))\r\n",
    "print(get_synonyms(\"business necktie\"))\r\n",
    "print(get_synonyms(\"summer T-shirt\"))\r\n",
    "print(get_synonyms(\"blue vest\"))\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[['casual', 'nonchalant'], ['winter', 'wintertime'], ['cloth', 'fabric']]\n",
      "[['business', 'occupation'], ['necktie', 'tie']]\n",
      "[['summer', 'summertime'], ['T-shirt', 'tee_shirt']]\n",
      "[['blue', 'dispirited'], ['vest', 'singlet']]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "source": [
    "\"wintertime\" == \"winter\""
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "metadata": {},
     "execution_count": 102
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ]
}