{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit ('.venv': venv)"
  },
  "interpreter": {
   "hash": "3fa4fbff26baf5372c1227c893813242fd1c5dcfd93352f945b61348dd56c099"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from whoosh.fields import Schema, TEXT, KEYWORD, ID, STORED, DATETIME\n",
    "from whoosh.index import create_in\n",
    "from whoosh.analysis import StemmingAnalyzer, NgramFilter, StopFilter\n",
    "from whoosh.qparser import MultifieldParser\n",
    "from whoosh import scoring\n",
    "import whoosh\n",
    "import json\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"E:/Users/Lucas xD/Downloads/Products_Q_US_edited.json\"\n",
    "index_path = \"./hb_index\"\n",
    "word2vec_model_path = 'E:/Users/Lucas xD/Downloads/GoogleNews-vectors-negative300.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: # nich mehrmals in Speicher laden... sind 3gb\n",
    "    model\n",
    "except NameError:\n",
    "    model = KeyedVectors.load_word2vec_format(word2vec_model_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data = json.load(open(dataset_path,'r'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = Schema(brand=TEXT(),\n",
    "                colors=TEXT(),\n",
    "                gender=TEXT(),\n",
    "                longDescription=TEXT(analyzer=StemmingAnalyzer() | NgramFilter(minsize=2, maxsize=4) | StopFilter()),\n",
    "                name=TEXT(analyzer=StemmingAnalyzer() | NgramFilter(minsize=2, maxsize=4) | StopFilter(), field_boost=3.0),\n",
    "                productId=TEXT(),\n",
    "                shortDescription=TEXT(analyzer=StemmingAnalyzer() | NgramFilter(minsize=2, maxsize=4) | StopFilter()),\n",
    "                sizes=TEXT(),\n",
    "                styleName=TEXT(),\n",
    "                variants=TEXT(),\n",
    "                image=TEXT(),\n",
    "                id=ID(stored=True)\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_new_index = True\n",
    "if(create_new_index):\n",
    "    index = create_in(index_path, schema)\n",
    "else:    \n",
    "    index = whoosh.index.open_dir(index_path)\n",
    "\n",
    "writer = index.writer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for doc in json_data:\n",
    "    writer.add_document(\n",
    "            brand=str(doc['brand']),\n",
    "            colors=str(doc['colors']),\n",
    "            gender=str(doc['gender']),\n",
    "            longDescription=str(doc['longDescription']),\n",
    "            name=str(doc['name']),\n",
    "            productId=str(doc['productId']),\n",
    "            shortDescription=str(doc['shortDescription']),\n",
    "            sizes=str(doc['sizes']),\n",
    "            styleName=str(doc['styleName']),\n",
    "            variants=str(doc['variants']),\n",
    "            image=str(doc['image']),\n",
    "            id=str(doc['id'])\n",
    "    )\n",
    "         \n",
    "writer.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Results with Word2Vec:\nSimilar words used: ['duine']\n519 24.138011707535227\n536 24.138011707535227\n992 24.138011707535227\n1679 24.138011707535227\n2 20.302635421035188\n6 20.302635421035188\n17 20.302635421035188\n215 20.302635421035188\n255 20.302635421035188\n278 20.302635421035188\n<Top 10 Results for Or([Term('name', 'dre'), Term('longDescription', 'dre'), Term('shortDescription', 'dre'), Term('name', 'duin'), Term('longDescription', 'duin'), Term('shortDescription', 'duin')]) runtime=0.002712100000053397>\n________________________\n\nResults without Word2Vec:\n519 24.138011707535227\n536 24.138011707535227\n992 24.138011707535227\n1679 24.138011707535227\n2 20.302635421035188\n6 20.302635421035188\n17 20.302635421035188\n215 20.302635421035188\n255 20.302635421035188\n278 20.302635421035188\n<Hit {'id': '519.0'}>\n<Hit {'id': '536.0'}>\n<Hit {'id': '992.0'}>\n<Hit {'id': '1679.0'}>\n<Hit {'id': '2.0'}>\n<Hit {'id': '6.0'}>\n<Hit {'id': '17.0'}>\n<Hit {'id': '215.0'}>\n<Hit {'id': '255.0'}>\n<Hit {'id': '278.0'}>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "use_synonyms = True\n",
    "\n",
    "search_term = \"dre\"\n",
    "\n",
    "if(use_synonyms):\n",
    "    try:\n",
    "        similarity_list = model.most_similar(search_term, topn=1)\n",
    "        similar_words = [sim_tuple[0] for sim_tuple in similarity_list]\n",
    "    except KeyError:\n",
    "        similar_words = []\n",
    "    keywords = \" OR \".join([search_term] + similar_words)\n",
    "\n",
    "    print(\"Results with Word2Vec:\")\n",
    "    print(f\"Similar words used: {similar_words}\")\n",
    "    with index.searcher(weighting=scoring.TF_IDF()) as searcher:\n",
    "        query = MultifieldParser([\"name\", \"longDescription\", 'shortDescription'], index.schema).parse(keywords)\n",
    "        results = searcher.search(query)\n",
    "        for docnum, score in results.items():\n",
    "            print(docnum, score)\n",
    "        print(results)\n",
    "else:\n",
    "    keywords  = search_term\n",
    "\n",
    "# results = []\n",
    "\n",
    "\n",
    "\n",
    "print(\"________________________\\n\")\n",
    "print(\"Results without Word2Vec:\")\n",
    "with index.searcher(weighting=scoring.TF_IDF()) as searcher:\n",
    "    query = MultifieldParser([\"name\", \"longDescription\", \"shortDescription\"], index.schema).parse(search_term)\n",
    "    results = searcher.search(query)\n",
    "    for docnum, score in results.items():\n",
    "        print(docnum, score)\n",
    "    for doc in results:\n",
    "        print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['summer', 'dress', 'winter']"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "stem = StemmingAnalyzer(stoplist=frozenset(['and', 'is', 'it', 'an', 'as', 'at', 'have', 'in', 'yet', 'if', 'from', 'for', 'when', 'by', 'to', 'you', 'be', 'we', 'that', 'may', 'not', 'with', 'tbd', 'a', 'on', 'your', 'this', 'of', 'us', 'will', 'can', 'the', 'or', 'are']), minsize=3)\n",
    "output = [token.text for token in stem(\"the summer dress for the winter\")]\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}