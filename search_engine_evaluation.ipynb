{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit ('.venv': venv)"
  },
  "interpreter": {
   "hash": "3fa4fbff26baf5372c1227c893813242fd1c5dcfd93352f945b61348dd56c099"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "import numpy as np"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "def precision_at_k(r, k):\r\n",
    "    assert k >= 1\r\n",
    "    r = np.asarray(r)[:k] != 0\r\n",
    "    if r.size != k:\r\n",
    "        raise ValueError('Relevanz score < k')\r\n",
    "    return np.mean(r)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "def average_precision(r):\r\n",
    "    r = np.asarray(r) != 0\r\n",
    "    out = [precision_at_k(r, k + 1) for k in range(r.size) if r[k]]\r\n",
    "    if not out:\r\n",
    "        return 0.\r\n",
    "    return np.mean(out)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "def mean_average_precision(rs):\r\n",
    "    return np.mean([average_precision(r) for r in rs])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "test1 = [[1, 1, 0, 1, 0], [0, 0, 1, 1, 1]] # BA Beispiel\r\n",
    "test2 = [[1, 1, 1, 1, 1], [0, 0, 0, 0, 0]]\r\n",
    "test3 = [[1, 1, 1, 1, 1], [1, 1, 1, 1, 1]]\r\n",
    "test4 = [[0, 0, 0, 0, 0], [0, 0, 0, 0, 0]]\r\n",
    "\r\n",
    "\r\n",
    "print(mean_average_precision(test1))\r\n",
    "print(mean_average_precision(test2))\r\n",
    "print(mean_average_precision(test3))\r\n",
    "print(mean_average_precision(test4))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.6972222222222222\n",
      "0.5\n",
      "1.0\n",
      "0.0\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "from flask import Flask, json, request"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "\r\n",
    "def json_response(payload, status=200):\r\n",
    "    return (json.dumps(payload), status, {'content-type': 'application/json'})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "jsonTest = {'0': [{'score': 60.9176157115223, 'rating': 1, 'ranking': 0}, {'score': 57.875198563054134, 'rating': 1, 'ranking': 1}, {'score': 57.875198563054134, 'rating': 0, 'ranking': 2}, {'score': 57.875198563054134, 'rating': 0, 'ranking': 3}, {'score': 54.2979302092234, 'rating': 1, 'ranking': 6}, {'score': 57.875198563054134, 'rating': 1, 'ranking': 4}, {'score': 54.469250578633385, 'rating': 1, 'ranking': 5}, {'score': 50.89198222480265, 'rating': 1, 'ranking': 7}, {'score': 50.89198222480265, 'rating': 0, 'ranking': 8}, {'score': 50.89198222480265, 'rating': 1, 'ranking': 9}], '1': [{'score': 74.12525988109097, 'rating': 1, 'ranking': 6}, {'score': 87.23094749123473, 'rating': 1, 'ranking': 0}, {'score': 81.03058575184154, 'rating': 0, 'ranking': 1}, {'score': 80.32562162048416, 'rating': 1, 'ranking': 2}, {'score': 74.12525988109097, 'rating': 0, 'ranking': 3}, {'score': 74.12525988109097, 'rating': 0, 'ranking': 4}, {'score': 74.12525988109097, 'rating': 1, 'ranking': 5}, {'score': 74.12525988109097, 'rating': 1, 'ranking': 7}, {'score': 74.12525988109097, 'rating': 1, 'ranking': 8}, {'score': 74.12525988109097, 'rating': 1, 'ranking': 9}], '2': [{'score': 102.30070084356836, 'rating':\r\n",
    "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        1, 'ranking': 0}, {'score': 102.30070084356836, 'rating': 0, 'ranking': 1}, {'score':\r\n",
    "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     102.30070084356836, 'rating': 0, 'ranking': 2}, {'score': 102.30070084356836, 'rating': 1, 'ranking': 3}, {'score': 102.30070084356836, 'rating': 1, 'ranking': 4}, {'score': 102.30070084356836, 'rating': 1, 'ranking': 5}, {'score': 102.30070084356836, 'rating': 1, 'ranking': 7}, {'score': 102.30070084356836, 'rating': 0, 'ranking': 6}, {'score': 102.30070084356836, 'rating': 0, 'ranking': 8}, {'score': 102.30070084356836, 'rating': 1, 'ranking': 9}], '3': [{'score': 38.76666340151514, 'rating': 1, 'ranking': 0}, {'score': 38.76666340151514, 'rating': 0, 'ranking': 1}, {'score': 38.76666340151514, 'rating': 1, 'ranking': 2}, {'score': 38.76666340151514, 'rating': 1, 'ranking': 3}, {'score': 38.76666340151514, 'rating': 1, 'ranking': 4}]}\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "print(jsonTest)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'0': [{'score': 60.9176157115223, 'rating': 1, 'ranking': 0}, {'score': 57.875198563054134, 'rating': 1, 'ranking': 1}, {'score': 57.875198563054134, 'rating': 0, 'ranking': 2}, {'score': 57.875198563054134, 'rating': 0, 'ranking': 3}, {'score': 54.2979302092234, 'rating': 1, 'ranking': 6}, {'score': 57.875198563054134, 'rating': 1, 'ranking': 4}, {'score': 54.469250578633385, 'rating': 1, 'ranking': 5}, {'score': 50.89198222480265, 'rating': 1, 'ranking': 7}, {'score': 50.89198222480265, 'rating': 0, 'ranking': 8}, {'score': 50.89198222480265, 'rating': 1, 'ranking': 9}], '1': [{'score': 74.12525988109097, 'rating': 1, 'ranking': 6}, {'score': 87.23094749123473, 'rating': 1, 'ranking': 0}, {'score': 81.03058575184154, 'rating': 0, 'ranking': 1}, {'score': 80.32562162048416, 'rating': 1, 'ranking': 2}, {'score': 74.12525988109097, 'rating': 0, 'ranking': 3}, {'score': 74.12525988109097, 'rating': 0, 'ranking': 4}, {'score': 74.12525988109097, 'rating': 1, 'ranking': 5}, {'score': 74.12525988109097, 'rating': 1, 'ranking': 7}, {'score': 74.12525988109097, 'rating': 1, 'ranking': 8}, {'score': 74.12525988109097, 'rating': 1, 'ranking': 9}], '2': [{'score': 102.30070084356836, 'rating': 1, 'ranking': 0}, {'score': 102.30070084356836, 'rating': 0, 'ranking': 1}, {'score': 102.30070084356836, 'rating': 0, 'ranking': 2}, {'score': 102.30070084356836, 'rating': 1, 'ranking': 3}, {'score': 102.30070084356836, 'rating': 1, 'ranking': 4}, {'score': 102.30070084356836, 'rating': 1, 'ranking': 5}, {'score': 102.30070084356836, 'rating': 1, 'ranking': 7}, {'score': 102.30070084356836, 'rating': 0, 'ranking': 6}, {'score': 102.30070084356836, 'rating': 0, 'ranking': 8}, {'score': 102.30070084356836, 'rating': 1, 'ranking': 9}], '3': [{'score': 38.76666340151514, 'rating': 1, 'ranking': 0}, {'score': 38.76666340151514, 'rating': 0, 'ranking': 1}, {'score': 38.76666340151514, 'rating': 1, 'ranking': 2}, {'score': 38.76666340151514, 'rating': 1, 'ranking': 3}, {'score': 38.76666340151514, 'rating': 1, 'ranking': 4}]}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "def calc_map(data):\r\n",
    "    all_ratings = []\r\n",
    "    for query in data:\r\n",
    "        rating_arr = data[query]\r\n",
    "        sorted_arr = sorted(rating_arr, key=lambda k: k['ranking'])\r\n",
    "        query_ratings = []\r\n",
    "        for res in sorted_arr:\r\n",
    "            query_ratings.append(res['rating'])\r\n",
    "        all_ratings.append(query_ratings)\r\n",
    "    \r\n",
    "    return mean_average_precision(all_ratings)\r\n",
    "\r\n",
    "calc_map(jsonTest)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.7302437641723356"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "def calc_avg_tfidf(data):\r\n",
    "    avg_tfidf = []\r\n",
    "    rating_sum = 0\r\n",
    "    for query in data:\r\n",
    "        rating_arr = data[query]\r\n",
    "        score_sum = 0\r\n",
    "        for rating in rating_arr:\r\n",
    "            score = rating['score']\r\n",
    "            score_sum = float(score_sum) + float(score)\r\n",
    "        avg_score = round(score_sum/len(rating_arr), 2)\r\n",
    "        avg_tfidf.append(avg_score)\r\n",
    "\r\n",
    "    return avg_tfidf\r\n",
    "\r\n",
    "\r\n",
    "print(calc_avg_tfidf(jsonTest))\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[55.39, 76.75, 102.3, 38.77]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "def calc_highest_tfidf(data):\r\n",
    "    highest_arr = []\r\n",
    "    for query in data:\r\n",
    "        rating_arr = data[query]\r\n",
    "        sorted_arr = sorted(rating_arr, key=lambda k: k['ranking'])\r\n",
    "        highest_arr.append(sorted_arr[0]['score'])\r\n",
    "    \r\n",
    "    return highest_arr\r\n",
    "\r\n",
    "print(calc_highest_tfidf(jsonTest))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[60.9176157115223, 87.23094749123473, 102.30070084356836, 38.76666340151514]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "source": [
    "import numpy as np\r\n",
    "\r\n",
    "def cum_gain(relevance):\r\n",
    "    return np.asarray(relevance).sum()\r\n",
    "\r\n",
    "\r\n",
    "def dcg(relevance):\r\n",
    "    rel = np.asarray(relevance)\r\n",
    "    p = len(rel)\r\n",
    "\r\n",
    "    log2i = np.log2(np.asarray(range(1, p + 1)) + 1)\r\n",
    "    return ((np.power(2, rel) - 1) / log2i).sum()\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "def idcg(relevance):\r\n",
    "    rel = np.asarray(relevance).copy()\r\n",
    "    rel.sort()\r\n",
    "    return dcg(rel[::-1])\r\n",
    "\r\n",
    "\r\n",
    "def ndcg(relevance, nranks = 10):\r\n",
    "    rel = np.asarray(relevance)\r\n",
    "    pad = max(0, nranks - len(rel))\r\n",
    "\r\n",
    "    rel = np.pad(rel, (0, pad), 'constant')\r\n",
    "\r\n",
    "    rel = rel[0:min(nranks, len(rel))]\r\n",
    "\r\n",
    "    ideal_dcg = idcg(rel)\r\n",
    "    if ideal_dcg == 0:\r\n",
    "        return 0.0\r\n",
    "\r\n",
    "    return dcg(rel) / ideal_dcg\r\n",
    "\r\n",
    "r = [3, 1, 1, 1, 1, 1, 1, 1, 0, 0]\r\n",
    "ndcg(r, 10)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "metadata": {},
     "execution_count": 74
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "source": [
    "def calc_ndcg(data):\r\n",
    "    ndcg_score = {}\r\n",
    "    for query in data:\r\n",
    "        rating_arr = data[query]\r\n",
    "        sorted_arr = sorted(rating_arr, key=lambda k: k['ranking'])\r\n",
    "        query_ratings = []\r\n",
    "        for res in sorted_arr:\r\n",
    "            query_ratings.append(res['rating'])\r\n",
    "        ndcg_value = ndcg(query_ratings)\r\n",
    "        ndcg_score[query] = ndcg_value\r\n",
    "    return ndcg_score\r\n",
    "\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'0': 0.9103499485143787, '1': 0.8507697982583751, '2': 0.840710074262534, '3': 0.9047172294870751}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "def calc_avg_tfidf(data):\r\n",
    "    avg_tfidf = {}\r\n",
    "    for query in data:\r\n",
    "        rating_arr = data[query]\r\n",
    "        score_sum = 0\r\n",
    "        for rating in rating_arr:\r\n",
    "            score = rating['score']\r\n",
    "            score_sum = float(score_sum) + float(score)\r\n",
    "        avg_score = round(score_sum/len(rating_arr), 2)\r\n",
    "        avg_tfidf[query] = avg_score\r\n",
    "\r\n",
    "    return avg_tfidf\r\n",
    "\r\n",
    "calc_avg_tfidf(jsonTest)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'0': 55.39, '1': 76.75, '2': 102.3, '3': 38.77}"
      ]
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "import uuid\r\n",
    "uuid.uuid4().hex"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'f42bd18d427a433c9b51270bf607a1b5'"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "def calc_precision(data):\r\n",
    "    precision = []\r\n",
    "    for query in data:\r\n",
    "        rating_arr = data[query]\r\n",
    "        good_count = 0\r\n",
    "        for rating in rating_arr:\r\n",
    "            if(rating['rating'] == 1):\r\n",
    "                good_count = good_count + 1\r\n",
    "        precision.append((good_count/len(rating_arr)))\r\n",
    "    return precision\r\n",
    "\r\n",
    "print(calc_precision(jsonTest))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.7, 0.7, 0.6, 0.8]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "def precision_at_k(r, k):\r\n",
    "    assert k >= 1\r\n",
    "    r = np.asarray(r)[:k] != 0\r\n",
    "    if r.size != k:\r\n",
    "        raise ValueError('Relevance score length < k')\r\n",
    "    return np.mean(r)\r\n",
    "\r\n",
    "def calc_precision_at_k(data, k):\r\n",
    "    all_ratings = []\r\n",
    "    for query in data:\r\n",
    "        rating_arr = data[query]\r\n",
    "        sorted_arr = sorted(rating_arr, key=lambda k: k['ranking'])\r\n",
    "        query_ratings = []\r\n",
    "        for res in sorted_arr:\r\n",
    "            query_ratings.append(res['rating'])\r\n",
    "        all_ratings.append(precision_at_k(query_ratings, k))\r\n",
    "    \r\n",
    "    return all_ratings\r\n",
    "    \r\n",
    "print(calc_precision_at_k(jsonTest, 5))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.6, 0.4, 0.6, 0.8]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "source": [
    "\r\n",
    "file = r\"C:\\\\Users\\\\lucas\\\\Documents\\\\Github\\\\BA_Search_Engine\\\\backend\\\\repository\\\\data\\\\metrics.json\"\r\n",
    "def load_metrics():\r\n",
    "    \r\n",
    "    with open(file, 'r') as f:\r\n",
    "        data = json.load(f)\r\n",
    "    return data\r\n",
    "\r\n",
    "data = load_metrics()\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "import pandas as pd\r\n",
    "import json\r\n",
    "\r\n",
    "file = r\"C:\\\\Users\\\\lucas\\\\Documents\\\\Github\\\\BA_Search_Engine\\\\backend\\\\repository\\\\data\\\\metrics.json\"\r\n",
    "with open(file, encoding=\"utf8\") as json_file:\r\n",
    "    data = json.load(json_file)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "# for user in data:\r\n",
    "#     print(user)\r\n",
    "#     search_type = data[user]\r\n",
    "#     mlMetrics = data[user]['ml']['metrics']\r\n",
    "#     ftMetrics = data[user]['ft']['metrics']\r\n",
    "#     # print(mlMetrics)\r\n",
    "#     for metric in ftMetrics:\r\n",
    "#         if metric != 'map':\r\n",
    "#             for queryIndex in ftMetrics[metric]:\r\n",
    "#                 value =  ftMetrics[metric][queryIndex]\r\n",
    "#                 print(value)\r\n",
    "# len(data)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "import xlsxwriter\r\n",
    "\r\n",
    "# Create a workbook and add a worksheet.\r\n",
    "workbook = xlsxwriter.Workbook('metrics.xlsx')\r\n",
    "worksheet = workbook.add_worksheet()\r\n",
    "\r\n",
    "header_format = workbook.add_format({\r\n",
    "    'bold': 1,\r\n",
    "    'align': 'center',\r\n",
    "})\r\n",
    "\r\n",
    "subHeader_format = workbook.add_format({\r\n",
    "    'bold': 0,\r\n",
    "    'align': 'center',\r\n",
    "})\r\n",
    "\r\n",
    "# Start from the first cell. Rows and columns are zero indexed.\r\n",
    "row = 0\r\n",
    "col = 0\r\n",
    "\r\n",
    "user_count = len(data)\r\n",
    "query_count = 10\r\n",
    "count = 1\r\n",
    "for user in data:\r\n",
    "    user_row = count\r\n",
    "    worksheet.merge_range(f'B{user_row}:K{user_row}', user, header_format)\r\n",
    "    worksheet.merge_range(f'B{user_row+1}:F{user_row+1}', 'ML-Searchengine', subHeader_format)\r\n",
    "    worksheet.write(f'B{user_row+2}', 'Avg. TF-IDF')\r\n",
    "    worksheet.write(f'C{user_row+2}', 'Top TF-IDF')\r\n",
    "    worksheet.write(f'D{user_row+2}', 'nDCG')\r\n",
    "    worksheet.write(f'E{user_row+2}', 'Precision')\r\n",
    "\r\n",
    "    worksheet.merge_range(f'G{user_row+1}:K{user_row+1}', 'Fulltext-Searchengine', subHeader_format)\r\n",
    "    worksheet.write(f'G{user_row+2}', 'Avg. TF-IDF')\r\n",
    "    worksheet.write(f'H{user_row+2}', 'Top TF-IDF')\r\n",
    "    worksheet.write(f'I{user_row+2}', 'nDCG')\r\n",
    "    worksheet.write(f'J{user_row+2}', 'Precision')\r\n",
    "\r\n",
    "    mlMetrics = data[user]['ml']['metrics']\r\n",
    "    ftMetrics = data[user]['ft']['metrics']\r\n",
    "\r\n",
    "    for i in range(query_count):\r\n",
    "        worksheet.write(user_row+2+i, 0, f'Query {i+1}')\r\n",
    "\r\n",
    "    m_row = user_row + 2\r\n",
    "    m_col = 1\r\n",
    "    for metric in mlMetrics:\r\n",
    "        if metric != 'map':\r\n",
    "            for queryIndex in mlMetrics[metric]:\r\n",
    "                value =  mlMetrics[metric][queryIndex]\r\n",
    "                worksheet.write(m_row+int(queryIndex), m_col, value)\r\n",
    "            m_col += 1\r\n",
    "            m_row = user_row + 2\r\n",
    "    \r\n",
    "    m_col = len(mlMetrics) + 1 # anzahl an metriken + 1 weil die erste Spalte für Query Bezeichnung genutzt wird\r\n",
    "    for metric in ftMetrics:\r\n",
    "        if metric != 'map':\r\n",
    "            for queryIndex in ftMetrics[metric]:\r\n",
    "                value =  ftMetrics[metric][queryIndex]\r\n",
    "                worksheet.write(m_row+int(queryIndex), m_col, value)\r\n",
    "            m_col += 1\r\n",
    "            m_row = user_row + 2\r\n",
    "        \r\n",
    "\r\n",
    "        \r\n",
    "    worksheet.write(query_count + count + 2, 0, 'MAP')\r\n",
    "    worksheet.write(query_count + count + 2, 1, mlMetrics['map'])\r\n",
    "    worksheet.write(query_count + count + 2, len(mlMetrics) + 1, ftMetrics['map'])\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "    count  = count + query_count + 4 # 4 weil 3 header über den queries + map Zeile\r\n",
    "\r\n",
    "workbook.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ]
}