{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "pd.set_option('display.max_colwidth', -1) # damit der komplette Output im Notebook angezeigt wird"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-1b07f7d21eed>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_option\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'display.max_colwidth'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# damit der komplette Output im Notebook angezeigt wird\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Parrot Libary Test (Pretrained Model)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from parrot import Parrot\r\n",
    "import torch\r\n",
    "import warnings\r\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "def random_state(seed):\r\n",
    "    torch.manual_seed(seed)\r\n",
    "    if torch.cuda.is_available():\r\n",
    "        torch.cuda.manual_seed_all(seed)\r\n",
    "\r\n",
    "random_state(1234)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "parrot = Parrot(model_tag=\"prithivida/parrot_paraphraser_on_T5\", use_gpu=True)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-04d0ae55bfc7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mparrot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mParrot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_tag\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"prithivida/parrot_paraphraser_on_T5\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\lucas\\Documents\\Github\\BA_Jupyter_Notebook\\.venv\\lib\\site-packages\\parrot\\parrot.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, model_tag, use_gpu)\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madequacy_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAdequacy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfluency_score\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mFluency\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiversity_score\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mDiversity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mrephrase\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_phrase\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiversity_ranker\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"levenshtein\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdo_diverse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstyle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madequacy_threshold\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.90\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfluency_threshold\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.90\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\lucas\\Documents\\Github\\BA_Jupyter_Notebook\\.venv\\lib\\site-packages\\parrot\\filters.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, model_tag)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_tag\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'paraphrase-distilroberta-base-v2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[0msentence_transformers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSentenceTransformer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     83\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiversity_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSentenceTransformer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_tag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\lucas\\Documents\\Github\\BA_Jupyter_Notebook\\.venv\\lib\\site-packages\\sentence_transformers\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0m__version__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"2.0.0\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0m__MODEL_HUB_ORGANIZATION__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'sentence-transformers'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSentencesDataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mParallelSentencesDataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mLoggingHandler\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLoggingHandler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mSentenceTransformer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSentenceTransformer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\lucas\\Documents\\Github\\BA_Jupyter_Notebook\\.venv\\lib\\site-packages\\sentence_transformers\\datasets\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mDenoisingAutoEncoderDataset\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDenoisingAutoEncoderDataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mNoDuplicatesDataLoader\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mNoDuplicatesDataLoader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mParallelSentencesDataset\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mParallelSentencesDataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mSentencesDataset\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSentencesDataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mSentenceLabelDataset\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSentenceLabelDataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\lucas\\Documents\\Github\\BA_Jupyter_Notebook\\.venv\\lib\\site-packages\\sentence_transformers\\datasets\\DenoisingAutoEncoderDataset.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreaders\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInputExample\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mInputExample\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtreebank\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTreebankWordDetokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\lucas\\Documents\\Github\\BA_Jupyter_Notebook\\.venv\\lib\\site-packages\\nltk\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[1;31m# Import top-level functionality into top-level namespace\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollocations\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecorators\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemoize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatstruct\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\lucas\\Documents\\Github\\BA_Jupyter_Notebook\\.venv\\lib\\site-packages\\nltk\\collocations.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;31m# these two unused imports are referenced in collocations.doctest\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m from nltk.metrics import (\n\u001b[0m\u001b[0;32m     40\u001b[0m     \u001b[0mContingencyMeasures\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[0mBigramAssocMeasures\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\lucas\\Documents\\Github\\BA_Jupyter_Notebook\\.venv\\lib\\site-packages\\nltk\\metrics\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \"\"\"\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m from nltk.metrics.scores import (\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[0maccuracy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mprecision\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\lucas\\Documents\\Github\\BA_Jupyter_Notebook\\.venv\\lib\\site-packages\\nltk\\metrics\\scores.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbetai\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mbetai\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\lucas\\Documents\\Github\\BA_Jupyter_Notebook\\.venv\\lib\\site-packages\\scipy\\stats\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    439\u001b[0m \"\"\"\n\u001b[0;32m    440\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 441\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    442\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mdistributions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    443\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mmorestats\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\lucas\\Documents\\Github\\BA_Jupyter_Notebook\\.venv\\lib\\site-packages\\scipy\\stats\\stats.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspecial\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mspecial\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdistributions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmstats_basic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m from ._stats_mstats_common import (_find_repeats, linregress, theilslopes,\n",
      "\u001b[1;32mc:\\Users\\lucas\\Documents\\Github\\BA_Jupyter_Notebook\\.venv\\lib\\site-packages\\scipy\\stats\\distributions.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#       instead of `git blame -Lxxx,+x`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_distn_infrastructure\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mrv_discrete\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrv_continuous\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrv_frozen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_continuous_distns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\lucas\\Documents\\Github\\BA_Jupyter_Notebook\\.venv\\lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;31m# for functions of continuous distributions (e.g. moments, entropy, cdf)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mintegrate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;31m# to approximate the pdf of a continuous distribution given its cdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\lucas\\Documents\\Github\\BA_Jupyter_Notebook\\.venv\\lib\\site-packages\\scipy\\integrate\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_ode\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_bvp\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msolve_bvp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m from ._ivp import (solve_ivp, OdeSolution, DenseOutput,\n\u001b[0m\u001b[0;32m     96\u001b[0m                    OdeSolver, RK23, RK45, DOP853, Radau, BDF, LSODA)\n\u001b[0;32m     97\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_quad_vec\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mquad_vec\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\lucas\\Documents\\Github\\BA_Jupyter_Notebook\\.venv\\lib\\site-packages\\scipy\\integrate\\_ivp\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;34m\"\"\"Suite of ODE solvers implemented in Python.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mivp\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msolve_ivp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mrk\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRK23\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRK45\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDOP853\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mradau\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRadau\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mbdf\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBDF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\lucas\\Documents\\Github\\BA_Jupyter_Notebook\\.venv\\lib\\site-packages\\scipy\\integrate\\_ivp\\ivp.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mbdf\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBDF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mradau\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRadau\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mrk\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRK23\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRK45\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDOP853\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mlsoda\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLSODA\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimize\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mOptimizeResult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mget_code\u001b[1;34m(self, fullname)\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mget_data\u001b[1;34m(self, path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "phrases = [\"a suit for a casual meeting\", \"beautiful dresses\", \"a t-shirt with logo print\"]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "for phrase in phrases:\r\n",
    "    print(\"\\n\", \"-\"*100)\r\n",
    "    print(\"Input_phrase: \", phrase)\r\n",
    "    print(\"-\"*100)\r\n",
    "    para_phrases = parrot.augment(input_phrase=phrase)\r\n",
    "    for para_phrase in para_phrases:\r\n",
    "        print(para_phrase)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Input_phrase:  a suit for a casual meeting\n",
      "----------------------------------------------------------------------------------------------------\n",
      "('dress for casual meetings', 18)\n",
      "('a suit for a casual meeting', 12)\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Input_phrase:  beautiful dresses\n",
      "----------------------------------------------------------------------------------------------------\n",
      "('wonderful dresses', 17)\n",
      "('pretty dresses', 17)\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Input_phrase:  a t-shirt with logo print\n",
      "----------------------------------------------------------------------------------------------------\n",
      "('a t-shirt with the logo printed', 18)\n",
      "('a t-shirt with logo print', 12)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## BART selbst trainieren"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import os\r\n",
    "from datetime import datetime\r\n",
    "import logging\r\n",
    "\r\n",
    "import pandas as pd\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from simpletransformers.seq2seq import Seq2SeqModel, Seq2SeqArgs"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import warnings\r\n",
    "\r\n",
    "import pandas as pd\r\n",
    "\r\n",
    "\r\n",
    "def load_data(\r\n",
    "    file_path, input_text_column, target_text_column, label_column, keep_label=1\r\n",
    "):\r\n",
    "    df = pd.read_csv(file_path, sep=\"\\t\", error_bad_lines=False)\r\n",
    "    df = df.loc[df[label_column] == keep_label]\r\n",
    "    df = df.rename(\r\n",
    "        columns={input_text_column: \"input_text\", target_text_column: \"target_text\"}\r\n",
    "    )\r\n",
    "    df = df[[\"input_text\", \"target_text\"]]\r\n",
    "    df[\"prefix\"] = \"paraphrase\"\r\n",
    "\r\n",
    "    return df\r\n",
    "\r\n",
    "\r\n",
    "def clean_unnecessary_spaces(out_string):\r\n",
    "    if not isinstance(out_string, str):\r\n",
    "        warnings.warn(f\">>> {out_string} <<< is not a string.\")\r\n",
    "        out_string = str(out_string)\r\n",
    "    out_string = (\r\n",
    "        out_string.replace(\" .\", \".\")\r\n",
    "        .replace(\" ?\", \"?\")\r\n",
    "        .replace(\" !\", \"!\")\r\n",
    "        .replace(\" ,\", \",\")\r\n",
    "        .replace(\" ' \", \"'\")\r\n",
    "        .replace(\" n't\", \"n't\")\r\n",
    "        .replace(\" 'm\", \"'m\")\r\n",
    "        .replace(\" 's\", \"'s\")\r\n",
    "        .replace(\" 've\", \"'ve\")\r\n",
    "        .replace(\" 're\", \"'re\")\r\n",
    "    )\r\n",
    "    return out_string\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "logging.basicConfig(level=logging.INFO)\r\n",
    "transformers_logger = logging.getLogger(\"transformers\")\r\n",
    "transformers_logger.setLevel(logging.ERROR)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# Google Data\r\n",
    "train_df = pd.read_csv(\"data/train.tsv\", sep=\"\\t\").astype(str)\r\n",
    "eval_df = pd.read_csv(\"data/dev.tsv\", sep=\"\\t\").astype(str)\r\n",
    "\r\n",
    "train_df = train_df.loc[train_df[\"label\"] == \"1\"]\r\n",
    "eval_df = eval_df.loc[eval_df[\"label\"] == \"1\"]\r\n",
    "\r\n",
    "train_df = train_df.rename(\r\n",
    "    columns={\"sentence1\": \"input_text\", \"sentence2\": \"target_text\"}\r\n",
    ")\r\n",
    "eval_df = eval_df.rename(\r\n",
    "    columns={\"sentence1\": \"input_text\", \"sentence2\": \"target_text\"}\r\n",
    ")\r\n",
    "\r\n",
    "train_df = train_df[[\"input_text\", \"target_text\"]]\r\n",
    "eval_df = eval_df[[\"input_text\", \"target_text\"]]\r\n",
    "\r\n",
    "train_df[\"prefix\"] = \"paraphrase\"\r\n",
    "eval_df[\"prefix\"] = \"paraphrase\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "print(train_df.shape)\r\n",
    "print(eval_df.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(21829, 3)\n",
      "(3539, 3)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "print(train_df.iloc[:1]['input_text'])\r\n",
    "print(train_df.iloc[:1]['target_text'])\r\n",
    "print(eval_df.iloc[:1]['input_text'])\r\n",
    "print(eval_df.iloc[:1]['target_text'])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1    The NBA season of 1975 -- 76 was the 30th seas...\n",
      "Name: input_text, dtype: object\n",
      "1    The 1975 -- 76 season of the National Basketba...\n",
      "Name: target_text, dtype: object\n",
      "1    They were there to enjoy us and they were ther...\n",
      "Name: input_text, dtype: object\n",
      "1    They were there for us to enjoy and they were ...\n",
      "Name: target_text, dtype: object\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# MSRP Data\r\n",
    "train_df = pd.concat(\r\n",
    "    [\r\n",
    "        train_df,\r\n",
    "        load_data(\"data/msr_paraphrase_train.txt\", \"#1 String\", \"#2 String\", \"Quality\"),\r\n",
    "    ]\r\n",
    ")\r\n",
    "eval_df = pd.concat(\r\n",
    "    [\r\n",
    "        eval_df,\r\n",
    "        load_data(\"data/msr_paraphrase_test.txt\", \"#1 String\", \"#2 String\", \"Quality\"),\r\n",
    "    ]\r\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "b'Skipping line 102: expected 5 fields, saw 6\\nSkipping line 656: expected 5 fields, saw 6\\nSkipping line 867: expected 5 fields, saw 6\\nSkipping line 880: expected 5 fields, saw 6\\nSkipping line 980: expected 5 fields, saw 6\\nSkipping line 1439: expected 5 fields, saw 6\\nSkipping line 1473: expected 5 fields, saw 6\\nSkipping line 1822: expected 5 fields, saw 6\\nSkipping line 1952: expected 5 fields, saw 6\\nSkipping line 2009: expected 5 fields, saw 6\\nSkipping line 2230: expected 5 fields, saw 6\\nSkipping line 2506: expected 5 fields, saw 6\\nSkipping line 2523: expected 5 fields, saw 6\\nSkipping line 2809: expected 5 fields, saw 6\\nSkipping line 2887: expected 5 fields, saw 6\\nSkipping line 2920: expected 5 fields, saw 6\\nSkipping line 2944: expected 5 fields, saw 6\\nSkipping line 3241: expected 5 fields, saw 6\\nSkipping line 3358: expected 5 fields, saw 6\\nSkipping line 3459: expected 5 fields, saw 6\\nSkipping line 3491: expected 5 fields, saw 6\\nSkipping line 3643: expected 5 fields, saw 6\\nSkipping line 3696: expected 5 fields, saw 6\\nSkipping line 3955: expected 5 fields, saw 6\\n'\n",
      "b'Skipping line 34: expected 5 fields, saw 6\\nSkipping line 121: expected 5 fields, saw 6\\nSkipping line 211: expected 5 fields, saw 6\\nSkipping line 263: expected 5 fields, saw 6\\nSkipping line 345: expected 5 fields, saw 6\\nSkipping line 696: expected 5 fields, saw 6\\nSkipping line 733: expected 5 fields, saw 6\\nSkipping line 847: expected 5 fields, saw 6\\nSkipping line 1392: expected 5 fields, saw 6\\nSkipping line 1467: expected 5 fields, saw 6\\nSkipping line 1551: expected 5 fields, saw 6\\n'\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# Quora Data\r\n",
    "\r\n",
    "# The Quora Dataset is not separated into train/test, so we do it manually the first time.\r\n",
    "# df = load_data(\r\n",
    "#     \"data/quora_duplicate_questions.tsv\", \"question1\", \"question2\", \"is_duplicate\"\r\n",
    "# )\r\n",
    "# q_train, q_test = train_test_split(df)\r\n",
    "\r\n",
    "# q_train.to_csv(\"data/quora_train.tsv\", sep=\"\\t\")\r\n",
    "# q_test.to_csv(\"data/quora_test.tsv\", sep=\"\\t\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "q_train = pd.read_csv(\"data/quora_train.tsv\", sep=\"\\t\")\r\n",
    "q_test = pd.read_csv(\"data/quora_test.tsv\", sep=\"\\t\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "train_df = pd.concat([train_df, q_train])\r\n",
    "eval_df = pd.concat([eval_df, q_test])\r\n",
    "\r\n",
    "train_df = train_df[[\"prefix\", \"input_text\", \"target_text\"]]\r\n",
    "eval_df = eval_df[[\"prefix\", \"input_text\", \"target_text\"]]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "train_df = train_df.dropna()\r\n",
    "eval_df = eval_df.dropna()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "train_df[\"input_text\"] = train_df[\"input_text\"].apply(clean_unnecessary_spaces)\r\n",
    "train_df[\"target_text\"] = train_df[\"target_text\"].apply(clean_unnecessary_spaces)\r\n",
    "\r\n",
    "eval_df[\"input_text\"] = eval_df[\"input_text\"].apply(clean_unnecessary_spaces)\r\n",
    "eval_df[\"target_text\"] = eval_df[\"target_text\"].apply(clean_unnecessary_spaces)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "print(train_df)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "            prefix                                         input_text  \\\n",
      "1       paraphrase  The NBA season of 1975 -- 76 was the 30th seas...   \n",
      "3       paraphrase  When comparable rates of flow can be maintaine...   \n",
      "4       paraphrase  It is the seat of Zerendi District in Akmola R...   \n",
      "5       paraphrase  William Henry Henry Harman was born on 17 Febr...   \n",
      "7       paraphrase  With a discrete amount of probabilities Formul...   \n",
      "...            ...                                                ...   \n",
      "111942  paraphrase  What was the craziest dream that you've ever had?   \n",
      "111943  paraphrase             How do I increase height at age of 16?   \n",
      "111944  paraphrase  If superconductors have infinite permeability ...   \n",
      "111945  paraphrase  How can I contact someone on Quora and send pr...   \n",
      "111946  paraphrase  Why should Jayalalittha be awarded by Bharat R...   \n",
      "\n",
      "                                              target_text  \n",
      "1       The 1975 -- 76 season of the National Basketba...  \n",
      "3       The results are high when comparable flow rate...  \n",
      "4       It is the seat of the district of Zerendi in A...  \n",
      "5       William Henry Harman was born in Waynesboro, V...  \n",
      "7       Given a discrete set of probabilities formula ...  \n",
      "...                                                   ...  \n",
      "111942       Which is the weirdest dream youâ€™ve ever had?  \n",
      "111943  My age in 22 and my height is 5'5 I want more ...  \n",
      "111944  If superconductors have infinite permeability ...  \n",
      "111945  Is it possible to send messages privately thro...  \n",
      "111946  What are your views on Jayalalitha's name reco...  \n",
      "\n",
      "[136422 rows x 3 columns]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "model_args = Seq2SeqArgs()\r\n",
    "model_args.eval_batch_size = 16\r\n",
    "model_args.evaluate_during_training = True\r\n",
    "model_args.evaluate_during_training_steps = 2500\r\n",
    "model_args.evaluate_during_training_verbose = True\r\n",
    "model_args.fp16 = False\r\n",
    "model_args.learning_rate = 5e-5\r\n",
    "model_args.max_seq_length = 128\r\n",
    "model_args.num_train_epochs = 2\r\n",
    "model_args.overwrite_output_dir = True\r\n",
    "model_args.reprocess_input_data = True\r\n",
    "model_args.save_eval_checkpoints = False\r\n",
    "model_args.save_steps = -1\r\n",
    "model_args.train_batch_size = 4\r\n",
    "model_args.use_multiprocessing = False\r\n",
    "\r\n",
    "model_args.do_sample = True\r\n",
    "model_args.num_beams = None\r\n",
    "model_args.num_return_sequences = 3\r\n",
    "model_args.max_length = 128\r\n",
    "model_args.top_k = 30\r\n",
    "model_args.top_p = 0.95\r\n",
    "\r\n",
    "model_args.wandb_project = \"Paraphrasing with BART\"\r\n",
    "model_args.WANDB_NOTEBOOK_NAME = \"Paraphrasing for Semantic Search\"\r\n",
    "\r\n",
    "\r\n",
    "model = Seq2SeqModel(\r\n",
    "    encoder_decoder_type=\"bart\",\r\n",
    "    encoder_decoder_name=\"facebook/bart-large\",\r\n",
    "    args=model_args,\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "import gc\r\n",
    "import torch\r\n",
    "gc.collect()\r\n",
    "# torch.cuda.empty_cache()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "4299"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model.train_model(train_df, eval_data=eval_df)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "to_predict = [\r\n",
    "    prefix + \": \" + str(input_text)\r\n",
    "    for prefix, input_text in zip(\r\n",
    "        eval_df[\"prefix\"].tolist(), eval_df[\"input_text\"].tolist()\r\n",
    "    )\r\n",
    "]\r\n",
    "truth = eval_df[\"target_text\"].tolist()\r\n",
    "\r\n",
    "preds = model.predict(to_predict)\r\n",
    "\r\n",
    "# Saving the predictions if needed\r\n",
    "os.makedirs(\"predictions\", exist_ok=True)\r\n",
    "\r\n",
    "with open(f\"predictions/predictions_{datetime.now()}.txt\", \"w\") as f:\r\n",
    "    for i, text in enumerate(eval_df[\"input_text\"].tolist()):\r\n",
    "        f.write(str(text) + \"\\n\\n\")\r\n",
    "\r\n",
    "        f.write(\"Truth:\\n\")\r\n",
    "        f.write(truth[i] + \"\\n\\n\")\r\n",
    "\r\n",
    "        f.write(\"Prediction:\\n\")\r\n",
    "        for pred in preds[i]:\r\n",
    "            f.write(str(pred) + \"\\n\")\r\n",
    "        f.write(\r\n",
    "            \"________________________________________________________________________________\\n\"\r\n",
    "        )\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pretrained Sentence Transformers"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import json\r\n",
    "import pandas as pd\r\n",
    "import re"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "with open(r\"E:\\Users\\Lucas xD\\Downloads\\Products_Q_US_edited.json\", encoding=\"utf8\") as json_file:\r\n",
    "    data = json.load(json_file)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "df = pd.json_normalize(data)\r\n",
    "df = df.drop(columns=['brand', 'colors', 'gender', 'productId', 'sizes', 'styleName', 'variants', 'image', 'id', 'longDescription'])\r\n",
    "\r\n",
    "df.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                               name  \\\n",
       "0          Stretch Cotton V-Neck T-Shirt | Dredosos   \n",
       "1                 Leather belt with embossed detail   \n",
       "2         Italian Leather Derby Dress Shoe | Prindo   \n",
       "3   Virgin Wool Tuxedo, Regular Fit | Stars/Glamour   \n",
       "4  Italian Virgin Wool Suit, Slim Fit | Huge/Genius   \n",
       "\n",
       "                                    shortDescription  \n",
       "0  This t-shirt by HUGO is crafted from cotton wi...  \n",
       "1  Upgrade your everyday collection with this tim...  \n",
       "2  Crafted from fine Italian calfskin with a prin...  \n",
       "3  This regular fit tuxedo by BOSS is crafted in ...  \n",
       "4  Our best-selling suit just got better with an ...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>shortDescription</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Stretch Cotton V-Neck T-Shirt | Dredosos</td>\n",
       "      <td>This t-shirt by HUGO is crafted from cotton wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Leather belt with embossed detail</td>\n",
       "      <td>Upgrade your everyday collection with this tim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Italian Leather Derby Dress Shoe | Prindo</td>\n",
       "      <td>Crafted from fine Italian calfskin with a prin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Virgin Wool Tuxedo, Regular Fit | Stars/Glamour</td>\n",
       "      <td>This regular fit tuxedo by BOSS is crafted in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Italian Virgin Wool Suit, Slim Fit | Huge/Genius</td>\n",
       "      <td>Our best-selling suit just got better with an ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "df = df.stack().reset_index() # all in one column\r\n",
    "df = df.drop(columns=['level_0', 'level_1'])\r\n",
    "df.columns = ['sentences']\r\n",
    "df.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                           sentences\n",
       "0           Stretch Cotton V-Neck T-Shirt | Dredosos\n",
       "1  This t-shirt by HUGO is crafted from cotton wi...\n",
       "2                  Leather belt with embossed detail\n",
       "3  Upgrade your everyday collection with this tim...\n",
       "4          Italian Leather Derby Dress Shoe | Prindo"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Stretch Cotton V-Neck T-Shirt | Dredosos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This t-shirt by HUGO is crafted from cotton wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Leather belt with embossed detail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Upgrade your everyday collection with this tim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Italian Leather Derby Dress Shoe | Prindo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def pre_process(text):\r\n",
    "    # lowercase\r\n",
    "    text=text.lower()\r\n",
    "    \r\n",
    "    #remove tags\r\n",
    "    text=re.sub(\"</?.*?>\",\" <> \",text)\r\n",
    "    \r\n",
    "    # remove special characters and digits\r\n",
    "    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\r\n",
    "    \r\n",
    "    return text"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "df['sentences'] = df['sentences'].apply(lambda x:pre_process(x))\r\n",
    "df.to_csv('./data/processedData.csv', index=False)\r\n",
    "df.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                           sentences\n",
       "0             stretch cotton v neck t shirt dredosos\n",
       "1  this t shirt by hugo is crafted from cotton wi...\n",
       "2                  leather belt with embossed detail\n",
       "3  upgrade your everyday collection with this tim...\n",
       "4            italian leather derby dress shoe prindo"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>stretch cotton v neck t shirt dredosos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>this t shirt by hugo is crafted from cotton wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>leather belt with embossed detail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>upgrade your everyday collection with this tim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>italian leather derby dress shoe prindo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "df = pd.read_csv('./data/processedData.csv', )\r\n",
    "df['sentences'] = df['sentences'].astype(str)\r\n",
    "df.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                           sentences\n",
       "0             stretch cotton v neck t shirt dredosos\n",
       "1  this t shirt by hugo is crafted from cotton wi...\n",
       "2                  leather belt with embossed detail\n",
       "3  upgrade your everyday collection with this tim...\n",
       "4            italian leather derby dress shoe prindo"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>stretch cotton v neck t shirt dredosos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>this t shirt by hugo is crafted from cotton wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>leather belt with embossed detail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>upgrade your everyday collection with this tim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>italian leather derby dress shoe prindo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### mit Tensorflow"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "from tensorflow.keras.losses import cosine_similarity\r\n",
    "from tensorflow import math\r\n",
    "from sentence_transformers import SentenceTransformer\r\n",
    "\r\n",
    "embedder = SentenceTransformer('paraphrase-MiniLM-L6-v2')\r\n",
    "corpus = df['sentences'].tolist()\r\n",
    "corpus_embeddings = embedder.encode(corpus, convert_to_tensor=True)\r\n",
    "\r\n",
    "# queries = ['white shoes that are confortable to wear', \r\n",
    "#             'a black suit for a formal event',\r\n",
    "#             'a dress for a summer evening walk on the beach']\r\n",
    "queries = ['shoes that are comfortable to wear']\r\n",
    "top_k = 3\r\n",
    "for query in queries:\r\n",
    "    query_embedding = embedder.encode(query, convert_to_tensor=True)\r\n",
    "\r\n",
    "    cos_scores = math.abs(cosine_similarity(\r\n",
    "        query_embedding.cpu(),\r\n",
    "        corpus_embeddings.cpu(),\r\n",
    "        axis=-1))\r\n",
    "    top_results = math.top_k(cos_scores, k=5)\r\n",
    "\r\n",
    "    print(\"\\n======================\")\r\n",
    "    print(\"Query:\", query)\r\n",
    "    print(\"Top 5 most similar sentences in product data:\")\r\n",
    "\r\n",
    "    for score, idx in zip(top_results.values.numpy(), top_results.indices.numpy()):\r\n",
    "        if(score > 0.5):\r\n",
    "            print(corpus[idx][:67], \"(Score: {:.4f})\".format(score))\r\n",
    "  "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "======================\n",
      "Query: shoes that are comfortable to wear\n",
      "Top 5 most similar sentences in product data:\n",
      "low top sneakers in mixed materials (Score: 0.7513)\n",
      "running style sneakers in tonal nappa leather and mesh (Score: 0.7508)\n",
      "modern sneakers by boss crafted with uppers in nappa leather suede  (Score: 0.7499)\n",
      "leather oxford shoes with modern broguing (Score: 0.7470)\n",
      "low top sneakers in hybrid materials (Score: 0.7427)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### mit Torch"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "embedder = SentenceTransformer('paraphrase-MiniLM-L6-v2')\r\n",
    "\r\n",
    "corpus = df['sentences'].tolist()\r\n",
    "corpus_embeddings = embedder.encode(corpus, convert_to_tensor=True)\r\n",
    "\r\n",
    "# corpus_embeddings = sentence_embeddings\r\n",
    "\r\n",
    "queries = ['white shoes that are confortable to wear', \r\n",
    "            'a black suit for a formal event',\r\n",
    "            'a dress for a summer evening walk on the beach']\r\n",
    "top_k = 3\r\n",
    "for query in queries:\r\n",
    "    query_embedding = embedder.encode(query, convert_to_tensor=True)\r\n",
    "\r\n",
    "    cos_scores = util.pytorch_cos_sim(query_embedding, corpus_embeddings)[0]\r\n",
    "    top_results = torch.topk(cos_scores, k=top_k)\r\n",
    "\r\n",
    "    print(\"\\n======================\")\r\n",
    "    print(\"Query:\", query)\r\n",
    "    print(\"Top 3 most similar sentences in product data:\")\r\n",
    "\r\n",
    "    for score, idx in zip(top_results[0], top_results[1]):\r\n",
    "        print(corpus[idx][:50], \"(Score: {:.4f})\".format(score))\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "======================\n",
      "Query: white shoes that are confortable to wear\n",
      "Top 3 most similar sentences in corpus:\n",
      "modern sneakers by boss crafted with uppers in nap (Score: 0.7399)\n",
      "low top sneakers in mixed materials (Score: 0.7361)\n",
      "running style sneakers in tonal nappa leather and  (Score: 0.7302)\n",
      "\n",
      "======================\n",
      "Query: a black suit for a formal event\n",
      "Top 3 most similar sentences in corpus:\n",
      "a contemporary tuxedo by boss menswear cut to a de (Score: 0.6458)\n",
      "slim fit tuxedo with silk trims and pocket square (Score: 0.6263)\n",
      "a two piece tuxedo by boss menswear cut to a narro (Score: 0.6233)\n",
      "\n",
      "======================\n",
      "Query: a dress for a summer evening walk on the beach\n",
      "Top 3 most similar sentences in corpus:\n",
      "short sleeved dress with sparkly pleated skirt (Score: 0.5366)\n",
      "long length sleeveless dress in silk with waterfal (Score: 0.5312)\n",
      "tie neck evening dress in lustrous fabric (Score: 0.5299)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "def calcZeit(t):\r\n",
    "  return f\"{t/60/60} Stunden :(\"\r\n",
    "\r\n",
    "calcZeit(5400)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'1.5 Stunden :('"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "import pandas as pd\r\n",
    "import re\r\n",
    "from tensorflow.keras.losses import cosine_similarity\r\n",
    "from tensorflow import math\r\n",
    "from sentence_transformers import SentenceTransformer\r\n",
    "\r\n",
    "\r\n",
    "queries = 'white shoes that are confortable to wear'\r\n",
    "\r\n",
    "def loadProdData():\r\n",
    "    df = pd.read_csv('./data/processedData.csv', )\r\n",
    "    df['sentences'] = df['sentences'].astype(str)\r\n",
    "    return df['sentences'].tolist()\r\n",
    "\r\n",
    "\r\n",
    "def loadProdTensor():\r\n",
    "    tensor = torch.load('./data/corpus_embeddings.pt')\r\n",
    "    return tensor\r\n",
    "\r\n",
    "\r\n",
    "def calc_similarity(query):\r\n",
    "    embedder = SentenceTransformer('paraphrase-MiniLM-L6-v2')\r\n",
    "\r\n",
    "    prod_list = loadProdData()\r\n",
    "    corpus_embeddings = loadProdTensor()\r\n",
    "\r\n",
    "    query_embedding = embedder.encode(query, convert_to_tensor=True)\r\n",
    "    cos_scores = math.abs(cosine_similarity(\r\n",
    "        query_embedding.cpu(),\r\n",
    "        corpus_embeddings.cpu(),\r\n",
    "        axis=-1))\r\n",
    "    top_results = math.top_k(cos_scores, k=3)\r\n",
    "\r\n",
    "    relevant_documents = []\r\n",
    "    for score, idx in zip(top_results.values.numpy(), top_results.indices.numpy()):\r\n",
    "        if(score > 0.7):\r\n",
    "            relevant_documents.append({'text': prod_list[idx], 'score': score})\r\n",
    "\r\n",
    "    return relevant_documents\r\n",
    "\r\n",
    "rel = calc_similarity(queries)\r\n",
    "for t in rel:\r\n",
    "    print(f\"der text:{t['text'][:30]}, der score {t['score']}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "der text:modern sneakers by boss crafte, der score 0.739911675453186\n",
      "der text:low top sneakers in mixed mate, der score 0.7360946536064148\n",
      "der text:running style sneakers in tona, der score 0.7301977872848511\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "bla = [{'id': 2, 'score': 2.3},{'id': 3, 'score': 2.3},{'id': 1, 'score': 2.3}]\r\n",
    "if not any(d['id'] == 21 for d in bla):\r\n",
    "    print('nicht drin')\r\n",
    "else:\r\n",
    "    print('drin')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "nicht drin\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.1",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit ('.venv': venv)"
  },
  "interpreter": {
   "hash": "3fa4fbff26baf5372c1227c893813242fd1c5dcfd93352f945b61348dd56c099"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}